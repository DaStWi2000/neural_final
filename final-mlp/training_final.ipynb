{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 19:02:14.335287: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('./data_processed/X_train.npy')\n",
    "t_train = np.load('./data_processed/t_train.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(483861, 47)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69077942\n",
      "Iteration 1, loss = 0.67683018\n",
      "Iteration 1, loss = 0.66097472\n",
      "Iteration 1, loss = 0.69111715\n",
      "Iteration 1, loss = 0.68487898\n",
      "Iteration 1, loss = 0.52484655\n",
      "Iteration 1, loss = 0.68619180\n",
      "Iteration 1, loss = 0.68903489\n",
      "Iteration 1, loss = 0.67895678\n",
      "Iteration 1, loss = 0.68007154\n",
      "Iteration 1, loss = 0.69161411\n",
      "Iteration 1, loss = 0.68195034\n",
      "Iteration 1, loss = 0.53623073\n",
      "Iteration 1, loss = 0.66274989\n",
      "Iteration 1, loss = 0.53978535\n",
      "Iteration 1, loss = 0.68516355\n",
      "Iteration 1, loss = 0.55172036\n",
      "Iteration 1, loss = 0.57255457\n",
      "Iteration 1, loss = 0.68560166\n",
      "Iteration 1, loss = 0.68415531\n",
      "Iteration 1, loss = 0.52385812\n",
      "Iteration 2, loss = 0.68217897\n",
      "Iteration 2, loss = 0.51973240\n",
      "Iteration 2, loss = 0.60063204\n",
      "Iteration 2, loss = 0.68640803\n",
      "Iteration 2, loss = 0.64254151\n",
      "Iteration 2, loss = 0.65578548\n",
      "Iteration 2, loss = 0.34499033\n",
      "Iteration 2, loss = 0.67746193\n",
      "Iteration 2, loss = 0.64787492\n",
      "Iteration 2, loss = 0.68743354\n",
      "Iteration 2, loss = 0.61365013\n",
      "Iteration 2, loss = 0.62986593\n",
      "Iteration 2, loss = 0.40888664\n",
      "Iteration 2, loss = 0.65528032\n",
      "Iteration 2, loss = 0.52997067\n",
      "Iteration 2, loss = 0.40201964\n",
      "Iteration 2, loss = 0.66447893\n",
      "Iteration 2, loss = 0.44464896\n",
      "Iteration 2, loss = 0.48474567\n",
      "Iteration 2, loss = 0.33382713\n",
      "Iteration 2, loss = 0.63970321\n",
      "Iteration 3, loss = 0.66591296\n",
      "Iteration 3, loss = 0.37774744\n",
      "Iteration 3, loss = 0.68356452\n",
      "Iteration 3, loss = 0.49865771\n",
      "Iteration 3, loss = 0.62128007\n",
      "Iteration 3, loss = 0.60812506\n",
      "Iteration 3, loss = 0.25697546\n",
      "Iteration 3, loss = 0.66492743\n",
      "Iteration 3, loss = 0.68012229\n",
      "Iteration 3, loss = 0.55813834\n",
      "Iteration 3, loss = 0.61542462\n",
      "Iteration 3, loss = 0.53133467\n",
      "Iteration 3, loss = 0.34596309\n",
      "Iteration 3, loss = 0.60777877\n",
      "Iteration 3, loss = 0.63921817\n",
      "Iteration 3, loss = 0.38201358\n",
      "Iteration 3, loss = 0.37451900\n",
      "Iteration 3, loss = 0.41199555\n",
      "Iteration 3, loss = 0.60429883\n",
      "Iteration 3, loss = 0.33499748\n",
      "Iteration 3, loss = 0.24394222\n",
      "Iteration 4, loss = 0.30069248\n",
      "Iteration 4, loss = 0.64537746\n",
      "Iteration 4, loss = 0.42357498\n",
      "Iteration 4, loss = 0.68185196\n",
      "Iteration 4, loss = 0.59425176\n",
      "Iteration 4, loss = 0.59354275\n",
      "Iteration 4, loss = 0.66391052\n",
      "Iteration 4, loss = 0.20864324\n",
      "Iteration 4, loss = 0.65664966\n",
      "Iteration 4, loss = 0.47327011\n",
      "Iteration 4, loss = 0.47885268\n",
      "Iteration 4, loss = 0.59349060\n",
      "Iteration 4, loss = 0.30495239\n",
      "Iteration 4, loss = 0.55978005\n",
      "Iteration 4, loss = 0.61097760\n",
      "Iteration 4, loss = 0.29477895\n",
      "Iteration 4, loss = 0.29784477\n",
      "Iteration 4, loss = 0.59119334\n",
      "Iteration 4, loss = 0.34312511\n",
      "Iteration 4, loss = 0.26717309\n",
      "Iteration 4, loss = 0.19477171\n",
      "Iteration 5, loss = 0.26175871\n",
      "Iteration 5, loss = 0.62782019\n",
      "Iteration 5, loss = 0.38306162\n",
      "Iteration 5, loss = 0.68042714\n",
      "Iteration 5, loss = 0.57566581\n",
      "Iteration 5, loss = 0.58733577\n",
      "Iteration 5, loss = 0.64376341\n",
      "Iteration 5, loss = 0.65150874\n",
      "Iteration 5, loss = 0.41987887\n",
      "Iteration 5, loss = 0.44146680\n",
      "Iteration 5, loss = 0.17652634\n",
      "Iteration 5, loss = 0.57876952\n",
      "Iteration 5, loss = 0.27247931\n",
      "Iteration 5, loss = 0.52258462\n",
      "Iteration 5, loss = 0.58376714\n",
      "Iteration 5, loss = 0.23193818\n",
      "Iteration 5, loss = 0.25241027\n",
      "Iteration 5, loss = 0.20989962\n",
      "Iteration 5, loss = 0.28551830\n",
      "Iteration 5, loss = 0.58495770\n",
      "Iteration 5, loss = 0.16410736\n",
      "Iteration 6, loss = 0.61598125\n",
      "Iteration 6, loss = 0.24030147\n",
      "Iteration 6, loss = 0.36128067\n",
      "Iteration 6, loss = 0.67914702\n",
      "Iteration 6, loss = 0.64766379\n",
      "Iteration 6, loss = 0.56304441\n",
      "Iteration 6, loss = 0.62811210\n",
      "Iteration 6, loss = 0.38553541\n",
      "Iteration 6, loss = 0.58399842\n",
      "Iteration 6, loss = 0.42214683\n",
      "Iteration 6, loss = 0.24538939\n",
      "Iteration 6, loss = 0.15362180\n",
      "Iteration 6, loss = 0.56768468\n",
      "Iteration 6, loss = 0.49396778\n",
      "Iteration 6, loss = 0.56194540\n",
      "Iteration 6, loss = 0.23037381\n",
      "Iteration 6, loss = 0.18457366\n",
      "Iteration 6, loss = 0.17027022\n",
      "Iteration 6, loss = 0.14315742\n",
      "Iteration 6, loss = 0.23969855\n",
      "Iteration 6, loss = 0.58109810\n",
      "Iteration 7, loss = 0.60843269\n",
      "Iteration 7, loss = 0.22681766\n",
      "Iteration 7, loss = 0.34805882\n",
      "Iteration 7, loss = 0.67798630\n",
      "Iteration 7, loss = 0.55381745\n",
      "Iteration 7, loss = 0.64466169\n",
      "Iteration 7, loss = 0.61789398\n",
      "Iteration 7, loss = 0.36479135\n",
      "Iteration 7, loss = 0.58190138\n",
      "Iteration 7, loss = 0.40910133\n",
      "Iteration 7, loss = 0.22280118\n",
      "Iteration 7, loss = 0.46935831\n",
      "Iteration 7, loss = 0.13632046\n",
      "Iteration 7, loss = 0.55847558\n",
      "Iteration 7, loss = 0.54413338\n",
      "Iteration 7, loss = 0.21724686\n",
      "Iteration 7, loss = 0.15096975\n",
      "Iteration 7, loss = 0.14125800\n",
      "Iteration 7, loss = 0.57825527\n",
      "Iteration 7, loss = 0.12665767\n",
      "Iteration 7, loss = 0.20329872\n",
      "Iteration 8, loss = 0.60360631\n",
      "Iteration 8, loss = 0.21769404\n",
      "Iteration 8, loss = 0.33932117\n",
      "Iteration 8, loss = 0.67682187\n",
      "Iteration 8, loss = 0.54637548\n",
      "Iteration 8, loss = 0.61130790\n",
      "Iteration 8, loss = 0.64176603\n",
      "Iteration 8, loss = 0.35091255\n",
      "Iteration 8, loss = 0.57980573\n",
      "Iteration 8, loss = 0.39936673\n",
      "Iteration 8, loss = 0.44885442\n",
      "Iteration 8, loss = 0.20256419\n",
      "Iteration 8, loss = 0.55034193\n",
      "Iteration 8, loss = 0.12211702\n",
      "Iteration 8, loss = 0.20836286\n",
      "Iteration 8, loss = 0.52906832\n",
      "Iteration 8, loss = 0.12539157\n",
      "Iteration 8, loss = 0.11869489\n",
      "Iteration 8, loss = 0.17389654\n",
      "Iteration 8, loss = 0.57608280\n",
      "Iteration 8, loss = 0.11341570\n",
      "Iteration 9, loss = 0.60044346\n",
      "Iteration 9, loss = 0.21076936\n",
      "Iteration 9, loss = 0.67586561\n",
      "Iteration 9, loss = 0.33287284\n",
      "Iteration 9, loss = 0.60687517\n",
      "Iteration 9, loss = 0.54047797\n",
      "Iteration 9, loss = 0.63928572\n",
      "Iteration 9, loss = 0.34067191\n",
      "Iteration 9, loss = 0.57802684\n",
      "Iteration 9, loss = 0.43158501\n",
      "Iteration 9, loss = 0.39134316\n",
      "Iteration 9, loss = 0.18407049\n",
      "Iteration 9, loss = 0.51594437\n",
      "Iteration 9, loss = 0.54292151\n",
      "Iteration 9, loss = 0.10999553\n",
      "Iteration 9, loss = 0.20128564\n",
      "Iteration 9, loss = 0.10551323\n",
      "Iteration 9, loss = 0.10077996\n",
      "Iteration 9, loss = 0.57392309\n",
      "Iteration 9, loss = 0.15011653\n",
      "Iteration 9, loss = 0.10206613\n",
      "Iteration 10, loss = 0.59800872\n",
      "Iteration 10, loss = 0.20498265\n",
      "Iteration 10, loss = 0.67485142\n",
      "Iteration 10, loss = 0.32727437\n",
      "Iteration 10, loss = 0.53531545\n",
      "Iteration 10, loss = 0.60343865\n",
      "Iteration 10, loss = 0.63728831\n",
      "Iteration 10, loss = 0.33240989\n",
      "Iteration 10, loss = 0.57651384\n",
      "Iteration 10, loss = 0.41712989\n",
      "Iteration 10, loss = 0.38443417\n",
      "Iteration 10, loss = 0.16747018\n",
      "Iteration 10, loss = 0.08957075\n",
      "Iteration 10, loss = 0.50412803\n",
      "Iteration 10, loss = 0.53600858\n",
      "Iteration 10, loss = 0.19577831\n",
      "Iteration 10, loss = 0.09951751\n",
      "Iteration 10, loss = 0.08675761\n",
      "Iteration 10, loss = 0.09222243\n",
      "Iteration 10, loss = 0.13071723\n",
      "Iteration 10, loss = 0.57221094\n",
      "Iteration 11, loss = 0.59609858\n",
      "Iteration 11, loss = 0.20033935\n",
      "Iteration 11, loss = 0.32230199\n",
      "Iteration 11, loss = 0.67392541\n",
      "Iteration 11, loss = 0.63525073\n",
      "Iteration 11, loss = 0.53065959\n",
      "Iteration 11, loss = 0.60085619\n",
      "Iteration 11, loss = 0.32546576\n",
      "Iteration 11, loss = 0.57497423\n",
      "Iteration 11, loss = 0.40405247\n",
      "Iteration 11, loss = 0.37895118\n",
      "Iteration 11, loss = 0.15346247\n",
      "Iteration 11, loss = 0.07679629\n",
      "Iteration 11, loss = 0.52966129\n",
      "Iteration 11, loss = 0.49352059\n",
      "Iteration 11, loss = 0.19113103\n",
      "Iteration 11, loss = 0.09030701\n",
      "Iteration 11, loss = 0.08358584\n",
      "Iteration 11, loss = 0.07494722\n",
      "Iteration 11, loss = 0.57061200\n",
      "Iteration 11, loss = 0.11488065\n",
      "Iteration 12, loss = 0.59447266\n",
      "Iteration 12, loss = 0.19606854\n",
      "Iteration 12, loss = 0.31794294\n",
      "Iteration 12, loss = 0.67306789\n",
      "Iteration 12, loss = 0.63338265\n",
      "Iteration 12, loss = 0.59872948\n",
      "Iteration 12, loss = 0.52683029\n",
      "Iteration 12, loss = 0.31926635\n",
      "Iteration 12, loss = 0.57365679\n",
      "Iteration 12, loss = 0.39226344\n",
      "Iteration 12, loss = 0.37392254\n",
      "Iteration 12, loss = 0.14089837\n",
      "Iteration 12, loss = 0.06614382\n",
      "Iteration 12, loss = 0.52394644\n",
      "Iteration 12, loss = 0.18654148\n",
      "Iteration 12, loss = 0.48388266\n",
      "Iteration 12, loss = 0.08220789\n",
      "Iteration 12, loss = 0.07608573\n",
      "Iteration 12, loss = 0.06559167\n",
      "Iteration 12, loss = 0.56892904\n",
      "Iteration 12, loss = 0.10193234\n",
      "Iteration 13, loss = 0.59281873\n",
      "Iteration 13, loss = 0.19215167\n",
      "Iteration 13, loss = 0.31371023\n",
      "Iteration 13, loss = 0.67215116\n",
      "Iteration 13, loss = 0.63175913\n",
      "Iteration 13, loss = 0.59649837\n",
      "Iteration 13, loss = 0.52336289\n",
      "Iteration 13, loss = 0.31367785\n",
      "Iteration 13, loss = 0.57232348\n",
      "Iteration 13, loss = 0.36920158\n",
      "Iteration 13, loss = 0.38145773\n",
      "Iteration 13, loss = 0.13016077\n",
      "Iteration 13, loss = 0.05743490\n",
      "Iteration 13, loss = 0.51857922\n",
      "Iteration 13, loss = 0.18236040\n",
      "Iteration 13, loss = 0.47474655\n",
      "Iteration 13, loss = 0.07525062\n",
      "Iteration 13, loss = 0.06968506\n",
      "Iteration 13, loss = 0.05777366\n",
      "Iteration 13, loss = 0.56763849\n",
      "Iteration 14, loss = 0.59128079\n",
      "Iteration 13, loss = 0.09088474\n",
      "Iteration 14, loss = 0.67131394\n",
      "Iteration 14, loss = 0.18859013\n",
      "Iteration 14, loss = 0.30972381\n",
      "Iteration 14, loss = 0.63016417\n",
      "Iteration 14, loss = 0.30845875\n",
      "Iteration 14, loss = 0.51989383\n",
      "Iteration 14, loss = 0.59465334\n",
      "Iteration 14, loss = 0.57106921\n",
      "Iteration 14, loss = 0.36492570\n",
      "Iteration 14, loss = 0.11984652\n",
      "Iteration 14, loss = 0.37185275\n",
      "Iteration 14, loss = 0.05025807\n",
      "Iteration 14, loss = 0.51350504\n",
      "Iteration 14, loss = 0.17840355\n",
      "Iteration 14, loss = 0.46633178\n",
      "Iteration 14, loss = 0.06961873\n",
      "Iteration 14, loss = 0.56617233\n",
      "Iteration 14, loss = 0.06399194\n",
      "Iteration 14, loss = 0.05113110\n",
      "Iteration 15, loss = 0.58973197\n",
      "Iteration 14, loss = 0.08169043\n",
      "Iteration 15, loss = 0.67040106\n",
      "Iteration 15, loss = 0.62859141\n",
      "Iteration 15, loss = 0.18518091\n",
      "Iteration 15, loss = 0.30599575\n",
      "Iteration 15, loss = 0.51683986\n",
      "Iteration 15, loss = 0.30370230\n",
      "Iteration 15, loss = 0.59263014\n",
      "Iteration 15, loss = 0.56981254Iteration 15, loss = 0.36066122\n",
      "\n",
      "Iteration 15, loss = 0.36235318\n",
      "Iteration 15, loss = 0.11044271\n",
      "Iteration 15, loss = 0.04427802\n",
      "Iteration 15, loss = 0.50887167\n",
      "Iteration 15, loss = 0.45828575\n",
      "Iteration 15, loss = 0.17474053\n",
      "Iteration 15, loss = 0.56495908\n",
      "Iteration 15, loss = 0.06455984\n",
      "Iteration 15, loss = 0.05893431\n",
      "Iteration 16, loss = 0.58793216\n",
      "Iteration 15, loss = 0.04560489\n",
      "Iteration 15, loss = 0.07387189\n",
      "Iteration 16, loss = 0.66952951\n",
      "Iteration 16, loss = 0.62701783\n",
      "Iteration 16, loss = 0.30225316\n",
      "Iteration 16, loss = 0.18180354\n",
      "Iteration 16, loss = 0.51393047\n",
      "Iteration 16, loss = 0.59079765\n",
      "Iteration 16, loss = 0.29935169\n",
      "Iteration 16, loss = 0.56876361\n",
      "Iteration 16, loss = 0.35684501\n",
      "Iteration 16, loss = 0.10184042\n",
      "Iteration 16, loss = 0.35389701\n",
      "Iteration 16, loss = 0.03933642\n",
      "Iteration 16, loss = 0.50456222\n",
      "Iteration 16, loss = 0.17124878\n",
      "Iteration 16, loss = 0.45085429\n",
      "Iteration 16, loss = 0.56363109\n",
      "Iteration 16, loss = 0.05981363\n",
      "Iteration 17, loss = 0.58652222\n",
      "Iteration 16, loss = 0.05465006\n",
      "Iteration 16, loss = 0.04091652\n",
      "Iteration 17, loss = 0.66881205\n",
      "Iteration 17, loss = 0.62553688\n",
      "Iteration 16, loss = 0.06720045\n",
      "Iteration 17, loss = 0.29865509\n",
      "Iteration 17, loss = 0.17881705\n",
      "Iteration 17, loss = 0.58904942\n",
      "Iteration 17, loss = 0.51118149\n",
      "Iteration 17, loss = 0.29522689\n",
      "Iteration 17, loss = 0.56761318\n",
      "Iteration 17, loss = 0.35294685\n",
      "Iteration 17, loss = 0.34589033\n",
      "Iteration 17, loss = 0.09397693\n",
      "Iteration 17, loss = 0.03501855\n",
      "Iteration 17, loss = 0.50041811\n",
      "Iteration 17, loss = 0.44397864\n",
      "Iteration 17, loss = 0.16786834\n",
      "Iteration 18, loss = 0.58481449\n",
      "Iteration 17, loss = 0.56266732\n",
      "Iteration 17, loss = 0.05076893\n",
      "Iteration 17, loss = 0.05536490\n",
      "Iteration 18, loss = 0.62437078\n",
      "Iteration 17, loss = 0.03690728\n",
      "Iteration 18, loss = 0.66803516\n",
      "Iteration 17, loss = 0.06152538\n",
      "Iteration 18, loss = 0.29515086\n",
      "Iteration 18, loss = 0.17575361\n",
      "Iteration 18, loss = 0.58729892\n",
      "Iteration 18, loss = 0.50867179\n",
      "Iteration 18, loss = 0.29117739\n",
      "Iteration 18, loss = 0.56640722\n",
      "Iteration 18, loss = 0.34913928\n",
      "Iteration 18, loss = 0.33857140\n",
      "Iteration 18, loss = 0.08680676\n",
      "Iteration 18, loss = 0.03147763\n",
      "Iteration 18, loss = 0.49637406\n",
      "Iteration 18, loss = 0.43757787\n",
      "Iteration 19, loss = 0.58310374\n",
      "Iteration 18, loss = 0.16464729\n",
      "Iteration 19, loss = 0.62265849\n",
      "Iteration 18, loss = 0.56138513\n",
      "Iteration 18, loss = 0.03348687\n",
      "Iteration 18, loss = 0.04735055\n",
      "Iteration 19, loss = 0.66723866\n",
      "Iteration 18, loss = 0.05142904\n",
      "Iteration 18, loss = 0.05644588\n",
      "Iteration 19, loss = 0.29158789\n",
      "Iteration 19, loss = 0.17289718\n",
      "Iteration 19, loss = 0.50611576\n",
      "Iteration 19, loss = 0.58538551\n",
      "Iteration 19, loss = 0.28748079\n",
      "Iteration 19, loss = 0.34530976\n",
      "Iteration 19, loss = 0.56531429\n",
      "Iteration 19, loss = 0.08053545\n",
      "Iteration 19, loss = 0.33214945\n",
      "Iteration 19, loss = 0.02836265\n",
      "Iteration 19, loss = 0.49303123\n",
      "Iteration 19, loss = 0.43144328\n",
      "Iteration 20, loss = 0.58139615\n",
      "Iteration 19, loss = 0.16178474\n",
      "Iteration 20, loss = 0.62121736\n",
      "Iteration 19, loss = 0.56037292\n",
      "Iteration 19, loss = 0.04436487\n",
      "Iteration 19, loss = 0.03055694\n",
      "Iteration 20, loss = 0.66658503\n",
      "Iteration 19, loss = 0.04769041\n",
      "Iteration 20, loss = 0.28811325\n",
      "Iteration 19, loss = 0.05194229\n",
      "Iteration 20, loss = 0.17010521\n",
      "Iteration 20, loss = 0.50357499\n",
      "Iteration 20, loss = 0.58372146\n",
      "Iteration 20, loss = 0.28390743\n",
      "Iteration 20, loss = 0.34172897\n",
      "Iteration 20, loss = 0.56420115\n",
      "Iteration 20, loss = 0.02566224\n",
      "Iteration 20, loss = 0.32581127\n",
      "Iteration 20, loss = 0.07490833\n",
      "Iteration 20, loss = 0.48901651\n",
      "Iteration 20, loss = 0.42567101\n",
      "Iteration 21, loss = 0.57984131\n",
      "Iteration 21, loss = 0.61975354\n",
      "Iteration 20, loss = 0.15902816\n",
      "Iteration 20, loss = 0.55923070\n",
      "Iteration 20, loss = 0.02809374\n",
      "Iteration 21, loss = 0.66579736\n",
      "Iteration 20, loss = 0.04180263\n",
      "Iteration 20, loss = 0.04456241\n",
      "Iteration 21, loss = 0.28481561\n",
      "Iteration 20, loss = 0.04817471\n",
      "Iteration 21, loss = 0.16740493\n",
      "Iteration 21, loss = 0.50084446\n",
      "Iteration 21, loss = 0.58226047\n",
      "Iteration 21, loss = 0.28047281\n",
      "Iteration 21, loss = 0.33834054\n",
      "Iteration 21, loss = 0.56322473\n",
      "Iteration 21, loss = 0.02334850\n",
      "Iteration 21, loss = 0.32004645\n",
      "Iteration 21, loss = 0.07039578\n",
      "Iteration 21, loss = 0.48572586\n",
      "Iteration 22, loss = 0.57796280\n",
      "Iteration 21, loss = 0.42044936\n",
      "Iteration 22, loss = 0.61818103\n",
      "Iteration 21, loss = 0.15628731\n",
      "Iteration 21, loss = 0.55845210\n",
      "Iteration 22, loss = 0.66503330\n",
      "Iteration 21, loss = 0.02577425\n",
      "Iteration 21, loss = 0.03934300\n",
      "Iteration 22, loss = 0.28161805\n",
      "Iteration 21, loss = 0.04159107\n",
      "Iteration 21, loss = 0.04455190\n",
      "Iteration 22, loss = 0.16472793\n",
      "Iteration 22, loss = 0.58064377\n",
      "Iteration 22, loss = 0.49841462\n",
      "Iteration 22, loss = 0.27708663\n",
      "Iteration 22, loss = 0.33497235\n",
      "Iteration 22, loss = 0.56214289\n",
      "Iteration 22, loss = 0.02136656\n",
      "Iteration 22, loss = 0.06581484\n",
      "Iteration 22, loss = 0.31426219\n",
      "Iteration 22, loss = 0.48249631\n",
      "Iteration 22, loss = 0.41549732\n",
      "Iteration 23, loss = 0.57614708\n",
      "Iteration 23, loss = 0.61646967\n",
      "Iteration 22, loss = 0.15373745\n",
      "Iteration 23, loss = 0.66436043\n",
      "Iteration 22, loss = 0.55743522\n",
      "Iteration 22, loss = 0.02380096\n",
      "Iteration 22, loss = 0.03705287\n",
      "Iteration 23, loss = 0.27872613\n",
      "Iteration 22, loss = 0.03941677\n",
      "Iteration 22, loss = 0.04139733\n",
      "Iteration 23, loss = 0.16247388\n",
      "Iteration 23, loss = 0.57914860\n",
      "Iteration 23, loss = 0.27374168\n",
      "Iteration 23, loss = 0.49617441\n",
      "Iteration 23, loss = 0.56114947\n",
      "Iteration 23, loss = 0.33169062\n",
      "Iteration 23, loss = 0.01976110\n",
      "Iteration 23, loss = 0.06188867\n",
      "Iteration 23, loss = 0.30933719\n",
      "Iteration 23, loss = 0.47956326\n",
      "Iteration 24, loss = 0.61477989\n",
      "Iteration 24, loss = 0.57428889\n",
      "Iteration 23, loss = 0.41060378\n",
      "Iteration 23, loss = 0.15158897\n",
      "Iteration 24, loss = 0.66364628\n",
      "Iteration 23, loss = 0.55682811\n",
      "Iteration 23, loss = 0.03489106\n",
      "Iteration 23, loss = 0.02193295\n",
      "Iteration 24, loss = 0.27576526\n",
      "Iteration 23, loss = 0.03721834\n",
      "Iteration 23, loss = 0.03850882\n",
      "Iteration 24, loss = 0.57738179\n",
      "Iteration 24, loss = 0.16003987\n",
      "Iteration 24, loss = 0.27060621\n",
      "Iteration 24, loss = 0.49414713\n",
      "Iteration 24, loss = 0.32845713\n",
      "Iteration 24, loss = 0.56037885\n",
      "Iteration 24, loss = 0.01826012\n",
      "Iteration 24, loss = 0.05828088\n",
      "Iteration 24, loss = 0.30435546\n",
      "Iteration 25, loss = 0.61315549\n",
      "Iteration 24, loss = 0.14925564\n",
      "Iteration 24, loss = 0.47677998\n",
      "Iteration 25, loss = 0.57237371\n",
      "Iteration 25, loss = 0.66285769\n",
      "Iteration 24, loss = 0.40611401\n",
      "Iteration 24, loss = 0.55586576\n",
      "Iteration 24, loss = 0.03309670\n",
      "Iteration 24, loss = 0.02033227\n",
      "Iteration 25, loss = 0.27291778\n",
      "Iteration 25, loss = 0.15759723\n",
      "Iteration 24, loss = 0.03528536\n",
      "Iteration 24, loss = 0.03594415\n",
      "Iteration 25, loss = 0.57610576\n",
      "Iteration 25, loss = 0.26751119\n",
      "Iteration 25, loss = 0.49168192\n",
      "Iteration 25, loss = 0.32542124\n",
      "Iteration 25, loss = 0.55919492\n",
      "Iteration 25, loss = 0.01697257\n",
      "Iteration 25, loss = 0.05478497\n",
      "Iteration 25, loss = 0.29978764\n",
      "Iteration 26, loss = 0.61164167\n",
      "Iteration 25, loss = 0.14711292\n",
      "Iteration 26, loss = 0.66220405\n",
      "Iteration 25, loss = 0.47403683\n",
      "Iteration 26, loss = 0.57063623\n",
      "Iteration 25, loss = 0.55526193\n",
      "Iteration 25, loss = 0.40192951\n",
      "Iteration 25, loss = 0.03129693\n",
      "Iteration 25, loss = 0.01892151\n",
      "Iteration 26, loss = 0.15542096\n",
      "Iteration 26, loss = 0.27032957\n",
      "Iteration 25, loss = 0.03345843\n",
      "Iteration 26, loss = 0.26472808\n",
      "Iteration 25, loss = 0.03332979\n",
      "Iteration 26, loss = 0.57453208\n",
      "Iteration 26, loss = 0.48950140\n",
      "Iteration 26, loss = 0.32252478\n",
      "Iteration 26, loss = 0.55850501\n",
      "Iteration 26, loss = 0.01577525\n",
      "Iteration 26, loss = 0.05144662\n",
      "Iteration 27, loss = 0.60965265\n",
      "Iteration 26, loss = 0.29560250\n",
      "Iteration 26, loss = 0.14514540\n",
      "Iteration 27, loss = 0.66170165\n",
      "Iteration 27, loss = 0.56860863\n",
      "Iteration 26, loss = 0.55424142\n",
      "Iteration 26, loss = 0.47135732\n",
      "Iteration 26, loss = 0.39810772\n",
      "Iteration 26, loss = 0.02976353\n",
      "Iteration 26, loss = 0.01767943\n",
      "Iteration 27, loss = 0.26765517\n",
      "Iteration 27, loss = 0.15321396\n",
      "Iteration 27, loss = 0.26189861\n",
      "Iteration 27, loss = 0.57313497\n",
      "Iteration 26, loss = 0.03145253\n",
      "Iteration 26, loss = 0.03135967\n",
      "Iteration 27, loss = 0.48751677\n",
      "Iteration 27, loss = 0.55737208\n",
      "Iteration 27, loss = 0.01468112\n",
      "Iteration 27, loss = 0.31965425\n",
      "Iteration 27, loss = 0.04829893\n",
      "Iteration 28, loss = 0.60813049\n",
      "Iteration 27, loss = 0.55372372\n",
      "Iteration 27, loss = 0.29147039\n",
      "Iteration 28, loss = 0.56699775\n",
      "Iteration 27, loss = 0.14291771\n",
      "Iteration 28, loss = 0.66093019\n",
      "Iteration 27, loss = 0.46893014\n",
      "Iteration 27, loss = 0.39413499\n",
      "Iteration 27, loss = 0.02838839\n",
      "Iteration 27, loss = 0.01652176\n",
      "Iteration 28, loss = 0.26510990\n",
      "Iteration 28, loss = 0.15131244\n",
      "Iteration 28, loss = 0.57169334\n",
      "Iteration 28, loss = 0.25937359\n",
      "Iteration 28, loss = 0.48527855\n",
      "Iteration 27, loss = 0.02961099\n",
      "Iteration 27, loss = 0.02972413\n",
      "Iteration 28, loss = 0.01369740\n",
      "Iteration 28, loss = 0.55661785\n",
      "Iteration 28, loss = 0.31697531\n",
      "Iteration 29, loss = 0.60640300\n",
      "Iteration 28, loss = 0.04543153\n",
      "Iteration 29, loss = 0.56508315\n",
      "Iteration 28, loss = 0.55293551\n",
      "Iteration 28, loss = 0.28782595\n",
      "Iteration 29, loss = 0.66018566\n",
      "Iteration 28, loss = 0.14106949\n",
      "Iteration 28, loss = 0.46655393\n",
      "Iteration 28, loss = 0.39051162\n",
      "Iteration 28, loss = 0.01556156\n",
      "Iteration 28, loss = 0.02709660\n",
      "Iteration 29, loss = 0.57036529\n",
      "Iteration 29, loss = 0.26262262\n",
      "Iteration 29, loss = 0.14935204\n",
      "Iteration 29, loss = 0.25665296\n",
      "Iteration 29, loss = 0.48352896\n",
      "Iteration 28, loss = 0.02795583\n",
      "Iteration 28, loss = 0.02800888\n",
      "Iteration 29, loss = 0.01283669\n",
      "Iteration 29, loss = 0.55573586\n",
      "Iteration 29, loss = 0.31427286\n",
      "Iteration 30, loss = 0.60498331\n",
      "Iteration 29, loss = 0.04297147\n",
      "Iteration 30, loss = 0.56322171\n",
      "Iteration 29, loss = 0.28402653\n",
      "Iteration 29, loss = 0.55234465\n",
      "Iteration 30, loss = 0.65956357\n",
      "Iteration 29, loss = 0.13908817\n",
      "Iteration 29, loss = 0.46415335\n",
      "Iteration 29, loss = 0.38739270\n",
      "Iteration 29, loss = 0.01466389\n",
      "Iteration 29, loss = 0.02582378\n",
      "Iteration 30, loss = 0.26015870\n",
      "Iteration 30, loss = 0.56901993\n",
      "Iteration 30, loss = 0.14726298\n",
      "Iteration 30, loss = 0.25413899\n",
      "Iteration 30, loss = 0.48152335\n",
      "Iteration 29, loss = 0.02655822\n",
      "Iteration 29, loss = 0.02643600\n",
      "Iteration 30, loss = 0.01203116\n",
      "Iteration 31, loss = 0.60352262\n",
      "Iteration 30, loss = 0.55484399\n",
      "Iteration 30, loss = 0.31167827\n",
      "Iteration 30, loss = 0.04064675\n",
      "Iteration 30, loss = 0.28053711\n",
      "Iteration 31, loss = 0.56182835\n",
      "Iteration 31, loss = 0.65891604\n",
      "Iteration 30, loss = 0.55164864\n",
      "Iteration 30, loss = 0.13733778\n",
      "Iteration 30, loss = 0.46204132\n",
      "Iteration 30, loss = 0.01389216\n",
      "Iteration 30, loss = 0.38383762\n",
      "Iteration 30, loss = 0.02470015\n",
      "Iteration 31, loss = 0.56785401\n",
      "Iteration 31, loss = 0.14527866\n",
      "Iteration 31, loss = 0.25777499\n",
      "Iteration 31, loss = 0.25178407\n",
      "Iteration 31, loss = 0.47964718\n",
      "Iteration 30, loss = 0.02505200\n",
      "Iteration 30, loss = 0.02501648\n",
      "Iteration 32, loss = 0.60231018\n",
      "Iteration 31, loss = 0.01136039\n",
      "Iteration 31, loss = 0.55422085\n",
      "Iteration 31, loss = 0.30914518\n",
      "Iteration 31, loss = 0.03862995\n",
      "Iteration 31, loss = 0.27738831\n",
      "Iteration 32, loss = 0.55997404\n",
      "Iteration 32, loss = 0.65845257\n",
      "Iteration 31, loss = 0.55104232\n",
      "Iteration 31, loss = 0.13538165\n",
      "Iteration 31, loss = 0.45962811\n",
      "Iteration 31, loss = 0.01306198\n",
      "Iteration 31, loss = 0.38096000\n",
      "Iteration 31, loss = 0.02351065\n",
      "Iteration 32, loss = 0.56648129\n",
      "Iteration 32, loss = 0.14345862\n",
      "Iteration 32, loss = 0.25560828\n",
      "Iteration 32, loss = 0.24946211\n",
      "Iteration 32, loss = 0.47779966\n",
      "Iteration 31, loss = 0.02371956\n",
      "Iteration 31, loss = 0.02380190\n",
      "Iteration 32, loss = 0.01069542\n",
      "Iteration 33, loss = 0.60087894\n",
      "Iteration 32, loss = 0.55329675\n",
      "Iteration 32, loss = 0.30682089\n",
      "Iteration 32, loss = 0.03669317\n",
      "Iteration 32, loss = 0.27440600\n",
      "Iteration 33, loss = 0.55832068\n",
      "Iteration 33, loss = 0.65775709\n",
      "Iteration 32, loss = 0.55039323\n",
      "Iteration 32, loss = 0.13371027\n",
      "Iteration 32, loss = 0.01231737\n",
      "Iteration 33, loss = 0.56516885\n",
      "Iteration 32, loss = 0.37808245\n",
      "Iteration 32, loss = 0.02240990\n",
      "Iteration 33, loss = 0.14169799\n",
      "Iteration 32, loss = 0.45769394\n",
      "Iteration 33, loss = 0.25326844\n",
      "Iteration 33, loss = 0.24735383\n",
      "Iteration 33, loss = 0.47623793\n",
      "Iteration 32, loss = 0.02247708\n",
      "Iteration 32, loss = 0.02256050\n",
      "Iteration 34, loss = 0.59974283\n",
      "Iteration 33, loss = 0.01015093\n",
      "Iteration 33, loss = 0.55239461\n",
      "Iteration 33, loss = 0.30435020\n",
      "Iteration 33, loss = 0.27138002\n",
      "Iteration 34, loss = 0.55673282\n",
      "Iteration 34, loss = 0.65702505\n",
      "Iteration 33, loss = 0.03473938\n",
      "Iteration 33, loss = 0.13210813\n",
      "Iteration 33, loss = 0.54991358\n",
      "Iteration 34, loss = 0.56384759\n",
      "Iteration 33, loss = 0.01169419\n",
      "Iteration 33, loss = 0.37504411\n",
      "Iteration 33, loss = 0.02137672\n",
      "Iteration 34, loss = 0.14011633\n",
      "Iteration 34, loss = 0.25108200\n",
      "Iteration 34, loss = 0.47437913\n",
      "Iteration 34, loss = 0.24514282\n",
      "Iteration 33, loss = 0.02143986\n",
      "Iteration 33, loss = 0.02132230\n",
      "Iteration 35, loss = 0.59884353\n",
      "Iteration 33, loss = 0.45554791\n",
      "Iteration 34, loss = 0.00966953\n",
      "Iteration 34, loss = 0.55157652\n",
      "Iteration 34, loss = 0.30211546\n",
      "Iteration 34, loss = 0.26870898\n",
      "Iteration 35, loss = 0.65628668\n",
      "Iteration 35, loss = 0.55541404\n",
      "Iteration 34, loss = 0.03294084\n",
      "Iteration 34, loss = 0.54911587\n",
      "Iteration 34, loss = 0.13049334\n",
      "Iteration 35, loss = 0.56283362\n",
      "Iteration 34, loss = 0.37253651\n",
      "Iteration 34, loss = 0.01108801\n",
      "Iteration 34, loss = 0.02038009\n",
      "Iteration 35, loss = 0.13835263\n",
      "Iteration 34, loss = 0.02049044\n",
      "Iteration 35, loss = 0.47293359\n",
      "Iteration 36, loss = 0.59743886\n",
      "Iteration 34, loss = 0.02008133\n",
      "Iteration 35, loss = 0.00925336\n",
      "Iteration 35, loss = 0.24330329\n",
      "Iteration 35, loss = 0.55085858\n",
      "Iteration 35, loss = 0.24916993\n",
      "Iteration 36, loss = 0.65591665\n",
      "Iteration 35, loss = 0.29974987\n",
      "Iteration 35, loss = 0.26571758\n",
      "Iteration 36, loss = 0.55358170\n",
      "Iteration 35, loss = 0.03130189\n",
      "Iteration 35, loss = 0.54865527\n",
      "Iteration 34, loss = 0.45375769\n",
      "Iteration 35, loss = 0.12905614\n",
      "Iteration 36, loss = 0.56159510\n",
      "Iteration 35, loss = 0.37012270\n",
      "Iteration 35, loss = 0.01049897\n",
      "Iteration 35, loss = 0.01952337\n",
      "Iteration 36, loss = 0.13677366\n",
      "Iteration 37, loss = 0.59646276\n",
      "Iteration 35, loss = 0.01944959\n",
      "Iteration 35, loss = 0.01914422\n",
      "Iteration 36, loss = 0.47118214\n",
      "Iteration 36, loss = 0.55009612\n",
      "Iteration 36, loss = 0.00884976\n",
      "Iteration 37, loss = 0.65517642\n",
      "Iteration 36, loss = 0.02993924\n",
      "Iteration 36, loss = 0.29777943\n",
      "Iteration 37, loss = 0.55233247\n",
      "Iteration 36, loss = 0.26291286\n",
      "Iteration 36, loss = 0.24133369\n",
      "Iteration 36, loss = 0.54807758\n",
      "Iteration 36, loss = 0.12750670\n",
      "Iteration 37, loss = 0.56070139\n",
      "Iteration 36, loss = 0.01003548\n",
      "Iteration 36, loss = 0.36765838\n",
      "Iteration 36, loss = 0.24683580\n",
      "Iteration 36, loss = 0.01869981\n",
      "Iteration 37, loss = 0.13512497\n",
      "Iteration 38, loss = 0.59537033\n",
      "Iteration 37, loss = 0.46960699\n",
      "Iteration 36, loss = 0.01861642\n",
      "Iteration 35, loss = 0.45205613\n",
      "Iteration 36, loss = 0.01835260\n",
      "Iteration 37, loss = 0.54938258\n",
      "Iteration 38, loss = 0.65456495\n",
      "Iteration 38, loss = 0.55090070Iteration 37, loss = 0.02875308\n",
      "\n",
      "Iteration 37, loss = 0.26077405\n",
      "Iteration 37, loss = 0.29561309\n",
      "Iteration 37, loss = 0.54769656\n",
      "Iteration 37, loss = 0.12610177\n",
      "Iteration 37, loss = 0.00844939\n",
      "Iteration 38, loss = 0.55949339\n",
      "Iteration 37, loss = 0.00956214\n",
      "Iteration 37, loss = 0.36559867\n",
      "Iteration 37, loss = 0.01817996\n",
      "Iteration 39, loss = 0.59419695\n",
      "Iteration 37, loss = 0.23941684\n",
      "Iteration 38, loss = 0.46793890\n",
      "Iteration 37, loss = 0.01781056\n",
      "Iteration 38, loss = 0.13378050\n",
      "Iteration 37, loss = 0.01747348\n",
      "Iteration 39, loss = 0.65383241\n",
      "Iteration 38, loss = 0.54880778\n",
      "Iteration 39, loss = 0.54983912\n",
      "Iteration 38, loss = 0.02746916\n",
      "Iteration 38, loss = 0.25813645\n",
      "Iteration 37, loss = 0.24480040\n",
      "Iteration 38, loss = 0.29382373\n",
      "Iteration 38, loss = 0.54715423\n",
      "Iteration 38, loss = 0.12468570\n",
      "Iteration 39, loss = 0.55850145\n",
      "Iteration 38, loss = 0.00926568\n",
      "Iteration 38, loss = 0.01742168\n",
      "Iteration 38, loss = 0.36301702\n",
      "Iteration 40, loss = 0.59326148\n",
      "Iteration 36, loss = 0.45006694\n",
      "Iteration 38, loss = 0.00813241\n",
      "Iteration 38, loss = 0.01709467\n",
      "Iteration 39, loss = 0.46686831\n",
      "Iteration 38, loss = 0.01668303\n",
      "Iteration 40, loss = 0.65298553\n",
      "Iteration 39, loss = 0.54795088\n",
      "Iteration 40, loss = 0.54835342\n",
      "Iteration 39, loss = 0.02627068\n",
      "Iteration 39, loss = 0.29188215\n",
      "Iteration 39, loss = 0.13225820\n",
      "Iteration 39, loss = 0.54645463\n",
      "Iteration 39, loss = 0.12340857\n",
      "Iteration 39, loss = 0.25579872\n",
      "Iteration 40, loss = 0.55734646\n",
      "Iteration 38, loss = 0.23759370\n",
      "Iteration 39, loss = 0.00887419\n",
      "Iteration 39, loss = 0.01685284\n",
      "Iteration 41, loss = 0.59248937\n",
      "Iteration 39, loss = 0.01645817\n",
      "Iteration 41, loss = 0.65267369\n",
      "Iteration 39, loss = 0.01600734\n",
      "Iteration 40, loss = 0.46524371\n",
      "Iteration 39, loss = 0.36080064\n",
      "Iteration 40, loss = 0.54718898\n",
      "Iteration 38, loss = 0.24297909\n",
      "Iteration 40, loss = 0.02523286\n",
      "Iteration 41, loss = 0.54704132\n",
      "Iteration 40, loss = 0.28992184\n",
      "Iteration 40, loss = 0.54598699\n",
      "Iteration 40, loss = 0.12211152\n",
      "Iteration 41, loss = 0.55661297\n",
      "Iteration 40, loss = 0.00855827\n",
      "Iteration 40, loss = 0.25351529\n",
      "Iteration 39, loss = 0.00783517\n",
      "Iteration 40, loss = 0.01623705\n",
      "Iteration 42, loss = 0.59140210\n",
      "Iteration 37, loss = 0.44819993\n",
      "Iteration 40, loss = 0.13082027\n",
      "Iteration 40, loss = 0.01571354\n",
      "Iteration 40, loss = 0.01541050\n",
      "Iteration 42, loss = 0.65181493\n",
      "Iteration 41, loss = 0.54652877\n",
      "Iteration 41, loss = 0.02423378\n",
      "Iteration 42, loss = 0.54593757\n",
      "Iteration 41, loss = 0.46371588\n",
      "Iteration 41, loss = 0.28803462\n",
      "Iteration 41, loss = 0.54539582\n",
      "Iteration 41, loss = 0.12076072\n",
      "Iteration 42, loss = 0.55553663\n",
      "Iteration 39, loss = 0.23580361\n",
      "Iteration 41, loss = 0.00823476\n",
      "Iteration 43, loss = 0.59057437\n",
      "Iteration 41, loss = 0.01572544\n",
      "Iteration 40, loss = 0.35902197\n",
      "Iteration 41, loss = 0.25143527\n",
      "Iteration 41, loss = 0.01512229\n",
      "Iteration 41, loss = 0.01470042\n",
      "Iteration 43, loss = 0.65150821\n",
      "Iteration 39, loss = 0.24098144\n",
      "Iteration 42, loss = 0.54553244\n",
      "Iteration 43, loss = 0.54461159\n",
      "Iteration 42, loss = 0.02333253\n",
      "Iteration 42, loss = 0.28657503\n",
      "Iteration 42, loss = 0.54469755\n",
      "Iteration 42, loss = 0.11954354\n",
      "Iteration 43, loss = 0.55452860\n",
      "Iteration 42, loss = 0.00800279\n",
      "Iteration 44, loss = 0.58956828\n",
      "Iteration 40, loss = 0.00752392\n",
      "Iteration 42, loss = 0.01518665\n",
      "Iteration 41, loss = 0.12948340\n",
      "Iteration 42, loss = 0.46256768\n",
      "Iteration 42, loss = 0.01449862\n",
      "Iteration 44, loss = 0.65082251\n",
      "Iteration 42, loss = 0.01406923\n",
      "Iteration 43, loss = 0.54491462\n",
      "Iteration 44, loss = 0.54333621\n",
      "Iteration 43, loss = 0.02285066\n",
      "Iteration 38, loss = 0.44650648\n",
      "Iteration 42, loss = 0.24912708\n",
      "Iteration 43, loss = 0.28487128\n",
      "Iteration 43, loss = 0.54427802\n",
      "Iteration 44, loss = 0.55380346\n",
      "Iteration 43, loss = 0.11842598\n",
      "Iteration 43, loss = 0.00768992\n",
      "Iteration 40, loss = 0.23422770\n",
      "Iteration 45, loss = 0.58884187\n",
      "Iteration 43, loss = 0.01460451\n",
      "Iteration 45, loss = 0.65026852\n",
      "Iteration 41, loss = 0.35677872\n",
      "Iteration 43, loss = 0.01348823\n",
      "Iteration 43, loss = 0.01403655\n",
      "Iteration 44, loss = 0.54438413\n",
      "Iteration 45, loss = 0.54242122\n",
      "Iteration 44, loss = 0.02196493\n",
      "Iteration 40, loss = 0.23913858\n",
      "Iteration 44, loss = 0.54374138\n",
      "Iteration 44, loss = 0.28321795\n",
      "Iteration 45, loss = 0.55284729\n",
      "Iteration 44, loss = 0.11720492\n",
      "Iteration 44, loss = 0.00747433\n",
      "Iteration 46, loss = 0.58799626\n",
      "Iteration 43, loss = 0.24711002\n",
      "Iteration 44, loss = 0.01421041\n",
      "Iteration 42, loss = 0.12833735\n",
      "Iteration 46, loss = 0.64961038\n",
      "Iteration 41, loss = 0.00727898\n",
      "Iteration 44, loss = 0.01299526\n",
      "Iteration 44, loss = 0.01342962\n",
      "Iteration 46, loss = 0.54131279\n",
      "Iteration 45, loss = 0.54367232\n",
      "Iteration 45, loss = 0.02133404\n",
      "Iteration 45, loss = 0.54317795\n",
      "Iteration 43, loss = 0.46138890\n",
      "Iteration 46, loss = 0.55182289\n",
      "Iteration 45, loss = 0.11612869\n",
      "Iteration 45, loss = 0.00729887\n",
      "Iteration 45, loss = 0.28212140\n",
      "Iteration 47, loss = 0.58726550\n",
      "Iteration 41, loss = 0.23246429\n",
      "Iteration 39, loss = 0.44501250\n",
      "Iteration 45, loss = 0.01367544\n",
      "Iteration 47, loss = 0.64902437\n",
      "Iteration 45, loss = 0.01261385\n",
      "Iteration 45, loss = 0.01297949\n",
      "Iteration 47, loss = 0.54016521\n",
      "Iteration 46, loss = 0.54306708\n",
      "Iteration 46, loss = 0.02084562\n",
      "Iteration 42, loss = 0.35504267\n",
      "Iteration 46, loss = 0.54254173\n",
      "Iteration 44, loss = 0.24503074\n",
      "Iteration 41, loss = 0.23717883\n",
      "Iteration 46, loss = 0.11509017\n",
      "Iteration 47, loss = 0.55103575\n",
      "Iteration 48, loss = 0.58644179\n",
      "Iteration 46, loss = 0.00708003\n",
      "Iteration 46, loss = 0.01326207\n",
      "Iteration 48, loss = 0.64828640\n",
      "Iteration 46, loss = 0.28034002\n",
      "Iteration 43, loss = 0.12699681\n",
      "Iteration 46, loss = 0.01251873\n",
      "Iteration 48, loss = 0.53925598\n",
      "Iteration 46, loss = 0.01269132\n",
      "Iteration 47, loss = 0.54243091\n",
      "Iteration 42, loss = 0.00702431\n",
      "Iteration 47, loss = 0.02019059\n",
      "Iteration 47, loss = 0.54217096\n",
      "Iteration 47, loss = 0.11392419\n",
      "Iteration 48, loss = 0.55038584\n",
      "Iteration 49, loss = 0.58574120\n",
      "Iteration 47, loss = 0.00690897\n",
      "Iteration 47, loss = 0.01292356\n",
      "Iteration 49, loss = 0.64778218\n",
      "Iteration 42, loss = 0.23079590\n",
      "Iteration 45, loss = 0.24334822\n",
      "Iteration 49, loss = 0.53792545\n",
      "Iteration 47, loss = 0.01197441\n",
      "Iteration 47, loss = 0.01226278\n",
      "Iteration 48, loss = 0.54198032\n",
      "Iteration 48, loss = 0.01962051\n",
      "Iteration 48, loss = 0.54163472\n",
      "Iteration 40, loss = 0.44349453\n",
      "Iteration 47, loss = 0.27901224\n",
      "Iteration 50, loss = 0.58482639\n",
      "Iteration 48, loss = 0.11285619\n",
      "Iteration 49, loss = 0.54953477\n",
      "Iteration 48, loss = 0.00677139\n",
      "Iteration 42, loss = 0.23521872\n",
      "Iteration 44, loss = 0.45989672\n",
      "Iteration 43, loss = 0.35307344\n",
      "Iteration 48, loss = 0.01246291\n",
      "Iteration 50, loss = 0.64731531\n",
      "Iteration 50, loss = 0.53723835\n",
      "Iteration 48, loss = 0.01158790\n",
      "Iteration 44, loss = 0.12578302\n",
      "Iteration 48, loss = 0.01188924\n",
      "Iteration 49, loss = 0.54130346\n",
      "Iteration 49, loss = 0.01896070\n",
      "Iteration 49, loss = 0.54123558\n",
      "Iteration 51, loss = 0.58416571\n",
      "Iteration 50, loss = 0.54855378\n",
      "Iteration 49, loss = 0.11186340\n",
      "Iteration 49, loss = 0.00663148\n",
      "Iteration 43, loss = 0.00682735\n",
      "Iteration 46, loss = 0.24152809\n",
      "Iteration 49, loss = 0.01219749Iteration 51, loss = 0.64676496\n",
      "\n",
      "Iteration 51, loss = 0.53633159\n",
      "Iteration 49, loss = 0.01138722\n",
      "Iteration 48, loss = 0.27748692\n",
      "Iteration 50, loss = 0.54055544\n",
      "Iteration 49, loss = 0.01168423\n",
      "Iteration 50, loss = 0.01840719\n",
      "Iteration 50, loss = 0.54071746\n",
      "Iteration 43, loss = 0.22943897\n",
      "Iteration 52, loss = 0.58365526\n",
      "Iteration 51, loss = 0.54784892\n",
      "Iteration 50, loss = 0.11089839\n",
      "Iteration 50, loss = 0.00647294\n",
      "Iteration 52, loss = 0.64620906\n",
      "Iteration 50, loss = 0.01183363\n",
      "Iteration 43, loss = 0.23373259\n",
      "Iteration 52, loss = 0.53540954\n",
      "Iteration 50, loss = 0.01108646\n",
      "Iteration 51, loss = 0.54000524\n",
      "Iteration 50, loss = 0.01134495\n",
      "Iteration 51, loss = 0.01776957\n",
      "Iteration 51, loss = 0.54022068\n",
      "Iteration 41, loss = 0.44183350\n",
      "Iteration 53, loss = 0.58268422\n",
      "Iteration 45, loss = 0.12466493\n",
      "Iteration 47, loss = 0.23962064\n",
      "Iteration 44, loss = 0.35140008\n",
      "Iteration 52, loss = 0.54704373\n",
      "Iteration 51, loss = 0.10994084\n",
      "Iteration 51, loss = 0.00632212\n",
      "Iteration 49, loss = 0.27618917\n",
      "Iteration 53, loss = 0.64556826\n",
      "Iteration 51, loss = 0.01153266\n",
      "Iteration 53, loss = 0.53468413\n",
      "Iteration 52, loss = 0.53934636\n",
      "Iteration 51, loss = 0.01074857\n",
      "Iteration 51, loss = 0.01107350\n",
      "Iteration 52, loss = 0.01719572\n",
      "Iteration 44, loss = 0.00662283\n",
      "Iteration 52, loss = 0.53982860\n",
      "Iteration 54, loss = 0.58233675\n",
      "Iteration 53, loss = 0.54656186\n",
      "Iteration 52, loss = 0.10899815\n",
      "Iteration 44, loss = 0.22795905\n",
      "Iteration 52, loss = 0.00622270\n",
      "Iteration 54, loss = 0.64487858\n",
      "Iteration 52, loss = 0.01122889\n",
      "Iteration 45, loss = 0.45863305\n",
      "Iteration 54, loss = 0.53373340\n",
      "Iteration 48, loss = 0.23810915\n",
      "Iteration 53, loss = 0.53882864\n",
      "Iteration 52, loss = 0.01044703\n",
      "Iteration 53, loss = 0.01678049\n",
      "Iteration 55, loss = 0.58158356\n",
      "Iteration 53, loss = 0.53936424\n",
      "Iteration 44, loss = 0.23216390\n",
      "Iteration 52, loss = 0.01074977\n",
      "Iteration 50, loss = 0.27470966\n",
      "Iteration 54, loss = 0.54584093\n",
      "Iteration 46, loss = 0.12347963\n",
      "Iteration 53, loss = 0.10812987\n",
      "Iteration 53, loss = 0.00608935\n",
      "Iteration 55, loss = 0.64436190\n",
      "Iteration 53, loss = 0.01091975\n",
      "Iteration 55, loss = 0.53283625\n",
      "Iteration 53, loss = 0.01021017\n",
      "Iteration 54, loss = 0.53817304\n",
      "Iteration 42, loss = 0.44057587\n",
      "Iteration 56, loss = 0.58116355\n",
      "Iteration 54, loss = 0.01670476\n",
      "Iteration 54, loss = 0.53896768\n",
      "Iteration 53, loss = 0.01052119\n",
      "Iteration 45, loss = 0.34944071\n",
      "Iteration 55, loss = 0.54518364\n",
      "Iteration 54, loss = 0.00597804\n",
      "Iteration 54, loss = 0.10726284\n",
      "Iteration 56, loss = 0.64371539\n",
      "Iteration 49, loss = 0.23642704\n",
      "Iteration 54, loss = 0.01077182\n",
      "Iteration 45, loss = 0.00648690\n",
      "Iteration 45, loss = 0.22627491\n",
      "Iteration 56, loss = 0.53232054\n",
      "Iteration 54, loss = 0.00996340\n",
      "Iteration 55, loss = 0.53745405\n",
      "Iteration 57, loss = 0.58049051\n",
      "Iteration 51, loss = 0.27362130\n",
      "Iteration 55, loss = 0.01586867\n",
      "Iteration 55, loss = 0.53835040\n",
      "Iteration 54, loss = 0.01025683\n",
      "Iteration 56, loss = 0.54460785\n",
      "Iteration 55, loss = 0.00594879\n",
      "Iteration 45, loss = 0.23041091\n",
      "Iteration 55, loss = 0.10650864\n",
      "Iteration 57, loss = 0.64309368\n",
      "Iteration 55, loss = 0.01044794\n",
      "Iteration 47, loss = 0.12222660\n",
      "Iteration 57, loss = 0.53149460\n",
      "Iteration 56, loss = 0.53720772\n",
      "Iteration 55, loss = 0.00970634\n",
      "Iteration 58, loss = 0.57965853\n",
      "Iteration 56, loss = 0.01557769\n",
      "Iteration 56, loss = 0.53819218\n",
      "Iteration 55, loss = 0.00996025\n",
      "Iteration 50, loss = 0.23489416\n",
      "Iteration 57, loss = 0.54383764\n",
      "Iteration 56, loss = 0.00582849\n",
      "Iteration 58, loss = 0.64273421\n",
      "Iteration 56, loss = 0.10547391\n",
      "Iteration 52, loss = 0.27240963\n",
      "Iteration 56, loss = 0.01023111\n",
      "Iteration 58, loss = 0.53047952\n",
      "Iteration 59, loss = 0.57889539\n",
      "Iteration 43, loss = 0.43921367\n",
      "Iteration 56, loss = 0.00944897\n",
      "Iteration 57, loss = 0.53654953\n",
      "Iteration 57, loss = 0.01518270\n",
      "Iteration 57, loss = 0.53761936\n",
      "Iteration 46, loss = 0.34796889\n",
      "Iteration 46, loss = 0.22498335\n",
      "Iteration 46, loss = 0.45746872\n",
      "Iteration 56, loss = 0.00964359\n",
      "Iteration 58, loss = 0.54341079\n",
      "Iteration 59, loss = 0.64203873\n",
      "Iteration 57, loss = 0.00573260\n",
      "Iteration 46, loss = 0.00627772\n",
      "Iteration 57, loss = 0.10494475\n",
      "Iteration 57, loss = 0.00997178\n",
      "Iteration 46, loss = 0.22872312\n",
      "Iteration 59, loss = 0.52982027\n",
      "Iteration 60, loss = 0.57852516\n",
      "Iteration 51, loss = 0.23341168\n",
      "Iteration 58, loss = 0.53593911\n",
      "Iteration 57, loss = 0.00921308\n",
      "Iteration 58, loss = 0.53707798\n",
      "Iteration 58, loss = 0.01480391\n",
      "Iteration 48, loss = 0.12109599\n",
      "Iteration 53, loss = 0.27116352\n",
      "Iteration 59, loss = 0.54264614\n",
      "Iteration 57, loss = 0.00941060\n",
      "Iteration 60, loss = 0.64154803\n",
      "Iteration 58, loss = 0.00569143\n",
      "Iteration 58, loss = 0.10405430\n",
      "Iteration 58, loss = 0.00981018\n",
      "Iteration 60, loss = 0.52948684\n",
      "Iteration 61, loss = 0.57788190\n",
      "Iteration 59, loss = 0.53538282\n",
      "Iteration 58, loss = 0.00900564\n",
      "Iteration 59, loss = 0.01448306\n",
      "Iteration 59, loss = 0.53669064\n",
      "Iteration 60, loss = 0.54189598\n",
      "Iteration 58, loss = 0.00915867\n",
      "Iteration 47, loss = 0.22354390\n",
      "Iteration 61, loss = 0.64089288\n",
      "Iteration 59, loss = 0.10355191\n",
      "Iteration 59, loss = 0.00563002\n",
      "Iteration 52, loss = 0.23199111\n",
      "Iteration 59, loss = 0.00977272\n",
      "Iteration 44, loss = 0.43764866\n",
      "Iteration 62, loss = 0.57710230\n",
      "Iteration 61, loss = 0.52838956\n",
      "Iteration 47, loss = 0.34638657\n",
      "Iteration 54, loss = 0.27009772\n",
      "Iteration 60, loss = 0.53497017\n",
      "Iteration 59, loss = 0.00879817\n",
      "Iteration 60, loss = 0.01412504\n",
      "Iteration 60, loss = 0.53626790\n",
      "Iteration 47, loss = 0.22712578\n",
      "Iteration 61, loss = 0.54127082\n",
      "Iteration 47, loss = 0.00615978\n",
      "Iteration 49, loss = 0.12006088\n",
      "Iteration 59, loss = 0.00890977\n",
      "Iteration 62, loss = 0.64031219\n",
      "Iteration 60, loss = 0.10265271\n",
      "Iteration 60, loss = 0.00558775\n",
      "Iteration 60, loss = 0.00942872\n",
      "Iteration 63, loss = 0.57676747\n",
      "Iteration 62, loss = 0.52759240\n",
      "Iteration 60, loss = 0.00862767\n",
      "Iteration 61, loss = 0.53455379\n",
      "Iteration 61, loss = 0.53600161\n",
      "Iteration 61, loss = 0.01372693\n",
      "Iteration 53, loss = 0.23032188\n",
      "Iteration 62, loss = 0.54070467\n",
      "Iteration 60, loss = 0.00863230\n",
      "Iteration 63, loss = 0.63994381\n",
      "Iteration 61, loss = 0.10202805\n",
      "Iteration 61, loss = 0.00551758\n",
      "Iteration 55, loss = 0.26910650\n",
      "Iteration 48, loss = 0.22234841\n",
      "Iteration 64, loss = 0.57607819\n",
      "Iteration 61, loss = 0.00925520\n",
      "Iteration 63, loss = 0.52703582\n",
      "Iteration 47, loss = 0.45625150\n",
      "Iteration 61, loss = 0.00855830\n",
      "Iteration 62, loss = 0.53390702\n",
      "Iteration 62, loss = 0.53541872\n",
      "Iteration 62, loss = 0.01339994\n",
      "Iteration 63, loss = 0.54016742\n",
      "Iteration 61, loss = 0.00844003\n",
      "Iteration 64, loss = 0.63946971\n",
      "Iteration 48, loss = 0.22562388\n",
      "Iteration 62, loss = 0.10136075\n",
      "Iteration 62, loss = 0.00545905\n",
      "Iteration 50, loss = 0.11915734\n",
      "Iteration 65, loss = 0.57573605\n",
      "Iteration 62, loss = 0.00905612\n",
      "Iteration 45, loss = 0.43638257\n",
      "Iteration 64, loss = 0.52645101\n",
      "Iteration 48, loss = 0.34486391\n",
      "Iteration 54, loss = 0.22910477\n",
      "Iteration 62, loss = 0.00823335\n",
      "Iteration 63, loss = 0.01312042\n",
      "Iteration 63, loss = 0.53507258\n",
      "Iteration 63, loss = 0.53334677\n",
      "Iteration 48, loss = 0.00603159\n",
      "Iteration 64, loss = 0.53948831\n",
      "Iteration 56, loss = 0.26782995\n",
      "Iteration 62, loss = 0.00825128\n",
      "Iteration 65, loss = 0.63883415\n",
      "Iteration 63, loss = 0.10060310\n",
      "Iteration 63, loss = 0.00542409\n",
      "Iteration 66, loss = 0.57508073\n",
      "Iteration 65, loss = 0.52567802\n",
      "Iteration 63, loss = 0.00888822\n",
      "Iteration 49, loss = 0.22100782\n",
      "Iteration 63, loss = 0.00812339\n",
      "Iteration 64, loss = 0.01285017\n",
      "Iteration 64, loss = 0.53468000\n",
      "Iteration 64, loss = 0.53270008\n",
      "Iteration 65, loss = 0.53885636\n",
      "Iteration 66, loss = 0.63825699\n",
      "Iteration 63, loss = 0.00806263\n",
      "Iteration 64, loss = 0.09987740\n",
      "Iteration 64, loss = 0.00534548\n",
      "Iteration 55, loss = 0.22778645\n",
      "Iteration 67, loss = 0.57468798\n",
      "Iteration 49, loss = 0.22431993\n",
      "Iteration 66, loss = 0.52541348\n",
      "Iteration 64, loss = 0.00874029\n",
      "Iteration 51, loss = 0.11812450\n",
      "Iteration 64, loss = 0.00790975\n",
      "Iteration 65, loss = 0.53434279\n",
      "Iteration 65, loss = 0.53249347\n",
      "Iteration 65, loss = 0.01273095\n",
      "Iteration 57, loss = 0.26710758\n",
      "Iteration 66, loss = 0.53844861\n",
      "Iteration 67, loss = 0.63783488\n",
      "Iteration 64, loss = 0.00792898\n",
      "Iteration 65, loss = 0.09922235\n",
      "Iteration 68, loss = 0.57408441\n",
      "Iteration 65, loss = 0.00529048\n",
      "Iteration 46, loss = 0.43494723\n",
      "Iteration 67, loss = 0.52476423\n",
      "Iteration 65, loss = 0.00854535\n",
      "Iteration 49, loss = 0.34341742\n",
      "Iteration 66, loss = 0.53202310\n",
      "Iteration 66, loss = 0.53400251\n",
      "Iteration 49, loss = 0.00590044\n",
      "Iteration 66, loss = 0.01243971\n",
      "Iteration 65, loss = 0.00781044\n",
      "Iteration 50, loss = 0.21982490\n",
      "Iteration 56, loss = 0.22642115\n",
      "Iteration 67, loss = 0.53794936\n",
      "Iteration 68, loss = 0.63713179\n",
      "Iteration 65, loss = 0.00777346\n",
      "Iteration 66, loss = 0.09852273\n",
      "Iteration 69, loss = 0.57359197\n",
      "Iteration 66, loss = 0.00531354\n",
      "Iteration 48, loss = 0.45546632\n",
      "Iteration 68, loss = 0.52382153\n",
      "Iteration 58, loss = 0.26608344\n",
      "Iteration 66, loss = 0.00835892\n",
      "Iteration 67, loss = 0.53141637\n",
      "Iteration 67, loss = 0.53377088\n",
      "Iteration 67, loss = 0.01225402\n",
      "Iteration 50, loss = 0.22297737\n",
      "Iteration 66, loss = 0.00772285\n",
      "Iteration 52, loss = 0.11709616\n",
      "Iteration 68, loss = 0.53723728\n",
      "Iteration 69, loss = 0.63659023\n",
      "Iteration 67, loss = 0.09793745\n",
      "Iteration 66, loss = 0.00771629\n",
      "Iteration 70, loss = 0.57342882\n",
      "Iteration 67, loss = 0.00522509\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 69, loss = 0.52341850\n",
      "Iteration 67, loss = 0.00819013\n",
      "Iteration 68, loss = 0.53314209\n",
      "Iteration 68, loss = 0.53105330\n",
      "Iteration 68, loss = 0.01205550\n",
      "Iteration 57, loss = 0.22516530\n",
      "Iteration 67, loss = 0.00759572\n",
      "Iteration 69, loss = 0.53737545\n",
      "Iteration 70, loss = 0.63620621\n",
      "Iteration 71, loss = 0.57252651\n",
      "Iteration 67, loss = 0.00747713\n",
      "Iteration 59, loss = 0.26506752\n",
      "Iteration 68, loss = 0.09719594\n",
      "Iteration 51, loss = 0.21860344\n",
      "Iteration 70, loss = 0.52278191\n",
      "Iteration 47, loss = 0.43393949\n",
      "Iteration 68, loss = 0.00804349\n",
      "Iteration 50, loss = 0.34204137\n",
      "Iteration 50, loss = 0.00579137\n",
      "Iteration 69, loss = 0.53271793\n",
      "Iteration 69, loss = 0.01191603\n",
      "Iteration 69, loss = 0.53055578\n",
      "Iteration 68, loss = 0.00745199\n",
      "Iteration 70, loss = 0.53635714\n",
      "Iteration 71, loss = 0.63562753\n",
      "Iteration 51, loss = 0.22156204\n",
      "Iteration 72, loss = 0.57178289\n",
      "Iteration 68, loss = 0.00737214\n",
      "Iteration 69, loss = 0.09668925\n",
      "Iteration 53, loss = 0.11615075\n",
      "Iteration 58, loss = 0.22400329\n",
      "Iteration 71, loss = 0.52236446\n",
      "Iteration 69, loss = 0.00790552\n",
      "Iteration 70, loss = 0.53220457\n",
      "Iteration 70, loss = 0.01172817\n",
      "Iteration 70, loss = 0.53003081\n",
      "Iteration 69, loss = 0.00736719\n",
      "Iteration 60, loss = 0.26416022\n",
      "Iteration 72, loss = 0.63538615\n",
      "Iteration 71, loss = 0.53597561\n",
      "Iteration 73, loss = 0.57188849\n",
      "Iteration 69, loss = 0.00725818\n",
      "Iteration 70, loss = 0.09607831\n",
      "Iteration 72, loss = 0.52182213\n",
      "Iteration 70, loss = 0.00772982\n",
      "Iteration 52, loss = 0.21722349\n",
      "Iteration 71, loss = 0.01172670\n",
      "Iteration 71, loss = 0.53195861\n",
      "Iteration 71, loss = 0.52964139\n",
      "Iteration 70, loss = 0.00724704\n",
      "Iteration 72, loss = 0.53541409\n",
      "Iteration 73, loss = 0.63472549\n",
      "Iteration 59, loss = 0.22260334\n",
      "Iteration 74, loss = 0.57118942\n",
      "Iteration 71, loss = 0.09545595\n",
      "Iteration 70, loss = 0.00711384\n",
      "Iteration 73, loss = 0.52129263\n",
      "Iteration 48, loss = 0.43269822\n",
      "Iteration 49, loss = 0.45420986\n",
      "Iteration 52, loss = 0.22014894\n",
      "Iteration 51, loss = 0.34049842\n",
      "Iteration 71, loss = 0.00759773\n",
      "Iteration 51, loss = 0.00571478\n",
      "Iteration 54, loss = 0.11520789\n",
      "Iteration 72, loss = 0.53161447\n",
      "Iteration 72, loss = 0.01138196\n",
      "Iteration 61, loss = 0.26335922\n",
      "Iteration 72, loss = 0.52912835\n",
      "Iteration 71, loss = 0.00719570\n",
      "Iteration 73, loss = 0.53482290\n",
      "Iteration 74, loss = 0.63408767\n",
      "Iteration 75, loss = 0.57075598\n",
      "Iteration 72, loss = 0.09479205\n",
      "Iteration 71, loss = 0.00723672\n",
      "Iteration 74, loss = 0.52062453\n",
      "Iteration 72, loss = 0.00750950\n",
      "Iteration 73, loss = 0.01118878\n",
      "Iteration 73, loss = 0.53147399\n",
      "Iteration 60, loss = 0.22136079\n",
      "Iteration 72, loss = 0.00700831\n",
      "Iteration 73, loss = 0.52880493\n",
      "Iteration 53, loss = 0.21622135\n",
      "Iteration 75, loss = 0.63350085\n",
      "Iteration 74, loss = 0.53436478\n",
      "Iteration 76, loss = 0.57042153\n",
      "Iteration 73, loss = 0.09441435\n",
      "Iteration 62, loss = 0.26220862\n",
      "Iteration 75, loss = 0.52013549\n",
      "Iteration 72, loss = 0.00696640\n",
      "Iteration 73, loss = 0.00752835\n",
      "Iteration 74, loss = 0.01101241\n",
      "Iteration 74, loss = 0.53132360\n",
      "Iteration 73, loss = 0.00697433\n",
      "Iteration 74, loss = 0.52827985\n",
      "Iteration 53, loss = 0.21875316\n",
      "Iteration 55, loss = 0.11419152\n",
      "Iteration 76, loss = 0.63307874\n",
      "Iteration 77, loss = 0.56974065\n",
      "Iteration 75, loss = 0.53398133\n",
      "Iteration 74, loss = 0.09378769\n",
      "Iteration 73, loss = 0.00687079\n",
      "Iteration 76, loss = 0.51980255\n",
      "Iteration 49, loss = 0.43142537\n",
      "Iteration 52, loss = 0.33922885\n",
      "Iteration 61, loss = 0.22033542\n",
      "Iteration 74, loss = 0.00740565\n",
      "Iteration 52, loss = 0.00559225\n",
      "Iteration 75, loss = 0.01077737\n",
      "Iteration 75, loss = 0.53084994\n",
      "Iteration 74, loss = 0.00688558\n",
      "Iteration 75, loss = 0.52779169\n",
      "Iteration 76, loss = 0.53376716\n",
      "Iteration 77, loss = 0.63254567\n",
      "Iteration 78, loss = 0.56963014\n",
      "Iteration 63, loss = 0.26128817\n",
      "Iteration 54, loss = 0.21524633\n",
      "Iteration 75, loss = 0.09336967\n",
      "Iteration 74, loss = 0.00676881\n",
      "Iteration 77, loss = 0.51940667\n",
      "Iteration 75, loss = 0.00724916\n",
      "Iteration 76, loss = 0.01065596\n",
      "Iteration 76, loss = 0.53023764\n",
      "Iteration 75, loss = 0.00723317\n",
      "Iteration 76, loss = 0.52739855\n",
      "Iteration 77, loss = 0.53314097\n",
      "Iteration 79, loss = 0.56896292\n",
      "Iteration 78, loss = 0.63210058\n",
      "Iteration 54, loss = 0.21752481\n",
      "Iteration 62, loss = 0.21887150\n",
      "Iteration 56, loss = 0.11340922\n",
      "Iteration 76, loss = 0.09262010\n",
      "Iteration 75, loss = 0.00670851\n",
      "Iteration 78, loss = 0.51874994\n",
      "Iteration 76, loss = 0.00714122\n",
      "Iteration 50, loss = 0.45305198\n",
      "Iteration 77, loss = 0.01045953\n",
      "Iteration 77, loss = 0.52993859\n",
      "Iteration 76, loss = 0.00682718\n",
      "Iteration 64, loss = 0.26023433\n",
      "Iteration 77, loss = 0.52709408\n",
      "Iteration 78, loss = 0.53276936\n",
      "Iteration 80, loss = 0.56865240\n",
      "Iteration 79, loss = 0.63161239\n",
      "Iteration 77, loss = 0.09201772\n",
      "Iteration 50, loss = 0.43036052\n",
      "Iteration 53, loss = 0.33757641\n",
      "Iteration 53, loss = 0.00550397\n",
      "Iteration 76, loss = 0.00664877\n",
      "Iteration 77, loss = 0.00705022\n",
      "Iteration 79, loss = 0.51870559\n",
      "Iteration 78, loss = 0.01029110\n",
      "Iteration 55, loss = 0.21405936\n",
      "Iteration 78, loss = 0.52961535\n",
      "Iteration 77, loss = 0.00673757\n",
      "Iteration 81, loss = 0.56830834\n",
      "Iteration 78, loss = 0.52662135\n",
      "Iteration 79, loss = 0.53233964\n",
      "Iteration 63, loss = 0.21785266\n",
      "Iteration 80, loss = 0.63125009\n",
      "Iteration 78, loss = 0.09166395\n",
      "Iteration 65, loss = 0.25951136\n",
      "Iteration 80, loss = 0.51780709\n",
      "Iteration 78, loss = 0.00696969\n",
      "Iteration 55, loss = 0.21623843\n",
      "Iteration 77, loss = 0.00650341\n",
      "Iteration 79, loss = 0.01004708\n",
      "Iteration 57, loss = 0.11252913\n",
      "Iteration 79, loss = 0.52947639\n",
      "Iteration 78, loss = 0.00672997\n",
      "Iteration 82, loss = 0.56766727\n",
      "Iteration 79, loss = 0.52614374\n",
      "Iteration 80, loss = 0.53178403\n",
      "Iteration 81, loss = 0.63093506\n",
      "Iteration 79, loss = 0.09105443\n",
      "Iteration 81, loss = 0.51750766\n",
      "Iteration 79, loss = 0.00689227\n",
      "Iteration 80, loss = 0.00999483\n",
      "Iteration 78, loss = 0.00642517\n",
      "Iteration 64, loss = 0.21684875\n",
      "Iteration 80, loss = 0.52891806\n",
      "Iteration 83, loss = 0.56736810\n",
      "Iteration 79, loss = 0.00668662\n",
      "Iteration 80, loss = 0.52567878\n",
      "Iteration 81, loss = 0.53149476\n",
      "Iteration 82, loss = 0.63004945\n",
      "Iteration 56, loss = 0.21302210\n",
      "Iteration 66, loss = 0.25865139\n",
      "Iteration 80, loss = 0.09061154\n",
      "Iteration 54, loss = 0.00543467\n",
      "Iteration 51, loss = 0.42917470\n",
      "Iteration 54, loss = 0.33629568\n",
      "Iteration 82, loss = 0.51673452\n",
      "Iteration 80, loss = 0.00682132\n",
      "Iteration 81, loss = 0.01009920\n",
      "Iteration 84, loss = 0.56714155\n",
      "Iteration 79, loss = 0.00635789\n",
      "Iteration 81, loss = 0.52867811\n",
      "Iteration 80, loss = 0.00661297\n",
      "Iteration 81, loss = 0.52530440\n",
      "Iteration 56, loss = 0.21510642\n",
      "Iteration 82, loss = 0.53099039\n",
      "Iteration 83, loss = 0.62979884\n",
      "Iteration 58, loss = 0.11156260\n",
      "Iteration 65, loss = 0.21572038\n",
      "Iteration 81, loss = 0.09003109\n",
      "Iteration 81, loss = 0.00674079\n",
      "Iteration 83, loss = 0.51653967\n",
      "Iteration 82, loss = 0.00980860\n",
      "Iteration 85, loss = 0.56628056\n",
      "Iteration 80, loss = 0.00626977\n",
      "Iteration 82, loss = 0.52859742\n",
      "Iteration 51, loss = 0.45215037\n",
      "Iteration 82, loss = 0.52495471\n",
      "Iteration 81, loss = 0.00660287\n",
      "Iteration 67, loss = 0.25774520\n",
      "Iteration 83, loss = 0.53074027\n",
      "Iteration 84, loss = 0.62925090\n",
      "Iteration 57, loss = 0.21219461\n",
      "Iteration 82, loss = 0.08962090\n",
      "Iteration 82, loss = 0.00660910\n",
      "Iteration 83, loss = 0.00957586\n",
      "Iteration 84, loss = 0.51584459\n",
      "Iteration 86, loss = 0.56597011\n",
      "Iteration 83, loss = 0.52822055\n",
      "Iteration 81, loss = 0.00641118\n",
      "Iteration 83, loss = 0.52456770\n",
      "Iteration 82, loss = 0.00649372\n",
      "Iteration 85, loss = 0.62885980\n",
      "Iteration 84, loss = 0.53032667\n",
      "Iteration 66, loss = 0.21461540\n",
      "Iteration 57, loss = 0.21409317\n",
      "Iteration 59, loss = 0.11086301\n",
      "Iteration 55, loss = 0.00534502\n",
      "Iteration 55, loss = 0.33475540\n",
      "Iteration 83, loss = 0.08920738\n",
      "Iteration 52, loss = 0.42784392\n",
      "Iteration 83, loss = 0.00652483\n",
      "Iteration 84, loss = 0.00951777\n",
      "Iteration 85, loss = 0.51602607\n",
      "Iteration 87, loss = 0.56558924\n",
      "Iteration 68, loss = 0.25688371\n",
      "Iteration 82, loss = 0.00614816\n",
      "Iteration 84, loss = 0.52765391\n",
      "Iteration 84, loss = 0.52403030\n",
      "Iteration 83, loss = 0.00643776\n",
      "Iteration 86, loss = 0.62820999\n",
      "Iteration 85, loss = 0.52987467\n",
      "Iteration 84, loss = 0.08871701\n",
      "Iteration 84, loss = 0.00653027\n",
      "Iteration 85, loss = 0.00940820\n",
      "Iteration 88, loss = 0.56511123\n",
      "Iteration 86, loss = 0.51527986\n",
      "Iteration 85, loss = 0.52729726\n",
      "Iteration 83, loss = 0.00609761\n",
      "Iteration 58, loss = 0.21089460\n",
      "Iteration 67, loss = 0.21392197\n",
      "Iteration 85, loss = 0.52382584\n",
      "Iteration 87, loss = 0.62768133\n",
      "Iteration 84, loss = 0.00640091\n",
      "Iteration 86, loss = 0.52932397\n",
      "Iteration 69, loss = 0.25608036\n",
      "Iteration 85, loss = 0.00638983\n",
      "Iteration 85, loss = 0.08835000\n",
      "Iteration 86, loss = 0.00930912\n",
      "Iteration 87, loss = 0.51455309\n",
      "Iteration 89, loss = 0.56465297\n",
      "Iteration 86, loss = 0.52692651\n",
      "Iteration 58, loss = 0.21282350\n",
      "Iteration 60, loss = 0.10994551\n",
      "Iteration 84, loss = 0.00611294\n",
      "Iteration 86, loss = 0.52313436\n",
      "Iteration 88, loss = 0.62739667\n",
      "Iteration 85, loss = 0.00630779\n",
      "Iteration 87, loss = 0.52930437\n",
      "Iteration 56, loss = 0.00528342\n",
      "Iteration 68, loss = 0.21270999\n",
      "Iteration 56, loss = 0.33382838\n",
      "Iteration 86, loss = 0.00634136\n",
      "Iteration 90, loss = 0.56450061\n",
      "Iteration 53, loss = 0.42664465\n",
      "Iteration 88, loss = 0.51411495\n",
      "Iteration 87, loss = 0.00923894\n",
      "Iteration 86, loss = 0.08779995\n",
      "Iteration 87, loss = 0.52689177\n",
      "Iteration 85, loss = 0.00611383\n",
      "Iteration 87, loss = 0.52309223\n",
      "Iteration 86, loss = 0.00626974\n",
      "Iteration 89, loss = 0.62691340\n",
      "Iteration 52, loss = 0.45107153\n",
      "Iteration 70, loss = 0.25526995\n",
      "Iteration 88, loss = 0.52879636\n",
      "Iteration 59, loss = 0.21017122\n",
      "Iteration 87, loss = 0.00629690\n",
      "Iteration 91, loss = 0.56399370\n",
      "Iteration 87, loss = 0.08737197\n",
      "Iteration 89, loss = 0.51381347\n",
      "Iteration 88, loss = 0.00912521\n",
      "Iteration 88, loss = 0.52646415\n",
      "Iteration 86, loss = 0.00597954\n",
      "Iteration 88, loss = 0.52277289\n",
      "Iteration 87, loss = 0.00630976\n",
      "Iteration 90, loss = 0.62650189\n",
      "Iteration 61, loss = 0.10942262\n",
      "Iteration 59, loss = 0.21171039\n",
      "Iteration 89, loss = 0.52834169\n",
      "Iteration 69, loss = 0.21157267\n",
      "Iteration 92, loss = 0.56357853\n",
      "Iteration 90, loss = 0.51357163\n",
      "Iteration 88, loss = 0.00624910\n",
      "Iteration 88, loss = 0.08697736\n",
      "Iteration 89, loss = 0.00904433\n",
      "Iteration 89, loss = 0.52638703\n",
      "Iteration 71, loss = 0.25487054\n",
      "Iteration 87, loss = 0.00591465\n",
      "Iteration 89, loss = 0.52235872\n",
      "Iteration 88, loss = 0.00623896\n",
      "Iteration 91, loss = 0.62629160\n",
      "Iteration 90, loss = 0.52802863\n",
      "Iteration 57, loss = 0.00526457\n",
      "Iteration 57, loss = 0.33255397\n",
      "Iteration 60, loss = 0.20921781\n",
      "Iteration 93, loss = 0.56348595\n",
      "Iteration 89, loss = 0.08657622\n",
      "Iteration 90, loss = 0.00894740\n",
      "Iteration 91, loss = 0.51296756\n",
      "Iteration 54, loss = 0.42592055\n",
      "Iteration 89, loss = 0.00615572\n",
      "Iteration 90, loss = 0.52606912\n",
      "Iteration 88, loss = 0.00589500\n",
      "Iteration 70, loss = 0.21074528\n",
      "Iteration 90, loss = 0.52185874\n",
      "Iteration 89, loss = 0.00612408\n",
      "Iteration 92, loss = 0.62557990\n",
      "Iteration 91, loss = 0.52776186\n",
      "Iteration 62, loss = 0.10835308\n",
      "Iteration 60, loss = 0.21066605\n",
      "Iteration 94, loss = 0.56281783\n",
      "Iteration 72, loss = 0.25382139\n",
      "Iteration 90, loss = 0.08628085\n",
      "Iteration 91, loss = 0.00876608\n",
      "Iteration 92, loss = 0.51245449Iteration 91, loss = 0.52580615\n",
      "\n",
      "Iteration 90, loss = 0.00617832\n",
      "Iteration 89, loss = 0.00584562\n",
      "Iteration 90, loss = 0.00612515\n",
      "Iteration 91, loss = 0.52156004\n",
      "Iteration 93, loss = 0.62521464\n",
      "Iteration 92, loss = 0.52726513\n",
      "Iteration 95, loss = 0.56245356\n",
      "Iteration 92, loss = 0.00870258\n",
      "Iteration 91, loss = 0.08562310\n",
      "Iteration 71, loss = 0.20961931\n",
      "Iteration 92, loss = 0.52528164\n",
      "Iteration 93, loss = 0.51225503\n",
      "Iteration 91, loss = 0.00608072\n",
      "Iteration 90, loss = 0.00579145\n",
      "Iteration 61, loss = 0.20825029\n",
      "Iteration 94, loss = 0.62457729\n",
      "Iteration 91, loss = 0.00608385\n",
      "Iteration 92, loss = 0.52129696\n",
      "Iteration 93, loss = 0.52723457\n",
      "Iteration 73, loss = 0.25341951\n",
      "Iteration 58, loss = 0.00515804\n",
      "Iteration 53, loss = 0.44986939\n",
      "Iteration 58, loss = 0.33144478\n",
      "Iteration 96, loss = 0.56237996\n",
      "Iteration 92, loss = 0.08522580\n",
      "Iteration 93, loss = 0.00855685\n",
      "Iteration 55, loss = 0.42470254\n",
      "Iteration 94, loss = 0.51213262\n",
      "Iteration 93, loss = 0.52522127\n",
      "Iteration 63, loss = 0.10774470\n",
      "Iteration 92, loss = 0.00621334\n",
      "Iteration 61, loss = 0.20948241\n",
      "Iteration 91, loss = 0.00586694\n",
      "Iteration 95, loss = 0.62425261\n",
      "Iteration 92, loss = 0.00602541\n",
      "Iteration 93, loss = 0.52093822\n",
      "Iteration 94, loss = 0.52675840\n",
      "Iteration 72, loss = 0.20900971\n",
      "Iteration 97, loss = 0.56168507\n",
      "Iteration 93, loss = 0.08513929\n",
      "Iteration 94, loss = 0.00848225\n",
      "Iteration 94, loss = 0.52497057\n",
      "Iteration 95, loss = 0.51111464\n",
      "Iteration 93, loss = 0.00603185\n",
      "Iteration 92, loss = 0.00582110\n",
      "Iteration 74, loss = 0.25229428\n",
      "Iteration 96, loss = 0.62403918\n",
      "Iteration 93, loss = 0.00604308\n",
      "Iteration 94, loss = 0.52050008\n",
      "Iteration 95, loss = 0.52672029\n",
      "Iteration 62, loss = 0.20746874\n",
      "Iteration 98, loss = 0.56130756\n",
      "Iteration 94, loss = 0.08461335\n",
      "Iteration 95, loss = 0.00831662\n",
      "Iteration 96, loss = 0.51138732\n",
      "Iteration 95, loss = 0.52449291\n",
      "Iteration 94, loss = 0.00600273\n",
      "Iteration 93, loss = 0.00571784\n",
      "Iteration 97, loss = 0.62341628\n",
      "Iteration 64, loss = 0.10693894\n",
      "Iteration 94, loss = 0.00596294\n",
      "Iteration 95, loss = 0.51983899\n",
      "Iteration 73, loss = 0.20793573\n",
      "Iteration 96, loss = 0.52588466\n",
      "Iteration 62, loss = 0.20828335\n",
      "Iteration 59, loss = 0.00509715\n",
      "Iteration 99, loss = 0.56092169\n",
      "Iteration 59, loss = 0.33016918\n",
      "Iteration 95, loss = 0.08420164\n",
      "Iteration 75, loss = 0.25183695\n",
      "Iteration 56, loss = 0.42375704\n",
      "Iteration 96, loss = 0.00841075\n",
      "Iteration 97, loss = 0.51049329\n",
      "Iteration 96, loss = 0.52441756\n",
      "Iteration 95, loss = 0.00596834\n",
      "Iteration 94, loss = 0.00569210\n",
      "Iteration 98, loss = 0.62287997\n",
      "Iteration 96, loss = 0.51966779\n",
      "Iteration 95, loss = 0.00598173\n",
      "Iteration 97, loss = 0.52584557\n",
      "Iteration 100, loss = 0.56067108\n",
      "Iteration 63, loss = 0.20673856\n",
      "Iteration 97, loss = 0.00816936\n",
      "Iteration 96, loss = 0.08375950\n",
      "Iteration 98, loss = 0.51041406\n",
      "Iteration 97, loss = 0.52410487\n",
      "Iteration 96, loss = 0.00594287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 74, loss = 0.20720470\n",
      "Iteration 99, loss = 0.62288599\n",
      "Iteration 95, loss = 0.00565764\n",
      "Iteration 97, loss = 0.51930270\n",
      "Iteration 96, loss = 0.00596000\n",
      "Iteration 98, loss = 0.52558743\n",
      "Iteration 76, loss = 0.25072482\n",
      "Iteration 101, loss = 0.56045902\n",
      "Iteration 65, loss = 0.10619003\n",
      "Iteration 98, loss = 0.00810837\n",
      "Iteration 98, loss = 0.52378326\n",
      "Iteration 97, loss = 0.08350850\n",
      "Iteration 63, loss = 0.20736806\n",
      "Iteration 99, loss = 0.50984896\n",
      "Iteration 54, loss = 0.44907844\n",
      "Iteration 100, loss = 0.62240645\n",
      "Iteration 98, loss = 0.51893782\n",
      "Iteration 96, loss = 0.00580213\n",
      "Iteration 97, loss = 0.00594708\n",
      "Iteration 99, loss = 0.52514987\n",
      "Iteration 102, loss = 0.56025835\n",
      "Iteration 60, loss = 0.00503728\n",
      "Iteration 60, loss = 0.32875430\n",
      "Iteration 75, loss = 0.20599372\n",
      "Iteration 99, loss = 0.00808524\n",
      "Iteration 57, loss = 0.42255610\n",
      "Iteration 98, loss = 0.08298702\n",
      "Iteration 100, loss = 0.50999637\n",
      "Iteration 99, loss = 0.52369130\n",
      "Iteration 64, loss = 0.20579738\n",
      "Iteration 101, loss = 0.62176895\n",
      "Iteration 77, loss = 0.25023122\n",
      "Iteration 99, loss = 0.51856352\n",
      "Iteration 100, loss = 0.52478905\n",
      "Iteration 97, loss = 0.00566598\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 98, loss = 0.00585770\n",
      "Iteration 103, loss = 0.55973501\n",
      "Iteration 101, loss = 0.50920363\n",
      "Iteration 100, loss = 0.00798770\n",
      "Iteration 100, loss = 0.52351510\n",
      "Iteration 99, loss = 0.08264546\n",
      "Iteration 66, loss = 0.10547219\n",
      "Iteration 102, loss = 0.62157950\n",
      "Iteration 100, loss = 0.51836342\n",
      "Iteration 64, loss = 0.20645041\n",
      "Iteration 99, loss = 0.00587602\n",
      "Iteration 101, loss = 0.52467794\n",
      "Iteration 76, loss = 0.20536881\n",
      "Iteration 104, loss = 0.55940174\n",
      "Iteration 102, loss = 0.50901753\n",
      "Iteration 101, loss = 0.00786923\n",
      "Iteration 101, loss = 0.52303548\n",
      "Iteration 100, loss = 0.08228216\n",
      "Iteration 78, loss = 0.24966726\n",
      "Iteration 103, loss = 0.62078261\n",
      "Iteration 101, loss = 0.51786598\n",
      "Iteration 100, loss = 0.00582863\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 102, loss = 0.52428795\n",
      "Iteration 65, loss = 0.20501036\n",
      "Iteration 105, loss = 0.55899902\n",
      "Iteration 61, loss = 0.00500317\n",
      "Iteration 61, loss = 0.32782997\n",
      "Iteration 103, loss = 0.50858765\n",
      "Iteration 102, loss = 0.52269229\n",
      "Iteration 102, loss = 0.00783074\n",
      "Iteration 58, loss = 0.42183794\n",
      "Iteration 101, loss = 0.08203034\n",
      "Iteration 77, loss = 0.20467683\n",
      "Iteration 104, loss = 0.62073053\n",
      "Iteration 102, loss = 0.51757164\n",
      "Iteration 103, loss = 0.52396245\n",
      "Iteration 67, loss = 0.10486743\n",
      "Iteration 106, loss = 0.55875189\n",
      "Iteration 79, loss = 0.24873097\n",
      "Iteration 65, loss = 0.20546779\n",
      "Iteration 104, loss = 0.50799272\n",
      "Iteration 103, loss = 0.52256729\n",
      "Iteration 103, loss = 0.00784923\n",
      "Iteration 102, loss = 0.08153620\n",
      "Iteration 105, loss = 0.62031885\n",
      "Iteration 103, loss = 0.51698432\n",
      "Iteration 104, loss = 0.52393476\n",
      "Iteration 107, loss = 0.55828759\n",
      "Iteration 55, loss = 0.44823816\n",
      "Iteration 105, loss = 0.50772905\n",
      "Iteration 104, loss = 0.52232400\n",
      "Iteration 104, loss = 0.00792499\n",
      "Iteration 78, loss = 0.20380247\n",
      "Iteration 66, loss = 0.20402234\n",
      "Iteration 103, loss = 0.08121531\n",
      "Iteration 104, loss = 0.51687037\n",
      "Iteration 106, loss = 0.61971331\n",
      "Iteration 105, loss = 0.52354594\n",
      "Iteration 80, loss = 0.24817216\n",
      "Iteration 108, loss = 0.55818061\n",
      "Iteration 62, loss = 0.00496720\n",
      "Iteration 106, loss = 0.50764569\n",
      "Iteration 105, loss = 0.52196127\n",
      "Iteration 62, loss = 0.32681269\n",
      "Iteration 68, loss = 0.10409205\n",
      "Iteration 105, loss = 0.00761058\n",
      "Iteration 104, loss = 0.08109160\n",
      "Iteration 59, loss = 0.42071528\n",
      "Iteration 66, loss = 0.20460269\n",
      "Iteration 107, loss = 0.61974180\n",
      "Iteration 105, loss = 0.51648138\n",
      "Iteration 106, loss = 0.52325416\n",
      "Iteration 109, loss = 0.55793248\n",
      "Iteration 79, loss = 0.20319099\n",
      "Iteration 107, loss = 0.50765853\n",
      "Iteration 106, loss = 0.52150662\n",
      "Iteration 106, loss = 0.00756750\n",
      "Iteration 105, loss = 0.08072469\n",
      "Iteration 81, loss = 0.24772446\n",
      "Iteration 108, loss = 0.61887868\n",
      "Iteration 67, loss = 0.20333345\n",
      "Iteration 106, loss = 0.51611146\n",
      "Iteration 110, loss = 0.55757452\n",
      "Iteration 108, loss = 0.50659849\n",
      "Iteration 107, loss = 0.52287454\n",
      "Iteration 107, loss = 0.52200646\n",
      "Iteration 107, loss = 0.00755669\n",
      "Iteration 106, loss = 0.08035053\n",
      "Iteration 107, loss = 0.51587678\n",
      "Iteration 69, loss = 0.10363733\n",
      "Iteration 109, loss = 0.61878119\n",
      "Iteration 80, loss = 0.20225383\n",
      "Iteration 111, loss = 0.55716083\n",
      "Iteration 67, loss = 0.20349937\n",
      "Iteration 109, loss = 0.50663260\n",
      "Iteration 63, loss = 0.00492363\n",
      "Iteration 108, loss = 0.52107189\n",
      "Iteration 63, loss = 0.32565601\n",
      "Iteration 107, loss = 0.08006642\n",
      "Iteration 82, loss = 0.24697397\n",
      "Iteration 108, loss = 0.00748336\n",
      "Iteration 108, loss = 0.52269297\n",
      "Iteration 60, loss = 0.41969231\n",
      "Iteration 110, loss = 0.61807956\n",
      "Iteration 108, loss = 0.51538860\n",
      "Iteration 112, loss = 0.55715912\n",
      "Iteration 110, loss = 0.50650624\n",
      "Iteration 68, loss = 0.20247143\n",
      "Iteration 109, loss = 0.52088902\n",
      "Iteration 108, loss = 0.07951870\n",
      "Iteration 109, loss = 0.00755239\n",
      "Iteration 81, loss = 0.20152478\n",
      "Iteration 111, loss = 0.61787162\n",
      "Iteration 109, loss = 0.51507514\n",
      "Iteration 113, loss = 0.55661556\n",
      "Iteration 56, loss = 0.44718315\n",
      "Iteration 109, loss = 0.52260550\n",
      "Iteration 111, loss = 0.50574093\n",
      "Iteration 83, loss = 0.24629843\n",
      "Iteration 70, loss = 0.10270236\n",
      "Iteration 109, loss = 0.07938400\n",
      "Iteration 110, loss = 0.52067247\n",
      "Iteration 110, loss = 0.00751036\n",
      "Iteration 68, loss = 0.20273343\n",
      "Iteration 112, loss = 0.61747281\n",
      "Iteration 110, loss = 0.51483671\n",
      "Iteration 114, loss = 0.55610780\n",
      "Iteration 112, loss = 0.50529717\n",
      "Iteration 82, loss = 0.20084865\n",
      "Iteration 64, loss = 0.00487500\n",
      "Iteration 111, loss = 0.00741329\n",
      "Iteration 111, loss = 0.52032379\n",
      "Iteration 110, loss = 0.07910441\n",
      "Iteration 64, loss = 0.32459927\n",
      "Iteration 110, loss = 0.52217540\n",
      "Iteration 69, loss = 0.20192341\n",
      "Iteration 113, loss = 0.61696771\n",
      "Iteration 115, loss = 0.55607026\n",
      "Iteration 61, loss = 0.41876211\n",
      "Iteration 84, loss = 0.24575752\n",
      "Iteration 111, loss = 0.51444383\n",
      "Iteration 113, loss = 0.50529734\n",
      "Iteration 112, loss = 0.00738448\n",
      "Iteration 111, loss = 0.07865568\n",
      "Iteration 112, loss = 0.52020996\n",
      "Iteration 71, loss = 0.10218844\n",
      "Iteration 116, loss = 0.55566431\n",
      "Iteration 114, loss = 0.61703555\n",
      "Iteration 112, loss = 0.51403973\n",
      "Iteration 114, loss = 0.50495038\n",
      "Iteration 83, loss = 0.20010028\n",
      "Iteration 69, loss = 0.20192539\n",
      "Iteration 111, loss = 0.52200158\n",
      "Iteration 113, loss = 0.00734199\n",
      "Iteration 112, loss = 0.07870399\n",
      "Iteration 113, loss = 0.52016416\n",
      "Iteration 85, loss = 0.24538855\n",
      "Iteration 115, loss = 0.61625404\n",
      "Iteration 117, loss = 0.55571678\n",
      "Iteration 113, loss = 0.51354873\n",
      "Iteration 70, loss = 0.20108149\n",
      "Iteration 115, loss = 0.50487449\n",
      "Iteration 114, loss = 0.00731714\n",
      "Iteration 113, loss = 0.07820167\n",
      "Iteration 65, loss = 0.00480284\n",
      "Iteration 114, loss = 0.51996231\n",
      "Iteration 65, loss = 0.32371613\n",
      "Iteration 118, loss = 0.55489013\n",
      "Iteration 116, loss = 0.61604790\n",
      "Iteration 114, loss = 0.51352552\n",
      "Iteration 112, loss = 0.52170391\n",
      "Iteration 84, loss = 0.19953832\n",
      "Iteration 62, loss = 0.41796152\n",
      "Iteration 116, loss = 0.50430623\n",
      "Iteration 72, loss = 0.10155020\n",
      "Iteration 115, loss = 0.00729244\n",
      "Iteration 86, loss = 0.24470560\n",
      "Iteration 114, loss = 0.07780354\n",
      "Iteration 115, loss = 0.51952938\n",
      "Iteration 70, loss = 0.20097757\n",
      "Iteration 119, loss = 0.55468699\n",
      "Iteration 117, loss = 0.61556609\n",
      "Iteration 57, loss = 0.44645830\n",
      "Iteration 115, loss = 0.51296852\n",
      "Iteration 117, loss = 0.50414376\n",
      "Iteration 116, loss = 0.00743654\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 113, loss = 0.52154204\n",
      "Iteration 116, loss = 0.51965689\n",
      "Iteration 115, loss = 0.07770682\n",
      "Iteration 71, loss = 0.20046095\n",
      "Iteration 120, loss = 0.55429502\n",
      "Iteration 85, loss = 0.19835545\n",
      "Iteration 118, loss = 0.61597641\n",
      "Iteration 116, loss = 0.51268004\n",
      "Iteration 118, loss = 0.50360819\n",
      "Iteration 87, loss = 0.24396222\n",
      "Iteration 117, loss = 0.51923959\n",
      "Iteration 116, loss = 0.07719918\n",
      "Iteration 73, loss = 0.10100302\n",
      "Iteration 121, loss = 0.55450678\n",
      "Iteration 66, loss = 0.00479312\n",
      "Iteration 119, loss = 0.61461818\n",
      "Iteration 66, loss = 0.32254092\n",
      "Iteration 117, loss = 0.51260781\n",
      "Iteration 71, loss = 0.20003270\n",
      "Iteration 114, loss = 0.52138990\n",
      "Iteration 63, loss = 0.41704876\n",
      "Iteration 119, loss = 0.50330663\n",
      "Iteration 117, loss = 0.07707497\n",
      "Iteration 118, loss = 0.51898515\n",
      "Iteration 86, loss = 0.19781344\n",
      "Iteration 122, loss = 0.55449017\n",
      "Iteration 120, loss = 0.61469076\n",
      "Iteration 118, loss = 0.51207255\n",
      "Iteration 88, loss = 0.24341187\n",
      "Iteration 72, loss = 0.19994046\n",
      "Iteration 120, loss = 0.50347143\n",
      "Iteration 119, loss = 0.51897297\n",
      "Iteration 123, loss = 0.55382622\n",
      "Iteration 118, loss = 0.07681602\n",
      "Iteration 121, loss = 0.61420279\n",
      "Iteration 119, loss = 0.51194522\n",
      "Iteration 115, loss = 0.52094727\n",
      "Iteration 74, loss = 0.10043973\n",
      "Iteration 121, loss = 0.50264964\n",
      "Iteration 87, loss = 0.19714526\n",
      "Iteration 120, loss = 0.51843562\n",
      "Iteration 124, loss = 0.55353502\n",
      "Iteration 119, loss = 0.07644159\n",
      "Iteration 89, loss = 0.24292246\n",
      "Iteration 72, loss = 0.19913460\n",
      "Iteration 122, loss = 0.61399398\n",
      "Iteration 67, loss = 0.00474372\n",
      "Iteration 120, loss = 0.51180918\n",
      "Iteration 67, loss = 0.32152263\n",
      "Iteration 122, loss = 0.50244166\n",
      "Iteration 121, loss = 0.51815193\n",
      "Iteration 64, loss = 0.41583832\n",
      "Iteration 125, loss = 0.55307312\n",
      "Iteration 73, loss = 0.19888868\n",
      "Iteration 120, loss = 0.07624714\n",
      "Iteration 116, loss = 0.52080395\n",
      "Iteration 58, loss = 0.44549716\n",
      "Iteration 123, loss = 0.61360301\n",
      "Iteration 88, loss = 0.19640845\n",
      "Iteration 121, loss = 0.51098472\n",
      "Iteration 123, loss = 0.50244198\n",
      "Iteration 122, loss = 0.51830331\n",
      "Iteration 126, loss = 0.55317398\n",
      "Iteration 90, loss = 0.24208533\n",
      "Iteration 75, loss = 0.09970378\n",
      "Iteration 121, loss = 0.07615391\n",
      "Iteration 124, loss = 0.61344812\n",
      "Iteration 122, loss = 0.51085660\n",
      "Iteration 124, loss = 0.50259476\n",
      "Iteration 73, loss = 0.19831311\n",
      "Iteration 117, loss = 0.52055251\n",
      "Iteration 123, loss = 0.51803384\n",
      "Iteration 127, loss = 0.55322675\n",
      "Iteration 122, loss = 0.07572299\n",
      "Iteration 89, loss = 0.19575854\n",
      "Iteration 125, loss = 0.61267712\n",
      "Iteration 68, loss = 0.00472082\n",
      "Iteration 123, loss = 0.51081386\n",
      "Iteration 74, loss = 0.19825905\n",
      "Iteration 125, loss = 0.50174995\n",
      "Iteration 68, loss = 0.32065043\n",
      "Iteration 91, loss = 0.24162391\n",
      "Iteration 128, loss = 0.55267922\n",
      "Iteration 124, loss = 0.51755976\n",
      "Iteration 123, loss = 0.07547609\n",
      "Iteration 65, loss = 0.41536459\n",
      "Iteration 126, loss = 0.61264111\n",
      "Iteration 76, loss = 0.09920472\n",
      "Iteration 118, loss = 0.52047174\n",
      "Iteration 124, loss = 0.51040915\n",
      "Iteration 126, loss = 0.50165979\n",
      "Iteration 129, loss = 0.55231414\n",
      "Iteration 125, loss = 0.51750136\n",
      "Iteration 90, loss = 0.19527404\n",
      "Iteration 124, loss = 0.07512722\n",
      "Iteration 127, loss = 0.61229998\n",
      "Iteration 74, loss = 0.19751954\n",
      "Iteration 92, loss = 0.24109706\n",
      "Iteration 127, loss = 0.50150507\n",
      "Iteration 125, loss = 0.51000821\n",
      "Iteration 130, loss = 0.55211600\n",
      "Iteration 126, loss = 0.51705686\n",
      "Iteration 75, loss = 0.19777232\n",
      "Iteration 125, loss = 0.07485167\n",
      "Iteration 128, loss = 0.61215495\n",
      "Iteration 119, loss = 0.52013003\n",
      "Iteration 69, loss = 0.00467159\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 131, loss = 0.55187125\n",
      "Iteration 126, loss = 0.51001993\n",
      "Iteration 128, loss = 0.50111950\n",
      "Iteration 91, loss = 0.19444969\n",
      "Iteration 127, loss = 0.51716561\n",
      "Iteration 69, loss = 0.31972021\n",
      "Iteration 77, loss = 0.09866895\n",
      "Iteration 126, loss = 0.07485616\n",
      "Iteration 59, loss = 0.44458679\n",
      "Iteration 93, loss = 0.24080062\n",
      "Iteration 129, loss = 0.61151683\n",
      "Iteration 66, loss = 0.41431492\n",
      "Iteration 75, loss = 0.19679187\n",
      "Iteration 127, loss = 0.50961974\n",
      "Iteration 132, loss = 0.55153777\n",
      "Iteration 128, loss = 0.51669329\n",
      "Iteration 129, loss = 0.50078808\n",
      "Iteration 120, loss = 0.51981932\n",
      "Iteration 127, loss = 0.07447826\n",
      "Iteration 130, loss = 0.61134500\n",
      "Iteration 76, loss = 0.19732788\n",
      "Iteration 92, loss = 0.19385401\n",
      "Iteration 133, loss = 0.55125804\n",
      "Iteration 129, loss = 0.51655230\n",
      "Iteration 128, loss = 0.50923181\n",
      "Iteration 130, loss = 0.50039873\n",
      "Iteration 94, loss = 0.24005398\n",
      "Iteration 128, loss = 0.07406210\n",
      "Iteration 131, loss = 0.61133263\n",
      "Iteration 78, loss = 0.09807325\n",
      "Iteration 130, loss = 0.51636167\n",
      "Iteration 121, loss = 0.51981255\n",
      "Iteration 134, loss = 0.55129093\n",
      "Iteration 129, loss = 0.50898805\n",
      "Iteration 131, loss = 0.50035004\n",
      "Iteration 76, loss = 0.19590218\n",
      "Iteration 70, loss = 0.31871473\n",
      "Iteration 129, loss = 0.07415222\n",
      "Iteration 132, loss = 0.61075888\n",
      "Iteration 93, loss = 0.19332234\n",
      "Iteration 67, loss = 0.41364194\n",
      "Iteration 131, loss = 0.51629679\n",
      "Iteration 135, loss = 0.55049216\n",
      "Iteration 132, loss = 0.50031466\n",
      "Iteration 130, loss = 0.50873888\n",
      "Iteration 95, loss = 0.23989891\n",
      "Iteration 77, loss = 0.19638664\n",
      "Iteration 130, loss = 0.07356508\n",
      "Iteration 133, loss = 0.61014085\n",
      "Iteration 122, loss = 0.51939213\n",
      "Iteration 132, loss = 0.51589679\n",
      "Iteration 136, loss = 0.55037703\n",
      "Iteration 131, loss = 0.50859978\n",
      "Iteration 133, loss = 0.49986945\n",
      "Iteration 79, loss = 0.09776170\n",
      "Iteration 94, loss = 0.19266315\n",
      "Iteration 131, loss = 0.07365055\n",
      "Iteration 134, loss = 0.61036058\n",
      "Iteration 96, loss = 0.23907877\n",
      "Iteration 77, loss = 0.19520546\n",
      "Iteration 137, loss = 0.55016529\n",
      "Iteration 133, loss = 0.51571938\n",
      "Iteration 134, loss = 0.49973063\n",
      "Iteration 132, loss = 0.50852232\n",
      "Iteration 60, loss = 0.44369506\n",
      "Iteration 132, loss = 0.07314111\n",
      "Iteration 123, loss = 0.51930541\n",
      "Iteration 135, loss = 0.60959937\n",
      "Iteration 71, loss = 0.31771143\n",
      "Iteration 78, loss = 0.19600058\n",
      "Iteration 138, loss = 0.54987431\n",
      "Iteration 134, loss = 0.51542949\n",
      "Iteration 133, loss = 0.50797154\n",
      "Iteration 135, loss = 0.49952699\n",
      "Iteration 95, loss = 0.19210864\n",
      "Iteration 68, loss = 0.41266332\n",
      "Iteration 133, loss = 0.07301619\n",
      "Iteration 136, loss = 0.60931362\n",
      "Iteration 97, loss = 0.23867677\n",
      "Iteration 80, loss = 0.09704548\n",
      "Iteration 139, loss = 0.54946191\n",
      "Iteration 135, loss = 0.51512805\n",
      "Iteration 136, loss = 0.49922574\n",
      "Iteration 134, loss = 0.50763670\n",
      "Iteration 124, loss = 0.51870320\n",
      "Iteration 134, loss = 0.07297507\n",
      "Iteration 137, loss = 0.60910181\n",
      "Iteration 78, loss = 0.19450972\n",
      "Iteration 96, loss = 0.19146219\n",
      "Iteration 140, loss = 0.54978576\n",
      "Iteration 135, loss = 0.50729772\n",
      "Iteration 137, loss = 0.49924530\n",
      "Iteration 79, loss = 0.19527409\n",
      "Iteration 136, loss = 0.51501736\n",
      "Iteration 135, loss = 0.07255303\n",
      "Iteration 98, loss = 0.23829939\n",
      "Iteration 138, loss = 0.60857812\n",
      "Iteration 72, loss = 0.31705528\n",
      "Iteration 141, loss = 0.54935841\n",
      "Iteration 125, loss = 0.51882686\n",
      "Iteration 138, loss = 0.49859943\n",
      "Iteration 136, loss = 0.50708010\n",
      "Iteration 137, loss = 0.51563016\n",
      "Iteration 136, loss = 0.07245133\n",
      "Iteration 81, loss = 0.09650840\n",
      "Iteration 69, loss = 0.41185046\n",
      "Iteration 139, loss = 0.60865489\n",
      "Iteration 97, loss = 0.19101097\n",
      "Iteration 142, loss = 0.54897448\n",
      "Iteration 79, loss = 0.19366654\n",
      "Iteration 139, loss = 0.49880448\n",
      "Iteration 99, loss = 0.23789319\n",
      "Iteration 137, loss = 0.50710228\n",
      "Iteration 138, loss = 0.51482746\n",
      "Iteration 137, loss = 0.07218172\n",
      "Iteration 140, loss = 0.60810418\n",
      "Iteration 80, loss = 0.19467303\n",
      "Iteration 126, loss = 0.51844581\n",
      "Iteration 143, loss = 0.54888923\n",
      "Iteration 140, loss = 0.49812823\n",
      "Iteration 138, loss = 0.50724753\n",
      "Iteration 98, loss = 0.19025861\n",
      "Iteration 139, loss = 0.51442484\n",
      "Iteration 138, loss = 0.07188086\n",
      "Iteration 61, loss = 0.44284464\n",
      "Iteration 141, loss = 0.60763440\n",
      "Iteration 82, loss = 0.09592321\n",
      "Iteration 100, loss = 0.23727273\n",
      "Iteration 144, loss = 0.54858152\n",
      "Iteration 73, loss = 0.31590770\n",
      "Iteration 141, loss = 0.49801638\n",
      "Iteration 139, loss = 0.50628321\n",
      "Iteration 139, loss = 0.07173705\n",
      "Iteration 127, loss = 0.51815356\n",
      "Iteration 140, loss = 0.51431091\n",
      "Iteration 80, loss = 0.19303492\n",
      "Iteration 70, loss = 0.41121036\n",
      "Iteration 142, loss = 0.60760253\n",
      "Iteration 145, loss = 0.54824234\n",
      "Iteration 99, loss = 0.18977153\n",
      "Iteration 81, loss = 0.19408450\n",
      "Iteration 140, loss = 0.50604880\n",
      "Iteration 142, loss = 0.49768232\n",
      "Iteration 140, loss = 0.07145634\n",
      "Iteration 141, loss = 0.51423339\n",
      "Iteration 143, loss = 0.60745110\n",
      "Iteration 101, loss = 0.23705113\n",
      "Iteration 146, loss = 0.54851442\n",
      "Iteration 128, loss = 0.51810560\n",
      "Iteration 143, loss = 0.49778357\n",
      "Iteration 141, loss = 0.50563511\n",
      "Iteration 83, loss = 0.09544359\n",
      "Iteration 141, loss = 0.07122136\n",
      "Iteration 142, loss = 0.51388796\n",
      "Iteration 144, loss = 0.60727302\n",
      "Iteration 100, loss = 0.18929167\n",
      "Iteration 147, loss = 0.54757158\n",
      "Iteration 81, loss = 0.19237222\n",
      "Iteration 144, loss = 0.49732559\n",
      "Iteration 74, loss = 0.31513699\n",
      "Iteration 142, loss = 0.50577384\n",
      "Iteration 102, loss = 0.23620773\n",
      "Iteration 142, loss = 0.07121430\n",
      "Iteration 143, loss = 0.51382450\n",
      "Iteration 82, loss = 0.19352261\n",
      "Iteration 145, loss = 0.60704413\n",
      "Iteration 71, loss = 0.41001287\n",
      "Iteration 129, loss = 0.51782008\n",
      "Iteration 148, loss = 0.54741172\n",
      "Iteration 145, loss = 0.49691446\n",
      "Iteration 143, loss = 0.50532589\n",
      "Iteration 143, loss = 0.07091067\n",
      "Iteration 144, loss = 0.51348250\n",
      "Iteration 84, loss = 0.09514384\n",
      "Iteration 101, loss = 0.18860853\n",
      "Iteration 146, loss = 0.60656798\n",
      "Iteration 149, loss = 0.54714837\n",
      "Iteration 146, loss = 0.49684504\n",
      "Iteration 103, loss = 0.23599857\n",
      "Iteration 144, loss = 0.50523600\n",
      "Iteration 62, loss = 0.44191311\n",
      "Iteration 145, loss = 0.51331858\n",
      "Iteration 144, loss = 0.07076344\n",
      "Iteration 130, loss = 0.51771844\n",
      "Iteration 82, loss = 0.19168498\n",
      "Iteration 147, loss = 0.60619437\n",
      "Iteration 150, loss = 0.54690509\n",
      "Iteration 147, loss = 0.49720777\n",
      "Iteration 83, loss = 0.19276691\n",
      "Iteration 145, loss = 0.50506735\n",
      "Iteration 146, loss = 0.51324232\n",
      "Iteration 75, loss = 0.31455583\n",
      "Iteration 102, loss = 0.18809029\n",
      "Iteration 145, loss = 0.07051774\n",
      "Iteration 148, loss = 0.60604435\n",
      "Iteration 151, loss = 0.54698633\n",
      "Iteration 104, loss = 0.23567192\n",
      "Iteration 148, loss = 0.49662603\n",
      "Iteration 72, loss = 0.40965201\n",
      "Iteration 85, loss = 0.09444456\n",
      "Iteration 146, loss = 0.50469139\n",
      "Iteration 131, loss = 0.51735314\n",
      "Iteration 147, loss = 0.51353975\n",
      "Iteration 146, loss = 0.07040563\n",
      "Iteration 149, loss = 0.60565472\n",
      "Iteration 152, loss = 0.54645324\n",
      "Iteration 149, loss = 0.49622490\n",
      "Iteration 83, loss = 0.19090483\n",
      "Iteration 103, loss = 0.18758760\n",
      "Iteration 147, loss = 0.50435615\n",
      "Iteration 148, loss = 0.51302172\n",
      "Iteration 147, loss = 0.07028000\n",
      "Iteration 84, loss = 0.19241422\n",
      "Iteration 105, loss = 0.23513342\n",
      "Iteration 150, loss = 0.60551195\n",
      "Iteration 153, loss = 0.54617204\n",
      "Iteration 150, loss = 0.49586330\n",
      "Iteration 132, loss = 0.51731906\n",
      "Iteration 149, loss = 0.51274830\n",
      "Iteration 148, loss = 0.50414226\n",
      "Iteration 148, loss = 0.07002310\n",
      "Iteration 86, loss = 0.09413162\n",
      "Iteration 76, loss = 0.31345032\n",
      "Iteration 151, loss = 0.60529816\n",
      "Iteration 154, loss = 0.54596130\n",
      "Iteration 104, loss = 0.18692853\n",
      "Iteration 151, loss = 0.49666439\n",
      "Iteration 150, loss = 0.51259893\n",
      "Iteration 73, loss = 0.40894158\n",
      "Iteration 149, loss = 0.50387461\n",
      "Iteration 106, loss = 0.23481457\n",
      "Iteration 149, loss = 0.06976922\n",
      "Iteration 84, loss = 0.19034397\n",
      "Iteration 152, loss = 0.60496553\n",
      "Iteration 155, loss = 0.54560136\n",
      "Iteration 133, loss = 0.51714888\n",
      "Iteration 85, loss = 0.19179129\n",
      "Iteration 151, loss = 0.51246784\n",
      "Iteration 152, loss = 0.49550203\n",
      "Iteration 63, loss = 0.44136231\n",
      "Iteration 150, loss = 0.50341662\n",
      "Iteration 150, loss = 0.06967636\n",
      "Iteration 105, loss = 0.18667072\n",
      "Iteration 153, loss = 0.60467252\n",
      "Iteration 156, loss = 0.54557016\n",
      "Iteration 87, loss = 0.09348974\n",
      "Iteration 152, loss = 0.51245575\n",
      "Iteration 153, loss = 0.49548397\n",
      "Iteration 107, loss = 0.23398409\n",
      "Iteration 151, loss = 0.50349791\n",
      "Iteration 151, loss = 0.06935975\n",
      "Iteration 134, loss = 0.51714261\n",
      "Iteration 154, loss = 0.60458818\n",
      "Iteration 77, loss = 0.31268239\n",
      "Iteration 157, loss = 0.54597596\n",
      "Iteration 85, loss = 0.18964899\n",
      "Iteration 153, loss = 0.51207931\n",
      "Iteration 154, loss = 0.49509811\n",
      "Iteration 152, loss = 0.06911898\n",
      "Iteration 152, loss = 0.50349169\n",
      "Iteration 74, loss = 0.40792199\n",
      "Iteration 86, loss = 0.19152980\n",
      "Iteration 106, loss = 0.18605808\n",
      "Iteration 155, loss = 0.60447550\n",
      "Iteration 158, loss = 0.54515078\n",
      "Iteration 108, loss = 0.23369988\n",
      "Iteration 154, loss = 0.51183163\n",
      "Iteration 155, loss = 0.49510124\n",
      "Iteration 135, loss = 0.51689183\n",
      "Iteration 153, loss = 0.50267458\n",
      "Iteration 153, loss = 0.06900802\n",
      "Iteration 88, loss = 0.09312184\n",
      "Iteration 156, loss = 0.60393494\n",
      "Iteration 159, loss = 0.54462405\n",
      "Iteration 155, loss = 0.51176041\n",
      "Iteration 156, loss = 0.49493717\n",
      "Iteration 154, loss = 0.50274526\n",
      "Iteration 107, loss = 0.18563443\n",
      "Iteration 154, loss = 0.06889040\n",
      "Iteration 86, loss = 0.18894177\n",
      "Iteration 109, loss = 0.23317774\n",
      "Iteration 157, loss = 0.60358492\n",
      "Iteration 160, loss = 0.54434103\n",
      "Iteration 78, loss = 0.31206335\n",
      "Iteration 136, loss = 0.51665057\n",
      "Iteration 87, loss = 0.19097407\n",
      "Iteration 157, loss = 0.49495645\n",
      "Iteration 156, loss = 0.51163199\n",
      "Iteration 155, loss = 0.50274407\n",
      "Iteration 155, loss = 0.06866085\n",
      "Iteration 158, loss = 0.60335892\n",
      "Iteration 75, loss = 0.40744486\n",
      "Iteration 161, loss = 0.54472001\n",
      "Iteration 89, loss = 0.09257393\n",
      "Iteration 108, loss = 0.18516189\n",
      "Iteration 157, loss = 0.51127657\n",
      "Iteration 64, loss = 0.44040065\n",
      "Iteration 158, loss = 0.49424892\n",
      "Iteration 156, loss = 0.50238478\n",
      "Iteration 156, loss = 0.06837346\n",
      "Iteration 110, loss = 0.23309448\n",
      "Iteration 159, loss = 0.60320116\n",
      "Iteration 137, loss = 0.51652938\n",
      "Iteration 162, loss = 0.54430715\n",
      "Iteration 87, loss = 0.18837449\n",
      "Iteration 158, loss = 0.51153305\n",
      "Iteration 159, loss = 0.49437092\n",
      "Iteration 157, loss = 0.06843283\n",
      "Iteration 157, loss = 0.50207505\n",
      "Iteration 88, loss = 0.19030717\n",
      "Iteration 160, loss = 0.60301943\n",
      "Iteration 109, loss = 0.18462088\n",
      "Iteration 163, loss = 0.54369869\n",
      "Iteration 79, loss = 0.31120733\n",
      "Iteration 159, loss = 0.51120133\n",
      "Iteration 160, loss = 0.49441839\n",
      "Iteration 111, loss = 0.23261039\n",
      "Iteration 158, loss = 0.50181713\n",
      "Iteration 158, loss = 0.06805187\n",
      "Iteration 90, loss = 0.09203737\n",
      "Iteration 138, loss = 0.51626188\n",
      "Iteration 161, loss = 0.60274442\n",
      "Iteration 164, loss = 0.54383354\n",
      "Iteration 76, loss = 0.40654703\n",
      "Iteration 160, loss = 0.51076771\n",
      "Iteration 161, loss = 0.49415625\n",
      "Iteration 159, loss = 0.50196328\n",
      "Iteration 159, loss = 0.06809044\n",
      "Iteration 88, loss = 0.18780382\n",
      "Iteration 110, loss = 0.18429794\n",
      "Iteration 162, loss = 0.60230613\n",
      "Iteration 165, loss = 0.54376134\n",
      "Iteration 89, loss = 0.18957630\n",
      "Iteration 112, loss = 0.23214839\n",
      "Iteration 161, loss = 0.51085351\n",
      "Iteration 139, loss = 0.51602912\n",
      "Iteration 162, loss = 0.49388402\n",
      "Iteration 160, loss = 0.50131590\n",
      "Iteration 160, loss = 0.06795969\n",
      "Iteration 163, loss = 0.60202029\n",
      "Iteration 166, loss = 0.54366879\n",
      "Iteration 91, loss = 0.09169445\n",
      "Iteration 162, loss = 0.51083894\n",
      "Iteration 80, loss = 0.31063307\n",
      "Iteration 161, loss = 0.50120830\n",
      "Iteration 163, loss = 0.49367002\n",
      "Iteration 161, loss = 0.06753240\n",
      "Iteration 111, loss = 0.18389414\n",
      "Iteration 164, loss = 0.60184226\n",
      "Iteration 167, loss = 0.54308416\n",
      "Iteration 113, loss = 0.23175081\n",
      "Iteration 140, loss = 0.51583886\n",
      "Iteration 89, loss = 0.18713540\n",
      "Iteration 65, loss = 0.43973241\n",
      "Iteration 163, loss = 0.51046567\n",
      "Iteration 77, loss = 0.40568031\n",
      "Iteration 90, loss = 0.18933444\n",
      "Iteration 162, loss = 0.50085345\n",
      "Iteration 164, loss = 0.49385781\n",
      "Iteration 162, loss = 0.06738228\n",
      "Iteration 165, loss = 0.60172808\n",
      "Iteration 168, loss = 0.54290116\n",
      "Iteration 164, loss = 0.51029405\n",
      "Iteration 112, loss = 0.18308342\n",
      "Iteration 163, loss = 0.50065883\n",
      "Iteration 165, loss = 0.49286525\n",
      "Iteration 92, loss = 0.09125032\n",
      "Iteration 114, loss = 0.23136603\n",
      "Iteration 163, loss = 0.06728709\n",
      "Iteration 141, loss = 0.51555662\n",
      "Iteration 169, loss = 0.54252076\n",
      "Iteration 166, loss = 0.60171844\n",
      "Iteration 165, loss = 0.51033420\n",
      "Iteration 164, loss = 0.50091301\n",
      "Iteration 166, loss = 0.49290637\n",
      "Iteration 90, loss = 0.18654042\n",
      "Iteration 81, loss = 0.30979069\n",
      "Iteration 164, loss = 0.06698070\n",
      "Iteration 170, loss = 0.54258894\n",
      "Iteration 91, loss = 0.18886513\n",
      "Iteration 167, loss = 0.60084747\n",
      "Iteration 113, loss = 0.18272074\n",
      "Iteration 166, loss = 0.50999395\n",
      "Iteration 115, loss = 0.23110430\n",
      "Iteration 165, loss = 0.50016289\n",
      "Iteration 142, loss = 0.51548136\n",
      "Iteration 78, loss = 0.40500037\n",
      "Iteration 167, loss = 0.49304402\n",
      "Iteration 165, loss = 0.06686762\n",
      "Iteration 171, loss = 0.54195952\n",
      "Iteration 93, loss = 0.09081792\n",
      "Iteration 168, loss = 0.60066643\n",
      "Iteration 167, loss = 0.50979659\n",
      "Iteration 166, loss = 0.50005136\n",
      "Iteration 168, loss = 0.49235099\n",
      "Iteration 166, loss = 0.06682762\n",
      "Iteration 172, loss = 0.54202844\n",
      "Iteration 169, loss = 0.60120146\n",
      "Iteration 114, loss = 0.18264032\n",
      "Iteration 91, loss = 0.18596223\n",
      "Iteration 116, loss = 0.23087182\n",
      "Iteration 168, loss = 0.50959574\n",
      "Iteration 143, loss = 0.51556564\n",
      "Iteration 92, loss = 0.18838588\n",
      "Iteration 167, loss = 0.50007771\n",
      "Iteration 169, loss = 0.49265154\n",
      "Iteration 167, loss = 0.06641999\n",
      "Iteration 82, loss = 0.30902796\n",
      "Iteration 173, loss = 0.54172525\n",
      "Iteration 170, loss = 0.60111939\n",
      "Iteration 66, loss = 0.43921271\n",
      "Iteration 169, loss = 0.50968774\n",
      "Iteration 94, loss = 0.09031010\n",
      "Iteration 170, loss = 0.49223033\n",
      "Iteration 168, loss = 0.49966696\n",
      "Iteration 79, loss = 0.40451388\n",
      "Iteration 168, loss = 0.06618052\n",
      "Iteration 115, loss = 0.18203642\n",
      "Iteration 174, loss = 0.54160807\n",
      "Iteration 144, loss = 0.51560549\n",
      "Iteration 117, loss = 0.23046418\n",
      "Iteration 171, loss = 0.60055141\n",
      "Iteration 170, loss = 0.50935804\n",
      "Iteration 169, loss = 0.49945179\n",
      "Iteration 92, loss = 0.18548755\n",
      "Iteration 171, loss = 0.49225308\n",
      "Iteration 169, loss = 0.06621795\n",
      "Iteration 175, loss = 0.54128779\n",
      "Iteration 93, loss = 0.18789731\n",
      "Iteration 172, loss = 0.59990531\n",
      "Iteration 171, loss = 0.50942962\n",
      "Iteration 116, loss = 0.18141539\n",
      "Iteration 170, loss = 0.49940345\n",
      "Iteration 172, loss = 0.49208290\n",
      "Iteration 145, loss = 0.51489313\n",
      "Iteration 170, loss = 0.06610933\n",
      "Iteration 118, loss = 0.23001794\n",
      "Iteration 176, loss = 0.54093113\n",
      "Iteration 95, loss = 0.08978676\n",
      "Iteration 173, loss = 0.59971723\n",
      "Iteration 172, loss = 0.50906465\n",
      "Iteration 83, loss = 0.30810208\n",
      "Iteration 171, loss = 0.49937515\n",
      "Iteration 171, loss = 0.06580413\n",
      "Iteration 173, loss = 0.49189214\n",
      "Iteration 177, loss = 0.54076561\n",
      "Iteration 80, loss = 0.40393263\n",
      "Iteration 174, loss = 0.59974565\n",
      "Iteration 173, loss = 0.50888089\n",
      "Iteration 93, loss = 0.18472901\n",
      "Iteration 117, loss = 0.18112182\n",
      "Iteration 94, loss = 0.18723707\n",
      "Iteration 146, loss = 0.51486389\n",
      "Iteration 119, loss = 0.22981928\n",
      "Iteration 172, loss = 0.49892454\n",
      "Iteration 174, loss = 0.49164827\n",
      "Iteration 172, loss = 0.06553776\n",
      "Iteration 178, loss = 0.54113351\n",
      "Iteration 175, loss = 0.59926849\n",
      "Iteration 174, loss = 0.50882927\n",
      "Iteration 96, loss = 0.08944587\n",
      "Iteration 173, loss = 0.49884937\n",
      "Iteration 173, loss = 0.06550534\n",
      "Iteration 175, loss = 0.49172839\n",
      "Iteration 179, loss = 0.54048263\n",
      "Iteration 118, loss = 0.18092441\n",
      "Iteration 176, loss = 0.59890522\n",
      "Iteration 147, loss = 0.51486309\n",
      "Iteration 175, loss = 0.50865388\n",
      "Iteration 67, loss = 0.43836019\n",
      "Iteration 120, loss = 0.22933584\n",
      "Iteration 84, loss = 0.30745172\n",
      "Iteration 174, loss = 0.49859771\n",
      "Iteration 95, loss = 0.18691757\n",
      "Iteration 94, loss = 0.18405129\n",
      "Iteration 180, loss = 0.54028606\n",
      "Iteration 174, loss = 0.06554558\n",
      "Iteration 176, loss = 0.49133721\n",
      "Iteration 177, loss = 0.59882267\n",
      "Iteration 81, loss = 0.40294394\n",
      "Iteration 176, loss = 0.50872407\n",
      "Iteration 119, loss = 0.18030327\n",
      "Iteration 175, loss = 0.49860459\n",
      "Iteration 175, loss = 0.06512089\n",
      "Iteration 181, loss = 0.54031230\n",
      "Iteration 148, loss = 0.51430343\n",
      "Iteration 177, loss = 0.49126908\n",
      "Iteration 178, loss = 0.59862474\n",
      "Iteration 97, loss = 0.08902753\n",
      "Iteration 121, loss = 0.22880556\n",
      "Iteration 177, loss = 0.50831106\n",
      "Iteration 176, loss = 0.49836635\n",
      "Iteration 182, loss = 0.53973603\n",
      "Iteration 176, loss = 0.06526057\n",
      "Iteration 178, loss = 0.49128384\n",
      "Iteration 179, loss = 0.59824062\n",
      "Iteration 178, loss = 0.50813313\n",
      "Iteration 96, loss = 0.18667446\n",
      "Iteration 95, loss = 0.18370222\n",
      "Iteration 120, loss = 0.18007628\n",
      "Iteration 149, loss = 0.51447514\n",
      "Iteration 177, loss = 0.49790405\n",
      "Iteration 85, loss = 0.30685730\n",
      "Iteration 183, loss = 0.53995603\n",
      "Iteration 177, loss = 0.06469138\n",
      "Iteration 122, loss = 0.22868061\n",
      "Iteration 179, loss = 0.49078097\n",
      "Iteration 180, loss = 0.59810281\n",
      "Iteration 179, loss = 0.50823458\n",
      "Iteration 98, loss = 0.08861609\n",
      "Iteration 82, loss = 0.40269872\n",
      "Iteration 178, loss = 0.49826279\n",
      "Iteration 184, loss = 0.54008103\n",
      "Iteration 178, loss = 0.06477874\n",
      "Iteration 180, loss = 0.49069658\n",
      "Iteration 181, loss = 0.59815218\n",
      "Iteration 180, loss = 0.50796972\n",
      "Iteration 121, loss = 0.17942477\n",
      "Iteration 150, loss = 0.51379597\n",
      "Iteration 97, loss = 0.18614160\n",
      "Iteration 123, loss = 0.22838804\n",
      "Iteration 179, loss = 0.49770382\n",
      "Iteration 96, loss = 0.18313638\n",
      "Iteration 185, loss = 0.54004629\n",
      "Iteration 179, loss = 0.06457856\n",
      "Iteration 181, loss = 0.49044103\n",
      "Iteration 182, loss = 0.59772446\n",
      "Iteration 181, loss = 0.50769119\n",
      "Iteration 68, loss = 0.43742861\n",
      "Iteration 180, loss = 0.49768214\n",
      "Iteration 186, loss = 0.53880562\n",
      "Iteration 180, loss = 0.06465208\n",
      "Iteration 182, loss = 0.49085107\n",
      "Iteration 99, loss = 0.08825115\n",
      "Iteration 86, loss = 0.30608669\n",
      "Iteration 151, loss = 0.51406345\n",
      "Iteration 122, loss = 0.17908411\n",
      "Iteration 183, loss = 0.59773609\n",
      "Iteration 182, loss = 0.50778758\n",
      "Iteration 124, loss = 0.22799891\n",
      "Iteration 83, loss = 0.40185302\n",
      "Iteration 181, loss = 0.49725130\n",
      "Iteration 183, loss = 0.49042870\n",
      "Iteration 181, loss = 0.06423961\n",
      "Iteration 187, loss = 0.53867630\n",
      "Iteration 183, loss = 0.50733895\n",
      "Iteration 98, loss = 0.18564267\n",
      "Iteration 184, loss = 0.59709659\n",
      "Iteration 97, loss = 0.18260227\n",
      "Iteration 152, loss = 0.51348846\n",
      "Iteration 123, loss = 0.17852194\n",
      "Iteration 182, loss = 0.49694638\n",
      "Iteration 184, loss = 0.48994432\n",
      "Iteration 188, loss = 0.53888192\n",
      "Iteration 182, loss = 0.06416116\n",
      "Iteration 184, loss = 0.50780686\n",
      "Iteration 185, loss = 0.59705472\n",
      "Iteration 125, loss = 0.22759899\n",
      "Iteration 100, loss = 0.08767523\n",
      "Iteration 183, loss = 0.49678316\n",
      "Iteration 189, loss = 0.53861296\n",
      "Iteration 183, loss = 0.06405133\n",
      "Iteration 185, loss = 0.48974102\n",
      "Iteration 185, loss = 0.50726658\n",
      "Iteration 87, loss = 0.30548637\n",
      "Iteration 186, loss = 0.59691301\n",
      "Iteration 153, loss = 0.51367409\n",
      "Iteration 124, loss = 0.17817357\n",
      "Iteration 99, loss = 0.18509755\n",
      "Iteration 98, loss = 0.18197075\n",
      "Iteration 84, loss = 0.40089585\n",
      "Iteration 184, loss = 0.49669374\n",
      "Iteration 184, loss = 0.06373831\n",
      "Iteration 190, loss = 0.53848592\n",
      "Iteration 186, loss = 0.48971549\n",
      "Iteration 126, loss = 0.22714506\n",
      "Iteration 187, loss = 0.59674278\n",
      "Iteration 186, loss = 0.50685859\n",
      "Iteration 191, loss = 0.53807799\n",
      "Iteration 101, loss = 0.08734828\n",
      "Iteration 185, loss = 0.49673592\n",
      "Iteration 185, loss = 0.06362013\n",
      "Iteration 187, loss = 0.49005064\n",
      "Iteration 187, loss = 0.50712393\n",
      "Iteration 188, loss = 0.59663683\n",
      "Iteration 154, loss = 0.51318623\n",
      "Iteration 69, loss = 0.43699855\n",
      "Iteration 125, loss = 0.17770081\n",
      "Iteration 127, loss = 0.22699364\n",
      "Iteration 192, loss = 0.53813632\n",
      "Iteration 186, loss = 0.49621566\n",
      "Iteration 188, loss = 0.48983845\n",
      "Iteration 188, loss = 0.50670805\n",
      "Iteration 186, loss = 0.06366728\n",
      "Iteration 100, loss = 0.18477063\n",
      "Iteration 189, loss = 0.59646591\n",
      "Iteration 99, loss = 0.18161314\n",
      "Iteration 88, loss = 0.30484278\n",
      "Iteration 193, loss = 0.53776750\n",
      "Iteration 155, loss = 0.51317237\n",
      "Iteration 189, loss = 0.48941872\n",
      "Iteration 189, loss = 0.50670157\n",
      "Iteration 187, loss = 0.49610764\n",
      "Iteration 85, loss = 0.40060234\n",
      "Iteration 126, loss = 0.17757941\n",
      "Iteration 187, loss = 0.06338890\n",
      "Iteration 190, loss = 0.59604031\n",
      "Iteration 102, loss = 0.08685102\n",
      "Iteration 128, loss = 0.22661887\n",
      "Iteration 194, loss = 0.53777849\n",
      "Iteration 190, loss = 0.50684122\n",
      "Iteration 190, loss = 0.48909596\n",
      "Iteration 188, loss = 0.06331261\n",
      "Iteration 188, loss = 0.49667400\n",
      "Iteration 191, loss = 0.59613127\n",
      "Iteration 101, loss = 0.18435706\n",
      "Iteration 156, loss = 0.51307195\n",
      "Iteration 100, loss = 0.18118643\n",
      "Iteration 127, loss = 0.17713689\n",
      "Iteration 191, loss = 0.50658381\n",
      "Iteration 195, loss = 0.53739683\n",
      "Iteration 189, loss = 0.49559362\n",
      "Iteration 191, loss = 0.48892087\n",
      "Iteration 189, loss = 0.06318344\n",
      "Iteration 192, loss = 0.59575344\n",
      "Iteration 129, loss = 0.22630236\n",
      "Iteration 89, loss = 0.30388897\n",
      "Iteration 103, loss = 0.08646232\n",
      "Iteration 192, loss = 0.50614804\n",
      "Iteration 196, loss = 0.53743652\n",
      "Iteration 190, loss = 0.06310703\n",
      "Iteration 192, loss = 0.48885674\n",
      "Iteration 190, loss = 0.49595217\n",
      "Iteration 193, loss = 0.59536238\n",
      "Iteration 157, loss = 0.51305650\n",
      "Iteration 86, loss = 0.39985936\n",
      "Iteration 128, loss = 0.17666385\n",
      "Iteration 197, loss = 0.53699623\n",
      "Iteration 102, loss = 0.18396811\n",
      "Iteration 193, loss = 0.50601590\n",
      "Iteration 191, loss = 0.06290619\n",
      "Iteration 193, loss = 0.48907815\n",
      "Iteration 130, loss = 0.22683023\n",
      "Iteration 191, loss = 0.49519481\n",
      "Iteration 70, loss = 0.43615066\n",
      "Iteration 194, loss = 0.59552981\n",
      "Iteration 101, loss = 0.18055523\n",
      "Iteration 198, loss = 0.53660193\n",
      "Iteration 194, loss = 0.50596879\n",
      "Iteration 192, loss = 0.06273529\n",
      "Iteration 194, loss = 0.48839219\n",
      "Iteration 158, loss = 0.51278578\n",
      "Iteration 192, loss = 0.49516708\n",
      "Iteration 129, loss = 0.17632306\n",
      "Iteration 195, loss = 0.59511056\n",
      "Iteration 104, loss = 0.08631876\n",
      "Iteration 90, loss = 0.30346169\n",
      "Iteration 131, loss = 0.22559363\n",
      "Iteration 199, loss = 0.53633892\n",
      "Iteration 195, loss = 0.50581741\n",
      "Iteration 193, loss = 0.06258074\n",
      "Iteration 195, loss = 0.48889860\n",
      "Iteration 193, loss = 0.49495131\n",
      "Iteration 196, loss = 0.59509216\n",
      "Iteration 103, loss = 0.18354767\n",
      "Iteration 87, loss = 0.39952608\n",
      "Iteration 102, loss = 0.18019899\n",
      "Iteration 159, loss = 0.51249916\n",
      "Iteration 196, loss = 0.50603613\n",
      "Iteration 200, loss = 0.53649145\n",
      "Iteration 194, loss = 0.06243240\n",
      "Iteration 130, loss = 0.17629850\n",
      "Iteration 196, loss = 0.48820070\n",
      "Iteration 197, loss = 0.59469072\n",
      "Iteration 194, loss = 0.49503695\n",
      "Iteration 132, loss = 0.22547808\n",
      "Iteration 105, loss = 0.08570859\n",
      "Iteration 201, loss = 0.53648608\n",
      "Iteration 197, loss = 0.50591733\n",
      "Iteration 195, loss = 0.06255686\n",
      "Iteration 197, loss = 0.48803081\n",
      "Iteration 195, loss = 0.49459608\n",
      "Iteration 198, loss = 0.59469178\n",
      "Iteration 160, loss = 0.51221347\n",
      "Iteration 104, loss = 0.18291649\n",
      "Iteration 91, loss = 0.30268760\n",
      "Iteration 202, loss = 0.53621510\n",
      "Iteration 131, loss = 0.17565138\n",
      "Iteration 198, loss = 0.50552157\n",
      "Iteration 196, loss = 0.06218261\n",
      "Iteration 198, loss = 0.48824303\n",
      "Iteration 103, loss = 0.17975500\n",
      "Iteration 196, loss = 0.49469718\n",
      "Iteration 199, loss = 0.59424530\n",
      "Iteration 133, loss = 0.22496188\n",
      "Iteration 88, loss = 0.39878110\n",
      "Iteration 203, loss = 0.53610079\n",
      "Iteration 71, loss = 0.43539968\n",
      "Iteration 199, loss = 0.50542787\n",
      "Iteration 197, loss = 0.06190640\n",
      "Iteration 197, loss = 0.49458562\n",
      "Iteration 199, loss = 0.48842959\n",
      "Iteration 106, loss = 0.08562853\n",
      "Iteration 161, loss = 0.51227123\n",
      "Iteration 200, loss = 0.59383856\n",
      "Iteration 132, loss = 0.17533826\n",
      "Iteration 204, loss = 0.53581515\n",
      "Iteration 200, loss = 0.50570803\n",
      "Iteration 198, loss = 0.06181231\n",
      "Iteration 198, loss = 0.49392751\n",
      "Iteration 200, loss = 0.48842496\n",
      "Iteration 134, loss = 0.22495869\n",
      "Iteration 105, loss = 0.18275770\n",
      "Iteration 201, loss = 0.59400870\n",
      "Iteration 205, loss = 0.53553096\n",
      "Iteration 104, loss = 0.17920490\n",
      "Iteration 92, loss = 0.30215323\n",
      "Iteration 201, loss = 0.50487293\n",
      "Iteration 162, loss = 0.51193233\n",
      "Iteration 199, loss = 0.06192073\n",
      "Iteration 199, loss = 0.49384534\n",
      "Iteration 201, loss = 0.48757380\n",
      "Iteration 133, loss = 0.17481288\n",
      "Iteration 202, loss = 0.59414984\n",
      "Iteration 206, loss = 0.53557680\n",
      "Iteration 107, loss = 0.08523525\n",
      "Iteration 89, loss = 0.39816168\n",
      "Iteration 202, loss = 0.50504314\n",
      "Iteration 135, loss = 0.22443008\n",
      "Iteration 200, loss = 0.06169494\n",
      "Iteration 200, loss = 0.49382289\n",
      "Iteration 202, loss = 0.48737110\n",
      "Iteration 203, loss = 0.59348772\n",
      "Iteration 106, loss = 0.18215630\n",
      "Iteration 207, loss = 0.53547973\n",
      "Iteration 163, loss = 0.51197734\n",
      "Iteration 203, loss = 0.50474624\n",
      "Iteration 134, loss = 0.17479178\n",
      "Iteration 201, loss = 0.06155190\n",
      "Iteration 105, loss = 0.17887564\n",
      "Iteration 201, loss = 0.49386288\n",
      "Iteration 203, loss = 0.48744433\n",
      "Iteration 208, loss = 0.53511008\n",
      "Iteration 204, loss = 0.59336094\n",
      "Iteration 136, loss = 0.22416060\n",
      "Iteration 204, loss = 0.50474084\n",
      "Iteration 202, loss = 0.06143130\n",
      "Iteration 202, loss = 0.49324576\n",
      "Iteration 93, loss = 0.30145189\n",
      "Iteration 108, loss = 0.08486598\n",
      "Iteration 204, loss = 0.48747768\n",
      "Iteration 164, loss = 0.51170457\n",
      "Iteration 209, loss = 0.53507954\n",
      "Iteration 205, loss = 0.59325418\n",
      "Iteration 135, loss = 0.17412688\n",
      "Iteration 205, loss = 0.50482948\n",
      "Iteration 72, loss = 0.43488461\n",
      "Iteration 90, loss = 0.39775555\n",
      "Iteration 203, loss = 0.49332644\n",
      "Iteration 203, loss = 0.06133133\n",
      "Iteration 107, loss = 0.18168282\n",
      "Iteration 205, loss = 0.48750163\n",
      "Iteration 137, loss = 0.22403327\n",
      "Iteration 106, loss = 0.17812857\n",
      "Iteration 210, loss = 0.53480855\n",
      "Iteration 206, loss = 0.59315257\n",
      "Iteration 206, loss = 0.50437635\n",
      "Iteration 204, loss = 0.49330672\n",
      "Iteration 204, loss = 0.06101224\n",
      "Iteration 165, loss = 0.51191199\n",
      "Iteration 206, loss = 0.48670222\n",
      "Iteration 211, loss = 0.53438525\n",
      "Iteration 136, loss = 0.17366380\n",
      "Iteration 109, loss = 0.08449163\n",
      "Iteration 207, loss = 0.59301064\n",
      "Iteration 207, loss = 0.50465504\n",
      "Iteration 205, loss = 0.06102059\n",
      "Iteration 205, loss = 0.49276574\n",
      "Iteration 207, loss = 0.48680736\n",
      "Iteration 138, loss = 0.22357625\n",
      "Iteration 94, loss = 0.30088861\n",
      "Iteration 208, loss = 0.59272877\n",
      "Iteration 212, loss = 0.53487690\n",
      "Iteration 108, loss = 0.18136723\n",
      "Iteration 208, loss = 0.50426476\n",
      "Iteration 166, loss = 0.51162252\n",
      "Iteration 206, loss = 0.06108797\n",
      "Iteration 206, loss = 0.49287049\n",
      "Iteration 107, loss = 0.17818463\n",
      "Iteration 137, loss = 0.17349280\n",
      "Iteration 91, loss = 0.39671224\n",
      "Iteration 208, loss = 0.48688705\n",
      "Iteration 209, loss = 0.59229597\n",
      "Iteration 213, loss = 0.53460973\n",
      "Iteration 209, loss = 0.50464428\n",
      "Iteration 207, loss = 0.06082140\n",
      "Iteration 110, loss = 0.08415076\n",
      "Iteration 139, loss = 0.22331071\n",
      "Iteration 207, loss = 0.49292153\n",
      "Iteration 209, loss = 0.48666461\n",
      "Iteration 210, loss = 0.59245000\n",
      "Iteration 214, loss = 0.53415988\n",
      "Iteration 167, loss = 0.51120165\n",
      "Iteration 210, loss = 0.50413808\n",
      "Iteration 138, loss = 0.17323247\n",
      "Iteration 208, loss = 0.06058414\n",
      "Iteration 109, loss = 0.18097389\n",
      "Iteration 208, loss = 0.49237854\n",
      "Iteration 210, loss = 0.48702037\n",
      "Iteration 211, loss = 0.59212591\n",
      "Iteration 95, loss = 0.30031300\n",
      "Iteration 215, loss = 0.53369789\n",
      "Iteration 108, loss = 0.17745115\n",
      "Iteration 211, loss = 0.50399182\n",
      "Iteration 140, loss = 0.22323693\n",
      "Iteration 73, loss = 0.43398053\n",
      "Iteration 209, loss = 0.06061116\n",
      "Iteration 209, loss = 0.49223363\n",
      "Iteration 211, loss = 0.48676327\n",
      "Iteration 168, loss = 0.51134567\n",
      "Iteration 216, loss = 0.53390201\n",
      "Iteration 212, loss = 0.59200560\n",
      "Iteration 111, loss = 0.08382327\n",
      "Iteration 92, loss = 0.39634513\n",
      "Iteration 139, loss = 0.17278290\n",
      "Iteration 212, loss = 0.50368468\n",
      "Iteration 210, loss = 0.06031285\n",
      "Iteration 212, loss = 0.48617540\n",
      "Iteration 210, loss = 0.49215340\n",
      "Iteration 110, loss = 0.18073576\n",
      "Iteration 213, loss = 0.59218990\n",
      "Iteration 217, loss = 0.53362992\n",
      "Iteration 141, loss = 0.22253437\n",
      "Iteration 213, loss = 0.50359033\n",
      "Iteration 169, loss = 0.51100260\n",
      "Iteration 211, loss = 0.06023753\n",
      "Iteration 109, loss = 0.17688590\n",
      "Iteration 211, loss = 0.49220211\n",
      "Iteration 213, loss = 0.48622854\n",
      "Iteration 214, loss = 0.59142663\n",
      "Iteration 218, loss = 0.53328953\n",
      "Iteration 140, loss = 0.17243696\n",
      "Iteration 96, loss = 0.29968943\n",
      "Iteration 214, loss = 0.50339310\n",
      "Iteration 112, loss = 0.08369161\n",
      "Iteration 212, loss = 0.06012218\n",
      "Iteration 212, loss = 0.49182553\n",
      "Iteration 214, loss = 0.48611429\n",
      "Iteration 219, loss = 0.53341302\n",
      "Iteration 142, loss = 0.22246278\n",
      "Iteration 215, loss = 0.59160112\n",
      "Iteration 215, loss = 0.50374534\n",
      "Iteration 170, loss = 0.51071124\n",
      "Iteration 93, loss = 0.39561729\n",
      "Iteration 111, loss = 0.18015897\n",
      "Iteration 213, loss = 0.05994768\n",
      "Iteration 141, loss = 0.17225052\n",
      "Iteration 213, loss = 0.49127861\n",
      "Iteration 220, loss = 0.53297946\n",
      "Iteration 215, loss = 0.48564887\n",
      "Iteration 216, loss = 0.59148267\n",
      "Iteration 216, loss = 0.50353205\n",
      "Iteration 110, loss = 0.17683985\n",
      "Iteration 214, loss = 0.05998072\n",
      "Iteration 143, loss = 0.22215935\n",
      "Iteration 221, loss = 0.53303195\n",
      "Iteration 214, loss = 0.49131254\n",
      "Iteration 216, loss = 0.48565723\n",
      "Iteration 217, loss = 0.59091000\n",
      "Iteration 113, loss = 0.08313739\n",
      "Iteration 171, loss = 0.51094049\n",
      "Iteration 217, loss = 0.50338171\n",
      "Iteration 97, loss = 0.29889363\n",
      "Iteration 74, loss = 0.43321596\n",
      "Iteration 142, loss = 0.17182306\n",
      "Iteration 215, loss = 0.05978436\n",
      "Iteration 222, loss = 0.53339475\n",
      "Iteration 112, loss = 0.17993374\n",
      "Iteration 217, loss = 0.48567852\n",
      "Iteration 215, loss = 0.49129102\n",
      "Iteration 218, loss = 0.59113193\n",
      "Iteration 218, loss = 0.50386865\n",
      "Iteration 144, loss = 0.22197832\n",
      "Iteration 94, loss = 0.39505535\n",
      "Iteration 216, loss = 0.05981176\n",
      "Iteration 111, loss = 0.17617155\n",
      "Iteration 172, loss = 0.51062894\n",
      "Iteration 223, loss = 0.53384217\n",
      "Iteration 218, loss = 0.48610358\n",
      "Iteration 219, loss = 0.59112083\n",
      "Iteration 216, loss = 0.49106891\n",
      "Iteration 219, loss = 0.50279004\n",
      "Iteration 143, loss = 0.17136397\n",
      "Iteration 114, loss = 0.08300331\n",
      "Iteration 217, loss = 0.05941512\n",
      "Iteration 224, loss = 0.53229278\n",
      "Iteration 219, loss = 0.48543178\n",
      "Iteration 217, loss = 0.49111200\n",
      "Iteration 220, loss = 0.59103648\n",
      "Iteration 220, loss = 0.50307499\n",
      "Iteration 145, loss = 0.22161723\n",
      "Iteration 113, loss = 0.17960370\n",
      "Iteration 173, loss = 0.51030630\n",
      "Iteration 98, loss = 0.29842040\n",
      "Iteration 218, loss = 0.05938618\n",
      "Iteration 225, loss = 0.53225988\n",
      "Iteration 220, loss = 0.48505527\n",
      "Iteration 218, loss = 0.49084877\n",
      "Iteration 221, loss = 0.59044480\n",
      "Iteration 221, loss = 0.50308692\n",
      "Iteration 144, loss = 0.17089165\n",
      "Iteration 112, loss = 0.17609067\n",
      "Iteration 219, loss = 0.05936161\n",
      "Iteration 226, loss = 0.53219146\n",
      "Iteration 95, loss = 0.39454724\n",
      "Iteration 222, loss = 0.59032944\n",
      "Iteration 115, loss = 0.08259246\n",
      "Iteration 219, loss = 0.49020263\n",
      "Iteration 221, loss = 0.48476131\n",
      "Iteration 222, loss = 0.50273131\n",
      "Iteration 146, loss = 0.22118272\n",
      "Iteration 174, loss = 0.51059635\n",
      "Iteration 227, loss = 0.53206630\n",
      "Iteration 220, loss = 0.05933031\n",
      "Iteration 114, loss = 0.17931155\n",
      "Iteration 145, loss = 0.17063872\n",
      "Iteration 223, loss = 0.59020738\n",
      "Iteration 220, loss = 0.49052123\n",
      "Iteration 222, loss = 0.48504955\n",
      "Iteration 223, loss = 0.50264433\n",
      "Iteration 75, loss = 0.43259571\n",
      "Iteration 113, loss = 0.17560382\n",
      "Iteration 228, loss = 0.53214510\n",
      "Iteration 99, loss = 0.29794857\n",
      "Iteration 147, loss = 0.22135691\n",
      "Iteration 221, loss = 0.05918500\n",
      "Iteration 175, loss = 0.51005019\n",
      "Iteration 224, loss = 0.59003761\n",
      "Iteration 221, loss = 0.49034796\n",
      "Iteration 223, loss = 0.48516805\n",
      "Iteration 224, loss = 0.50260139\n",
      "Iteration 116, loss = 0.08218711\n",
      "Iteration 146, loss = 0.17036534\n",
      "Iteration 229, loss = 0.53200831\n",
      "Iteration 222, loss = 0.05905168\n",
      "Iteration 96, loss = 0.39391726\n",
      "Iteration 225, loss = 0.58973534\n",
      "Iteration 222, loss = 0.49038143\n",
      "Iteration 225, loss = 0.50260293\n",
      "Iteration 224, loss = 0.48482994\n",
      "Iteration 115, loss = 0.17879041\n",
      "Iteration 176, loss = 0.50997951Iteration 148, loss = 0.22090012\n",
      "\n",
      "Iteration 230, loss = 0.53170514\n",
      "Iteration 223, loss = 0.05883250\n",
      "Iteration 226, loss = 0.58984814\n",
      "Iteration 223, loss = 0.49041958\n",
      "Iteration 226, loss = 0.50234174\n",
      "Iteration 225, loss = 0.48456046\n",
      "Iteration 114, loss = 0.17503603\n",
      "Iteration 147, loss = 0.17024258\n",
      "Iteration 231, loss = 0.53154745\n",
      "Iteration 117, loss = 0.08185273\n",
      "Iteration 224, loss = 0.05893579\n",
      "Iteration 100, loss = 0.29741574\n",
      "Iteration 227, loss = 0.58953064\n",
      "Iteration 224, loss = 0.48993250\n",
      "Iteration 227, loss = 0.50234372\n",
      "Iteration 226, loss = 0.48469090\n",
      "Iteration 177, loss = 0.50968220\n",
      "Iteration 149, loss = 0.22041107\n",
      "Iteration 232, loss = 0.53157654\n",
      "Iteration 225, loss = 0.05871983\n",
      "Iteration 116, loss = 0.17851673\n",
      "Iteration 228, loss = 0.58930288\n",
      "Iteration 227, loss = 0.48421205\n",
      "Iteration 228, loss = 0.50205566\n",
      "Iteration 225, loss = 0.48964849\n",
      "Iteration 97, loss = 0.39369537\n",
      "Iteration 148, loss = 0.17025950\n",
      "Iteration 233, loss = 0.53171964\n",
      "Iteration 115, loss = 0.17463989\n",
      "Iteration 226, loss = 0.05863544\n",
      "Iteration 229, loss = 0.58924228\n",
      "Iteration 228, loss = 0.48397941\n",
      "Iteration 229, loss = 0.50180522\n",
      "Iteration 226, loss = 0.48911080\n",
      "Iteration 178, loss = 0.50956467\n",
      "Iteration 150, loss = 0.22030865\n",
      "Iteration 118, loss = 0.08158144\n",
      "Iteration 76, loss = 0.43203437\n",
      "Iteration 234, loss = 0.53108686\n",
      "Iteration 227, loss = 0.05855764\n",
      "Iteration 230, loss = 0.58945201\n",
      "Iteration 229, loss = 0.48398181\n",
      "Iteration 227, loss = 0.48972388\n",
      "Iteration 149, loss = 0.16947430\n",
      "Iteration 230, loss = 0.50196905\n",
      "Iteration 101, loss = 0.29673844\n",
      "Iteration 117, loss = 0.17785816\n",
      "Iteration 235, loss = 0.53098705\n",
      "Iteration 179, loss = 0.51001019\n",
      "Iteration 228, loss = 0.05838196\n",
      "Iteration 151, loss = 0.22002384\n",
      "Iteration 231, loss = 0.58895119\n",
      "Iteration 231, loss = 0.50188913\n",
      "Iteration 230, loss = 0.48428663\n",
      "Iteration 228, loss = 0.48898633\n",
      "Iteration 116, loss = 0.17432131\n",
      "Iteration 98, loss = 0.39283074\n",
      "Iteration 236, loss = 0.53058145\n",
      "Iteration 150, loss = 0.16926537\n",
      "Iteration 119, loss = 0.08148116\n",
      "Iteration 229, loss = 0.05858601\n",
      "Iteration 232, loss = 0.58856950\n",
      "Iteration 232, loss = 0.50185722\n",
      "Iteration 229, loss = 0.48876419\n",
      "Iteration 231, loss = 0.48362256\n",
      "Iteration 237, loss = 0.53062639\n",
      "Iteration 180, loss = 0.50937773\n",
      "Iteration 152, loss = 0.22010926\n",
      "Iteration 233, loss = 0.58870142\n",
      "Iteration 230, loss = 0.05830405\n",
      "Iteration 118, loss = 0.17826688\n",
      "Iteration 233, loss = 0.50154276\n",
      "Iteration 232, loss = 0.48387831\n",
      "Iteration 230, loss = 0.48865517\n",
      "Iteration 102, loss = 0.29625052\n",
      "Iteration 238, loss = 0.53037810\n",
      "Iteration 151, loss = 0.16876313\n",
      "Iteration 117, loss = 0.17369172\n",
      "Iteration 234, loss = 0.58835502\n",
      "Iteration 231, loss = 0.05802158\n",
      "Iteration 233, loss = 0.48436001\n",
      "Iteration 234, loss = 0.50149959\n",
      "Iteration 231, loss = 0.48835369\n",
      "Iteration 181, loss = 0.50897740\n",
      "Iteration 120, loss = 0.08098902\n",
      "Iteration 153, loss = 0.21973018\n",
      "Iteration 239, loss = 0.53076411\n",
      "Iteration 99, loss = 0.39233101\n",
      "Iteration 235, loss = 0.58796090\n",
      "Iteration 232, loss = 0.05812327\n",
      "Iteration 235, loss = 0.50152947\n",
      "Iteration 234, loss = 0.48337080\n",
      "Iteration 232, loss = 0.48833037\n",
      "Iteration 152, loss = 0.16877438\n",
      "Iteration 119, loss = 0.17749502\n",
      "Iteration 77, loss = 0.43136045\n",
      "Iteration 240, loss = 0.53036479\n",
      "Iteration 236, loss = 0.58762608\n",
      "Iteration 182, loss = 0.50915459\n",
      "Iteration 236, loss = 0.50145501\n",
      "Iteration 233, loss = 0.05780690\n",
      "Iteration 233, loss = 0.48811431\n",
      "Iteration 118, loss = 0.17368849\n",
      "Iteration 235, loss = 0.48355882\n",
      "Iteration 154, loss = 0.21922764\n",
      "Iteration 241, loss = 0.53045433\n",
      "Iteration 103, loss = 0.29569270\n",
      "Iteration 121, loss = 0.08097678\n",
      "Iteration 237, loss = 0.58835216\n",
      "Iteration 237, loss = 0.50126317\n",
      "Iteration 234, loss = 0.05782559\n",
      "Iteration 236, loss = 0.48316620\n",
      "Iteration 153, loss = 0.16821250\n",
      "Iteration 234, loss = 0.48811377\n",
      "Iteration 242, loss = 0.52996374\n",
      "Iteration 183, loss = 0.50926616\n",
      "Iteration 238, loss = 0.58773191\n",
      "Iteration 100, loss = 0.39186500\n",
      "Iteration 120, loss = 0.17707261\n",
      "Iteration 155, loss = 0.21941873\n",
      "Iteration 235, loss = 0.05798673\n",
      "Iteration 238, loss = 0.50106323\n",
      "Iteration 237, loss = 0.48342782\n",
      "Iteration 235, loss = 0.48810300\n",
      "Iteration 243, loss = 0.52970666\n",
      "Iteration 119, loss = 0.17312844\n",
      "Iteration 154, loss = 0.16806353\n",
      "Iteration 239, loss = 0.58781955\n",
      "Iteration 236, loss = 0.05760319\n",
      "Iteration 239, loss = 0.50103618\n",
      "Iteration 236, loss = 0.48760233\n",
      "Iteration 238, loss = 0.48334959\n",
      "Iteration 122, loss = 0.08038301\n",
      "Iteration 184, loss = 0.50888272\n",
      "Iteration 244, loss = 0.53039039\n",
      "Iteration 104, loss = 0.29509033\n",
      "Iteration 156, loss = 0.21887776\n",
      "Iteration 237, loss = 0.05753510\n",
      "Iteration 240, loss = 0.58743967\n",
      "Iteration 240, loss = 0.50106208\n",
      "Iteration 237, loss = 0.48775079\n",
      "Iteration 239, loss = 0.48290453\n",
      "Iteration 121, loss = 0.17688286\n",
      "Iteration 245, loss = 0.52942286\n",
      "Iteration 155, loss = 0.16783861\n",
      "Iteration 241, loss = 0.50090031\n",
      "Iteration 238, loss = 0.05738967\n",
      "Iteration 101, loss = 0.39113506\n",
      "Iteration 241, loss = 0.58707694\n",
      "Iteration 238, loss = 0.48785193\n",
      "Iteration 120, loss = 0.17253595\n",
      "Iteration 185, loss = 0.50881389\n",
      "Iteration 240, loss = 0.48274122\n",
      "Iteration 78, loss = 0.43095764\n",
      "Iteration 246, loss = 0.52912745\n",
      "Iteration 157, loss = 0.21854765\n",
      "Iteration 123, loss = 0.08030222\n",
      "Iteration 242, loss = 0.50074120\n",
      "Iteration 239, loss = 0.05750434\n",
      "Iteration 239, loss = 0.48742256\n",
      "Iteration 242, loss = 0.58691674\n",
      "Iteration 156, loss = 0.16759338\n",
      "Iteration 247, loss = 0.52910292\n",
      "Iteration 241, loss = 0.48327366\n",
      "Iteration 122, loss = 0.17678499\n",
      "Iteration 105, loss = 0.29429688\n",
      "Iteration 243, loss = 0.50061033\n",
      "Iteration 186, loss = 0.50843967\n",
      "Iteration 243, loss = 0.58676015\n",
      "Iteration 240, loss = 0.05713373\n",
      "Iteration 240, loss = 0.48697867\n",
      "Iteration 158, loss = 0.21840863\n",
      "Iteration 248, loss = 0.52902211\n",
      "Iteration 242, loss = 0.48264805\n",
      "Iteration 121, loss = 0.17261261\n",
      "Iteration 244, loss = 0.50067486\n",
      "Iteration 241, loss = 0.48720045\n",
      "Iteration 241, loss = 0.05719318\n",
      "Iteration 244, loss = 0.58675307\n",
      "Iteration 157, loss = 0.16709147\n",
      "Iteration 249, loss = 0.52918069\n",
      "Iteration 102, loss = 0.39094937\n",
      "Iteration 124, loss = 0.07990476\n",
      "Iteration 243, loss = 0.48272660\n",
      "Iteration 187, loss = 0.50845940\n",
      "Iteration 245, loss = 0.50088895\n",
      "Iteration 159, loss = 0.21828054\n",
      "Iteration 242, loss = 0.48680254\n",
      "Iteration 242, loss = 0.05700296\n",
      "Iteration 245, loss = 0.58673309\n",
      "Iteration 250, loss = 0.52878295\n",
      "Iteration 123, loss = 0.17619823\n",
      "Iteration 244, loss = 0.48220010\n",
      "Iteration 158, loss = 0.16691794\n",
      "Iteration 246, loss = 0.50036574\n",
      "Iteration 243, loss = 0.05684035\n",
      "Iteration 246, loss = 0.58617846\n",
      "Iteration 243, loss = 0.48665280\n",
      "Iteration 251, loss = 0.52915105\n",
      "Iteration 106, loss = 0.29412310\n",
      "Iteration 122, loss = 0.17187046\n",
      "Iteration 188, loss = 0.50817071\n",
      "Iteration 245, loss = 0.48214734\n",
      "Iteration 160, loss = 0.21779141\n",
      "Iteration 125, loss = 0.07976370\n",
      "Iteration 247, loss = 0.50015635\n",
      "Iteration 244, loss = 0.05677584\n",
      "Iteration 247, loss = 0.58618892\n",
      "Iteration 252, loss = 0.52875779\n",
      "Iteration 244, loss = 0.48663910\n",
      "Iteration 246, loss = 0.48225547\n",
      "Iteration 79, loss = 0.43011298\n",
      "Iteration 103, loss = 0.39011454\n",
      "Iteration 159, loss = 0.16656596\n",
      "Iteration 124, loss = 0.17587940\n",
      "Iteration 248, loss = 0.50038823\n",
      "Iteration 245, loss = 0.05663220\n",
      "Iteration 189, loss = 0.50837323\n",
      "Iteration 253, loss = 0.52832705\n",
      "Iteration 245, loss = 0.48651114\n",
      "Iteration 248, loss = 0.58588423\n",
      "Iteration 247, loss = 0.48200677\n",
      "Iteration 161, loss = 0.21769136\n",
      "Iteration 123, loss = 0.17160668\n",
      "Iteration 249, loss = 0.50028620\n",
      "Iteration 246, loss = 0.05654949\n",
      "Iteration 254, loss = 0.52830551\n",
      "Iteration 246, loss = 0.48639476\n",
      "Iteration 249, loss = 0.58649301\n",
      "Iteration 126, loss = 0.07935136\n",
      "Iteration 248, loss = 0.48237276\n",
      "Iteration 107, loss = 0.29343387\n",
      "Iteration 160, loss = 0.16636600\n",
      "Iteration 190, loss = 0.50819131\n",
      "Iteration 250, loss = 0.50006320\n",
      "Iteration 247, loss = 0.05666106\n",
      "Iteration 255, loss = 0.52848648\n",
      "Iteration 250, loss = 0.58577645\n",
      "Iteration 247, loss = 0.48651147\n",
      "Iteration 125, loss = 0.17548978\n",
      "Iteration 162, loss = 0.21746359\n",
      "Iteration 249, loss = 0.48202915\n",
      "Iteration 104, loss = 0.38943141\n",
      "Iteration 251, loss = 0.50027918\n",
      "Iteration 248, loss = 0.05646781\n",
      "Iteration 256, loss = 0.52937958\n",
      "Iteration 248, loss = 0.48613597\n",
      "Iteration 251, loss = 0.58589344\n",
      "Iteration 161, loss = 0.16592319\n",
      "Iteration 124, loss = 0.17130880\n",
      "Iteration 250, loss = 0.48179299\n",
      "Iteration 191, loss = 0.50797219\n",
      "Iteration 127, loss = 0.07914313\n",
      "Iteration 252, loss = 0.50008093\n",
      "Iteration 163, loss = 0.21709699\n",
      "Iteration 257, loss = 0.52833776\n",
      "Iteration 249, loss = 0.05640644\n",
      "Iteration 252, loss = 0.58557739\n",
      "Iteration 249, loss = 0.48584063\n",
      "Iteration 251, loss = 0.48165509\n",
      "Iteration 108, loss = 0.29300194\n",
      "Iteration 126, loss = 0.17513379\n",
      "Iteration 253, loss = 0.49986970\n",
      "Iteration 250, loss = 0.05637256\n",
      "Iteration 162, loss = 0.16554981\n",
      "Iteration 258, loss = 0.52774380\n",
      "Iteration 253, loss = 0.58529443\n",
      "Iteration 250, loss = 0.48580293\n",
      "Iteration 192, loss = 0.50774018\n",
      "Iteration 252, loss = 0.48194711\n",
      "Iteration 80, loss = 0.42952629\n",
      "Iteration 125, loss = 0.17105756\n",
      "Iteration 164, loss = 0.21707781\n",
      "Iteration 259, loss = 0.52796538\n",
      "Iteration 251, loss = 0.05615174\n",
      "Iteration 254, loss = 0.58553411\n",
      "Iteration 105, loss = 0.38920884\n",
      "Iteration 254, loss = 0.49968614\n",
      "Iteration 128, loss = 0.07891843\n",
      "Iteration 251, loss = 0.48586941\n",
      "Iteration 253, loss = 0.48178830\n",
      "Iteration 163, loss = 0.16567529\n",
      "Iteration 260, loss = 0.52772975\n",
      "Iteration 193, loss = 0.50786168\n",
      "Iteration 252, loss = 0.05615081\n",
      "Iteration 255, loss = 0.58519160\n",
      "Iteration 255, loss = 0.50009213\n",
      "Iteration 127, loss = 0.17501300\n",
      "Iteration 252, loss = 0.48540472\n",
      "Iteration 254, loss = 0.48128063\n",
      "Iteration 165, loss = 0.21681825\n",
      "Iteration 109, loss = 0.29262047\n",
      "Iteration 261, loss = 0.52733029\n",
      "Iteration 253, loss = 0.05598178\n",
      "Iteration 256, loss = 0.49961528\n",
      "Iteration 256, loss = 0.58533678\n",
      "Iteration 126, loss = 0.17071387\n",
      "Iteration 253, loss = 0.48517857\n",
      "Iteration 255, loss = 0.48121720\n",
      "Iteration 129, loss = 0.07871212\n",
      "Iteration 164, loss = 0.16540678\n",
      "Iteration 194, loss = 0.50790312\n",
      "Iteration 262, loss = 0.52774234\n",
      "Iteration 254, loss = 0.05592223\n",
      "Iteration 257, loss = 0.49958609\n",
      "Iteration 257, loss = 0.58447818\n",
      "Iteration 106, loss = 0.38854528\n",
      "Iteration 166, loss = 0.21639197\n",
      "Iteration 254, loss = 0.48527394\n",
      "Iteration 256, loss = 0.48109060\n",
      "Iteration 128, loss = 0.17479998\n",
      "Iteration 263, loss = 0.52729949\n",
      "Iteration 258, loss = 0.49948781\n",
      "Iteration 255, loss = 0.05579019\n",
      "Iteration 258, loss = 0.58507673\n",
      "Iteration 165, loss = 0.16517757\n",
      "Iteration 255, loss = 0.48514976\n",
      "Iteration 195, loss = 0.50740727\n",
      "Iteration 257, loss = 0.48085204\n",
      "Iteration 127, loss = 0.17052128\n",
      "Iteration 264, loss = 0.52732669\n",
      "Iteration 110, loss = 0.29200542\n",
      "Iteration 130, loss = 0.07816202\n",
      "Iteration 256, loss = 0.05579099\n",
      "Iteration 259, loss = 0.49938707\n",
      "Iteration 167, loss = 0.21652931\n",
      "Iteration 259, loss = 0.58517545\n",
      "Iteration 256, loss = 0.48506808\n",
      "Iteration 81, loss = 0.42888532\n",
      "Iteration 258, loss = 0.48077499\n",
      "Iteration 265, loss = 0.52702473\n",
      "Iteration 260, loss = 0.49954825\n",
      "Iteration 129, loss = 0.17444909\n",
      "Iteration 257, loss = 0.05561650\n",
      "Iteration 166, loss = 0.16480233\n",
      "Iteration 260, loss = 0.58482708\n",
      "Iteration 196, loss = 0.50748725\n",
      "Iteration 107, loss = 0.38816473\n",
      "Iteration 257, loss = 0.48468214\n",
      "Iteration 259, loss = 0.48095989\n",
      "Iteration 168, loss = 0.21597352\n",
      "Iteration 266, loss = 0.52681929\n",
      "Iteration 258, loss = 0.05553861\n",
      "Iteration 261, loss = 0.49968787\n",
      "Iteration 261, loss = 0.58438021\n",
      "Iteration 128, loss = 0.16981314\n",
      "Iteration 131, loss = 0.07822359\n",
      "Iteration 258, loss = 0.48440915\n",
      "Iteration 260, loss = 0.48093868\n",
      "Iteration 267, loss = 0.52684114\n",
      "Iteration 167, loss = 0.16463419\n",
      "Iteration 197, loss = 0.50693937\n",
      "Iteration 259, loss = 0.05557635\n",
      "Iteration 111, loss = 0.29198225\n",
      "Iteration 262, loss = 0.49898990\n",
      "Iteration 262, loss = 0.58420114\n",
      "Iteration 130, loss = 0.17421696\n",
      "Iteration 169, loss = 0.21622050\n",
      "Iteration 261, loss = 0.48038238\n",
      "Iteration 259, loss = 0.48484095\n",
      "Iteration 268, loss = 0.52660263\n",
      "Iteration 263, loss = 0.49914338\n",
      "Iteration 260, loss = 0.05535370\n",
      "Iteration 263, loss = 0.58399489\n",
      "Iteration 262, loss = 0.48044496\n",
      "Iteration 269, loss = 0.52696161\n",
      "Iteration 260, loss = 0.48421387\n",
      "Iteration 129, loss = 0.16947314\n",
      "Iteration 168, loss = 0.16415598\n",
      "Iteration 108, loss = 0.38781501\n",
      "Iteration 198, loss = 0.50709519\n",
      "Iteration 132, loss = 0.07769877\n",
      "Iteration 264, loss = 0.49897945\n",
      "Iteration 261, loss = 0.05541596\n",
      "Iteration 264, loss = 0.58396756\n",
      "Iteration 170, loss = 0.21532811\n",
      "Iteration 270, loss = 0.52632944\n",
      "Iteration 261, loss = 0.48419628\n",
      "Iteration 263, loss = 0.48037318\n",
      "Iteration 131, loss = 0.17375752\n",
      "Iteration 262, loss = 0.05533363\n",
      "Iteration 265, loss = 0.49881236\n",
      "Iteration 265, loss = 0.58364486\n",
      "Iteration 112, loss = 0.29113994\n",
      "Iteration 82, loss = 0.42846508\n",
      "Iteration 169, loss = 0.16364612\n",
      "Iteration 199, loss = 0.50693282\n",
      "Iteration 271, loss = 0.52621419\n",
      "Iteration 264, loss = 0.48047715\n",
      "Iteration 262, loss = 0.48401468\n",
      "Iteration 263, loss = 0.05506962\n",
      "Iteration 266, loss = 0.58360108\n",
      "Iteration 266, loss = 0.49868231\n",
      "Iteration 171, loss = 0.21556989\n",
      "Iteration 130, loss = 0.16925422\n",
      "Iteration 133, loss = 0.07782362\n",
      "Iteration 272, loss = 0.52595759\n",
      "Iteration 265, loss = 0.48002797\n",
      "Iteration 263, loss = 0.48413660\n",
      "Iteration 109, loss = 0.38733459\n",
      "Iteration 264, loss = 0.05513118\n",
      "Iteration 267, loss = 0.58391030\n",
      "Iteration 267, loss = 0.49859483\n",
      "Iteration 200, loss = 0.50665124\n",
      "Iteration 132, loss = 0.17365539\n",
      "Iteration 170, loss = 0.16378101\n",
      "Iteration 273, loss = 0.52635748\n",
      "Iteration 266, loss = 0.48048063\n",
      "Iteration 264, loss = 0.48370380\n",
      "Iteration 172, loss = 0.21500013\n",
      "Iteration 265, loss = 0.05506708\n",
      "Iteration 268, loss = 0.49848247\n",
      "Iteration 268, loss = 0.58339793\n",
      "Iteration 113, loss = 0.29088570\n",
      "Iteration 267, loss = 0.48049416\n",
      "Iteration 274, loss = 0.52577828\n",
      "Iteration 131, loss = 0.16902063\n",
      "Iteration 265, loss = 0.48354393\n",
      "Iteration 201, loss = 0.50680189\n",
      "Iteration 134, loss = 0.07727003\n",
      "Iteration 266, loss = 0.05495128\n",
      "Iteration 269, loss = 0.49872366\n",
      "Iteration 171, loss = 0.16343455\n",
      "Iteration 269, loss = 0.58336124\n",
      "Iteration 268, loss = 0.48020973\n",
      "Iteration 275, loss = 0.52566205\n",
      "Iteration 266, loss = 0.48354612\n",
      "Iteration 173, loss = 0.21503727\n",
      "Iteration 133, loss = 0.17338972\n",
      "Iteration 267, loss = 0.05461213\n",
      "Iteration 270, loss = 0.49868829\n",
      "Iteration 270, loss = 0.58277300\n",
      "Iteration 110, loss = 0.38685355\n",
      "Iteration 202, loss = 0.50680857\n",
      "Iteration 269, loss = 0.48008319\n",
      "Iteration 267, loss = 0.48383190\n",
      "Iteration 276, loss = 0.52581711\n",
      "Iteration 172, loss = 0.16325920\n",
      "Iteration 268, loss = 0.05467236\n",
      "Iteration 271, loss = 0.49853167\n",
      "Iteration 271, loss = 0.58314105\n",
      "Iteration 132, loss = 0.16872422\n",
      "Iteration 135, loss = 0.07707420\n",
      "Iteration 83, loss = 0.42804582\n",
      "Iteration 174, loss = 0.21451082\n",
      "Iteration 270, loss = 0.47950219\n",
      "Iteration 277, loss = 0.52590552\n",
      "Iteration 114, loss = 0.29021942\n",
      "Iteration 268, loss = 0.48338582\n",
      "Iteration 269, loss = 0.05467046\n",
      "Iteration 272, loss = 0.49813847\n",
      "Iteration 272, loss = 0.58281106\n",
      "Iteration 203, loss = 0.50643219\n",
      "Iteration 134, loss = 0.17297838\n",
      "Iteration 271, loss = 0.47926428\n",
      "Iteration 278, loss = 0.52639043\n",
      "Iteration 269, loss = 0.48367045\n",
      "Iteration 173, loss = 0.16289144\n",
      "Iteration 270, loss = 0.05452731\n",
      "Iteration 273, loss = 0.49818859\n",
      "Iteration 273, loss = 0.58265177\n",
      "Iteration 175, loss = 0.21481119\n",
      "Iteration 111, loss = 0.38637788\n",
      "Iteration 279, loss = 0.52526208\n",
      "Iteration 272, loss = 0.47927153\n",
      "Iteration 270, loss = 0.48272368\n",
      "Iteration 133, loss = 0.16841752\n",
      "Iteration 136, loss = 0.07678884\n",
      "Iteration 271, loss = 0.05438131\n",
      "Iteration 274, loss = 0.49816728\n",
      "Iteration 204, loss = 0.50589344\n",
      "Iteration 274, loss = 0.58262278\n",
      "Iteration 174, loss = 0.16260244\n",
      "Iteration 280, loss = 0.52491680\n",
      "Iteration 273, loss = 0.47940690\n",
      "Iteration 271, loss = 0.48321567\n",
      "Iteration 135, loss = 0.17303120\n",
      "Iteration 115, loss = 0.28973371\n",
      "Iteration 272, loss = 0.05437124\n",
      "Iteration 176, loss = 0.21455504\n",
      "Iteration 275, loss = 0.49821466\n",
      "Iteration 275, loss = 0.58228052\n",
      "Iteration 281, loss = 0.52482302\n",
      "Iteration 274, loss = 0.47902744\n",
      "Iteration 272, loss = 0.48309969\n",
      "Iteration 205, loss = 0.50624246\n",
      "Iteration 273, loss = 0.05455176\n",
      "Iteration 276, loss = 0.49793493\n",
      "Iteration 175, loss = 0.16256252\n",
      "Iteration 134, loss = 0.16809328\n",
      "Iteration 276, loss = 0.58235847\n",
      "Iteration 137, loss = 0.07679873\n",
      "Iteration 282, loss = 0.52505789\n",
      "Iteration 112, loss = 0.38605546\n",
      "Iteration 275, loss = 0.47923212\n",
      "Iteration 177, loss = 0.21375798\n",
      "Iteration 273, loss = 0.48272274\n",
      "Iteration 274, loss = 0.05433821\n",
      "Iteration 277, loss = 0.49801327\n",
      "Iteration 136, loss = 0.17259055\n",
      "Iteration 277, loss = 0.58213256\n",
      "Iteration 84, loss = 0.42733054\n",
      "Iteration 283, loss = 0.52480222\n",
      "Iteration 206, loss = 0.50588183\n",
      "Iteration 276, loss = 0.47892316\n",
      "Iteration 274, loss = 0.48262940\n",
      "Iteration 176, loss = 0.16246364\n",
      "Iteration 116, loss = 0.28917302\n",
      "Iteration 275, loss = 0.05414645\n",
      "Iteration 278, loss = 0.49769150\n",
      "Iteration 278, loss = 0.58200306\n",
      "Iteration 284, loss = 0.52494754\n",
      "Iteration 178, loss = 0.21376725\n",
      "Iteration 135, loss = 0.16800051\n",
      "Iteration 277, loss = 0.47936645\n",
      "Iteration 138, loss = 0.07653020\n",
      "Iteration 275, loss = 0.48333107\n",
      "Iteration 276, loss = 0.05402173\n",
      "Iteration 279, loss = 0.49777891\n",
      "Iteration 279, loss = 0.58212044\n",
      "Iteration 207, loss = 0.50600437\n",
      "Iteration 285, loss = 0.52475447\n",
      "Iteration 177, loss = 0.16204468\n",
      "Iteration 137, loss = 0.17232782\n",
      "Iteration 276, loss = 0.48264820\n",
      "Iteration 113, loss = 0.38516575\n",
      "Iteration 278, loss = 0.47862255\n",
      "Iteration 277, loss = 0.05431603\n",
      "Iteration 280, loss = 0.49796365\n",
      "Iteration 280, loss = 0.58187569\n",
      "Iteration 286, loss = 0.52463064\n",
      "Iteration 179, loss = 0.21374783\n",
      "Iteration 277, loss = 0.48247775\n",
      "Iteration 279, loss = 0.47863452\n",
      "Iteration 208, loss = 0.50568419\n",
      "Iteration 281, loss = 0.49767257\n",
      "Iteration 117, loss = 0.28923308\n",
      "Iteration 278, loss = 0.05406555\n",
      "Iteration 136, loss = 0.16724052\n",
      "Iteration 281, loss = 0.58189800\n",
      "Iteration 139, loss = 0.07642974\n",
      "Iteration 287, loss = 0.52453166\n",
      "Iteration 178, loss = 0.16173215\n",
      "Iteration 278, loss = 0.48188074\n",
      "Iteration 280, loss = 0.47868642\n",
      "Iteration 282, loss = 0.49752400\n",
      "Iteration 180, loss = 0.21369058\n",
      "Iteration 279, loss = 0.05398679\n",
      "Iteration 138, loss = 0.17198646\n",
      "Iteration 282, loss = 0.58177826\n",
      "Iteration 288, loss = 0.52490325\n",
      "Iteration 209, loss = 0.50555879\n",
      "Iteration 279, loss = 0.48188651\n",
      "Iteration 114, loss = 0.38487112\n",
      "Iteration 281, loss = 0.47854427\n",
      "Iteration 283, loss = 0.49726185\n",
      "Iteration 179, loss = 0.16164746\n",
      "Iteration 280, loss = 0.05394599\n",
      "Iteration 283, loss = 0.58153762\n",
      "Iteration 289, loss = 0.52423142\n",
      "Iteration 85, loss = 0.42672634\n",
      "Iteration 137, loss = 0.16695558\n",
      "Iteration 280, loss = 0.48191373\n",
      "Iteration 140, loss = 0.07590964\n",
      "Iteration 181, loss = 0.21366614\n",
      "Iteration 282, loss = 0.47850153\n",
      "Iteration 284, loss = 0.49768591\n",
      "Iteration 281, loss = 0.05362713\n",
      "Iteration 290, loss = 0.52409517\n",
      "Iteration 284, loss = 0.58142618\n",
      "Iteration 118, loss = 0.28847241\n",
      "Iteration 210, loss = 0.50547039\n",
      "Iteration 281, loss = 0.48213027\n",
      "Iteration 139, loss = 0.17185653\n",
      "Iteration 180, loss = 0.16129281\n",
      "Iteration 283, loss = 0.47837543\n",
      "Iteration 282, loss = 0.05354980\n",
      "Iteration 285, loss = 0.49743879\n",
      "Iteration 291, loss = 0.52430572\n",
      "Iteration 285, loss = 0.58153591\n",
      "Iteration 182, loss = 0.21314933\n",
      "Iteration 282, loss = 0.48177437\n",
      "Iteration 284, loss = 0.47800679\n",
      "Iteration 286, loss = 0.49735290\n",
      "Iteration 138, loss = 0.16706754\n",
      "Iteration 283, loss = 0.05371758\n",
      "Iteration 115, loss = 0.38429015\n",
      "Iteration 141, loss = 0.07579473\n",
      "Iteration 211, loss = 0.50530255\n",
      "Iteration 286, loss = 0.58094868\n",
      "Iteration 292, loss = 0.52381157\n",
      "Iteration 181, loss = 0.16117243\n",
      "Iteration 283, loss = 0.48127423\n",
      "Iteration 285, loss = 0.47813996\n",
      "Iteration 287, loss = 0.49704773\n",
      "Iteration 140, loss = 0.17142897\n",
      "Iteration 284, loss = 0.05359667\n",
      "Iteration 287, loss = 0.58110811\n",
      "Iteration 293, loss = 0.52371519\n",
      "Iteration 183, loss = 0.21323455\n",
      "Iteration 119, loss = 0.28793590\n",
      "Iteration 284, loss = 0.48138362\n",
      "Iteration 212, loss = 0.50526733\n",
      "Iteration 286, loss = 0.47772693\n",
      "Iteration 288, loss = 0.49732473\n",
      "Iteration 288, loss = 0.58121730\n",
      "Iteration 285, loss = 0.05365565\n",
      "Iteration 294, loss = 0.52395037\n",
      "Iteration 182, loss = 0.16078274\n",
      "Iteration 142, loss = 0.07553342\n",
      "Iteration 139, loss = 0.16656554\n",
      "Iteration 285, loss = 0.48162476\n",
      "Iteration 287, loss = 0.47794722\n",
      "Iteration 184, loss = 0.21278207\n",
      "Iteration 289, loss = 0.49711390\n",
      "Iteration 289, loss = 0.58088290\n",
      "Iteration 86, loss = 0.42605370\n",
      "Iteration 286, loss = 0.05343799\n",
      "Iteration 295, loss = 0.52344188\n",
      "Iteration 116, loss = 0.38394912\n",
      "Iteration 213, loss = 0.50516561\n",
      "Iteration 141, loss = 0.17123113\n",
      "Iteration 286, loss = 0.48099498\n",
      "Iteration 288, loss = 0.47817635\n",
      "Iteration 290, loss = 0.49671914\n",
      "Iteration 290, loss = 0.58071717\n",
      "Iteration 183, loss = 0.16058735\n",
      "Iteration 296, loss = 0.52420642\n",
      "Iteration 287, loss = 0.05345420\n",
      "Iteration 120, loss = 0.28746137\n",
      "Iteration 185, loss = 0.21252708\n",
      "Iteration 287, loss = 0.48115681\n",
      "Iteration 289, loss = 0.47756792\n",
      "Iteration 291, loss = 0.58087798\n",
      "Iteration 143, loss = 0.07556529\n",
      "Iteration 140, loss = 0.16627127\n",
      "Iteration 297, loss = 0.52381168\n",
      "Iteration 291, loss = 0.49678604\n",
      "Iteration 288, loss = 0.05330360\n",
      "Iteration 214, loss = 0.50496457\n",
      "Iteration 288, loss = 0.48103606\n",
      "Iteration 184, loss = 0.16047484\n",
      "Iteration 290, loss = 0.47757522\n",
      "Iteration 292, loss = 0.58036987\n",
      "Iteration 142, loss = 0.17086649\n",
      "Iteration 292, loss = 0.49686941\n",
      "Iteration 298, loss = 0.52312147\n",
      "Iteration 289, loss = 0.05329156\n",
      "Iteration 117, loss = 0.38394747\n",
      "Iteration 186, loss = 0.21249357\n",
      "Iteration 289, loss = 0.48113860\n",
      "Iteration 293, loss = 0.58060194\n",
      "Iteration 291, loss = 0.47757870\n",
      "Iteration 299, loss = 0.52312436\n",
      "Iteration 293, loss = 0.49681167\n",
      "Iteration 215, loss = 0.50533305\n",
      "Iteration 290, loss = 0.05325808\n",
      "Iteration 144, loss = 0.07528897\n",
      "Iteration 141, loss = 0.16612910\n",
      "Iteration 185, loss = 0.16021409\n",
      "Iteration 290, loss = 0.48082260\n",
      "Iteration 294, loss = 0.58014624\n",
      "Iteration 121, loss = 0.28696538\n",
      "Iteration 292, loss = 0.47743876\n",
      "Iteration 300, loss = 0.52293694\n",
      "Iteration 294, loss = 0.49664805\n",
      "Iteration 291, loss = 0.05313300\n",
      "Iteration 187, loss = 0.21244416\n",
      "Iteration 143, loss = 0.17064005\n",
      "Iteration 216, loss = 0.50480921\n",
      "Iteration 291, loss = 0.48037351\n",
      "Iteration 295, loss = 0.58035967\n",
      "Iteration 293, loss = 0.47751600\n",
      "Iteration 301, loss = 0.52315561\n",
      "Iteration 87, loss = 0.42587057\n",
      "Iteration 295, loss = 0.49653081\n",
      "Iteration 292, loss = 0.05299792\n",
      "Iteration 186, loss = 0.16007023\n",
      "Iteration 118, loss = 0.38306276\n",
      "Iteration 296, loss = 0.57988270\n",
      "Iteration 145, loss = 0.07489145\n",
      "Iteration 292, loss = 0.48082347\n",
      "Iteration 142, loss = 0.16586576\n",
      "Iteration 294, loss = 0.47728750\n",
      "Iteration 302, loss = 0.52270456\n",
      "Iteration 188, loss = 0.21233537\n",
      "Iteration 293, loss = 0.05293698\n",
      "Iteration 296, loss = 0.49669296\n",
      "Iteration 217, loss = 0.50470753\n",
      "Iteration 297, loss = 0.57984618\n",
      "Iteration 144, loss = 0.17059138\n",
      "Iteration 293, loss = 0.48023328\n",
      "Iteration 303, loss = 0.52291493\n",
      "Iteration 295, loss = 0.47721658\n",
      "Iteration 294, loss = 0.05287963\n",
      "Iteration 122, loss = 0.28669520\n",
      "Iteration 297, loss = 0.49633639\n",
      "Iteration 187, loss = 0.15949245\n",
      "Iteration 298, loss = 0.57954587\n",
      "Iteration 189, loss = 0.21177519\n",
      "Iteration 294, loss = 0.48042285\n",
      "Iteration 304, loss = 0.52329908\n",
      "Iteration 296, loss = 0.47714283\n",
      "Iteration 298, loss = 0.49630393\n",
      "Iteration 295, loss = 0.05286485\n",
      "Iteration 218, loss = 0.50487096\n",
      "Iteration 146, loss = 0.07474555\n",
      "Iteration 143, loss = 0.16569448\n",
      "Iteration 299, loss = 0.57953239\n",
      "Iteration 119, loss = 0.38277785\n",
      "Iteration 295, loss = 0.48009909\n",
      "Iteration 305, loss = 0.52265677\n",
      "Iteration 188, loss = 0.15939097\n",
      "Iteration 299, loss = 0.49627163\n",
      "Iteration 297, loss = 0.47673684\n",
      "Iteration 296, loss = 0.05265750\n",
      "Iteration 145, loss = 0.17026165\n",
      "Iteration 190, loss = 0.21169138\n",
      "Iteration 300, loss = 0.57950204\n",
      "Iteration 296, loss = 0.47978028\n",
      "Iteration 306, loss = 0.52259268\n",
      "Iteration 219, loss = 0.50479706\n",
      "Iteration 300, loss = 0.49637691\n",
      "Iteration 298, loss = 0.47711638\n",
      "Iteration 297, loss = 0.05263064\n",
      "Iteration 123, loss = 0.28627130\n",
      "Iteration 189, loss = 0.15942827\n",
      "Iteration 301, loss = 0.57933708\n",
      "Iteration 307, loss = 0.52229258\n",
      "Iteration 147, loss = 0.07480512\n",
      "Iteration 297, loss = 0.48034876\n",
      "Iteration 301, loss = 0.49612445\n",
      "Iteration 88, loss = 0.42509745\n",
      "Iteration 144, loss = 0.16516884\n",
      "Iteration 298, loss = 0.05256644\n",
      "Iteration 299, loss = 0.47728574\n",
      "Iteration 191, loss = 0.21137437\n",
      "Iteration 220, loss = 0.50433680\n",
      "Iteration 302, loss = 0.57925599\n",
      "Iteration 308, loss = 0.52221551\n",
      "Iteration 146, loss = 0.17018743\n",
      "Iteration 298, loss = 0.47991559\n",
      "Iteration 302, loss = 0.49614119\n",
      "Iteration 300, loss = 0.47658175\n",
      "Iteration 120, loss = 0.38214938\n",
      "Iteration 299, loss = 0.05260948\n",
      "Iteration 190, loss = 0.15913448\n",
      "Iteration 309, loss = 0.52248750\n",
      "Iteration 303, loss = 0.57934070\n",
      "Iteration 299, loss = 0.47970092\n",
      "Iteration 303, loss = 0.49594691\n",
      "Iteration 300, loss = 0.05271101\n",
      "Iteration 301, loss = 0.47649675\n",
      "Iteration 192, loss = 0.21131415\n",
      "Iteration 148, loss = 0.07427888\n",
      "Iteration 221, loss = 0.50420345\n",
      "Iteration 145, loss = 0.16491957\n",
      "Iteration 124, loss = 0.28600905\n",
      "Iteration 310, loss = 0.52200405\n",
      "Iteration 304, loss = 0.57930762\n",
      "Iteration 304, loss = 0.49646541\n",
      "Iteration 300, loss = 0.47981764\n",
      "Iteration 302, loss = 0.47625916\n",
      "Iteration 301, loss = 0.05252202\n",
      "Iteration 191, loss = 0.15888473\n",
      "Iteration 147, loss = 0.16968707\n",
      "Iteration 311, loss = 0.52182392\n",
      "Iteration 305, loss = 0.57887555\n",
      "Iteration 305, loss = 0.49581507\n",
      "Iteration 193, loss = 0.21129596\n",
      "Iteration 301, loss = 0.47905805\n",
      "Iteration 222, loss = 0.50382356\n",
      "Iteration 302, loss = 0.05220104\n",
      "Iteration 303, loss = 0.47631984\n",
      "Iteration 121, loss = 0.38188181\n",
      "Iteration 312, loss = 0.52190097\n",
      "Iteration 149, loss = 0.07430748\n",
      "Iteration 306, loss = 0.57915348\n",
      "Iteration 306, loss = 0.49559127\n",
      "Iteration 146, loss = 0.16465225\n",
      "Iteration 192, loss = 0.15863140\n",
      "Iteration 302, loss = 0.47941486\n",
      "Iteration 303, loss = 0.05230351\n",
      "Iteration 304, loss = 0.47615891\n",
      "Iteration 313, loss = 0.52235855\n",
      "Iteration 125, loss = 0.28542381\n",
      "Iteration 194, loss = 0.21115384\n",
      "Iteration 307, loss = 0.57863710\n",
      "Iteration 148, loss = 0.16963835\n",
      "Iteration 223, loss = 0.50405223\n",
      "Iteration 89, loss = 0.42472219\n",
      "Iteration 307, loss = 0.49548306\n",
      "Iteration 303, loss = 0.47914987\n",
      "Iteration 304, loss = 0.05220069\n",
      "Iteration 305, loss = 0.47670286\n",
      "Iteration 314, loss = 0.52134193\n",
      "Iteration 193, loss = 0.15835426\n",
      "Iteration 308, loss = 0.57879819\n",
      "Iteration 308, loss = 0.49572467\n",
      "Iteration 304, loss = 0.47940069\n",
      "Iteration 150, loss = 0.07385091\n",
      "Iteration 305, loss = 0.05228264\n",
      "Iteration 306, loss = 0.47654312\n",
      "Iteration 122, loss = 0.38154271\n",
      "Iteration 315, loss = 0.52142695\n",
      "Iteration 147, loss = 0.16450268\n",
      "Iteration 224, loss = 0.50401564\n",
      "Iteration 195, loss = 0.21075561\n",
      "Iteration 309, loss = 0.57873257\n",
      "Iteration 309, loss = 0.49564240\n",
      "Iteration 305, loss = 0.47900037\n",
      "Iteration 306, loss = 0.05210136\n",
      "Iteration 307, loss = 0.47725735\n",
      "Iteration 149, loss = 0.16944354\n",
      "Iteration 316, loss = 0.52143702\n",
      "Iteration 194, loss = 0.15806187\n",
      "Iteration 310, loss = 0.57798713\n",
      "Iteration 310, loss = 0.49581096\n",
      "Iteration 126, loss = 0.28483991\n",
      "Iteration 306, loss = 0.47892520\n",
      "Iteration 307, loss = 0.05205049\n",
      "Iteration 196, loss = 0.21043370\n",
      "Iteration 308, loss = 0.47574287\n",
      "Iteration 225, loss = 0.50370687\n",
      "Iteration 317, loss = 0.52292498\n",
      "Iteration 151, loss = 0.07380960\n",
      "Iteration 311, loss = 0.57795586\n",
      "Iteration 148, loss = 0.16420295\n",
      "Iteration 311, loss = 0.49552380\n",
      "Iteration 307, loss = 0.47916501\n",
      "Iteration 308, loss = 0.05205977\n",
      "Iteration 195, loss = 0.15818196\n",
      "Iteration 309, loss = 0.47595207\n",
      "Iteration 318, loss = 0.52157671\n",
      "Iteration 123, loss = 0.38098135\n",
      "Iteration 312, loss = 0.57806991\n",
      "Iteration 150, loss = 0.16923445\n",
      "Iteration 312, loss = 0.49565885\n",
      "Iteration 226, loss = 0.50363980\n",
      "Iteration 197, loss = 0.21044507\n",
      "Iteration 308, loss = 0.47855832\n",
      "Iteration 309, loss = 0.05186790\n",
      "Iteration 310, loss = 0.47576579\n",
      "Iteration 319, loss = 0.52122461\n",
      "Iteration 313, loss = 0.57800694\n",
      "Iteration 313, loss = 0.49525587\n",
      "Iteration 90, loss = 0.42381960\n",
      "Iteration 196, loss = 0.15768607\n",
      "Iteration 127, loss = 0.28509536\n",
      "Iteration 152, loss = 0.07358048\n",
      "Iteration 309, loss = 0.47883355\n",
      "Iteration 310, loss = 0.05195898\n",
      "Iteration 311, loss = 0.47546790\n",
      "Iteration 149, loss = 0.16401840\n",
      "Iteration 320, loss = 0.52091336\n",
      "Iteration 314, loss = 0.57817592\n",
      "Iteration 227, loss = 0.50351867\n",
      "Iteration 314, loss = 0.49516954\n",
      "Iteration 198, loss = 0.21007750\n",
      "Iteration 310, loss = 0.47870552\n",
      "Iteration 151, loss = 0.16895408\n",
      "Iteration 311, loss = 0.05181782\n",
      "Iteration 321, loss = 0.52075377\n",
      "Iteration 312, loss = 0.47577918\n",
      "Iteration 315, loss = 0.57801811\n",
      "Iteration 197, loss = 0.15742348\n",
      "Iteration 124, loss = 0.38076423\n",
      "Iteration 315, loss = 0.49501716\n",
      "Iteration 312, loss = 0.05215491\n",
      "Iteration 311, loss = 0.47851305\n",
      "Iteration 322, loss = 0.52108929\n",
      "Iteration 228, loss = 0.50363753\n",
      "Iteration 313, loss = 0.47613868\n",
      "Iteration 153, loss = 0.07328957\n",
      "Iteration 199, loss = 0.21010443\n",
      "Iteration 316, loss = 0.57773503\n",
      "Iteration 316, loss = 0.49516252\n",
      "Iteration 150, loss = 0.16373549\n",
      "Iteration 313, loss = 0.05171978\n",
      "Iteration 312, loss = 0.47814851\n",
      "Iteration 128, loss = 0.28459647\n",
      "Iteration 323, loss = 0.52087473\n",
      "Iteration 314, loss = 0.47581798\n",
      "Iteration 152, loss = 0.16860270\n",
      "Iteration 198, loss = 0.15756134\n",
      "Iteration 317, loss = 0.57768010\n",
      "Iteration 317, loss = 0.49512766\n",
      "Iteration 314, loss = 0.05154943\n",
      "Iteration 229, loss = 0.50324837\n",
      "Iteration 313, loss = 0.47822100\n",
      "Iteration 324, loss = 0.52120978\n",
      "Iteration 315, loss = 0.47515362\n",
      "Iteration 200, loss = 0.20987359\n",
      "Iteration 318, loss = 0.57762020\n",
      "Iteration 318, loss = 0.49477807\n",
      "Iteration 125, loss = 0.38035012\n",
      "Iteration 154, loss = 0.07318250\n",
      "Iteration 315, loss = 0.05160361\n",
      "Iteration 314, loss = 0.47820106\n",
      "Iteration 325, loss = 0.52047974\n",
      "Iteration 199, loss = 0.15706149\n",
      "Iteration 151, loss = 0.16347397\n",
      "Iteration 316, loss = 0.47536225\n",
      "Iteration 319, loss = 0.57715863\n",
      "Iteration 319, loss = 0.49476791\n",
      "Iteration 91, loss = 0.42339540\n",
      "Iteration 230, loss = 0.50340698\n",
      "Iteration 153, loss = 0.16851473\n",
      "Iteration 316, loss = 0.05174100\n",
      "Iteration 201, loss = 0.20951321\n",
      "Iteration 326, loss = 0.52082003\n",
      "Iteration 315, loss = 0.47781267\n",
      "Iteration 317, loss = 0.47519017\n",
      "Iteration 129, loss = 0.28371139\n",
      "Iteration 320, loss = 0.49501405\n",
      "Iteration 320, loss = 0.57738353\n",
      "Iteration 200, loss = 0.15686410\n",
      "Iteration 317, loss = 0.05141378\n",
      "Iteration 316, loss = 0.47832055\n",
      "Iteration 327, loss = 0.52110150\n",
      "Iteration 318, loss = 0.47550423\n",
      "Iteration 155, loss = 0.07293932\n",
      "Iteration 321, loss = 0.49484201\n",
      "Iteration 231, loss = 0.50339496\n",
      "Iteration 321, loss = 0.57703738\n",
      "Iteration 152, loss = 0.16313801\n",
      "Iteration 202, loss = 0.20948501\n",
      "Iteration 126, loss = 0.37981269\n",
      "Iteration 318, loss = 0.05142997\n",
      "Iteration 317, loss = 0.47745028\n",
      "Iteration 328, loss = 0.52040844\n",
      "Iteration 319, loss = 0.47524218\n",
      "Iteration 154, loss = 0.16831126\n",
      "Iteration 322, loss = 0.49468308\n",
      "Iteration 322, loss = 0.57752965\n",
      "Iteration 201, loss = 0.15704925\n",
      "Iteration 319, loss = 0.05145145\n",
      "Iteration 318, loss = 0.47731261\n",
      "Iteration 329, loss = 0.52034831\n",
      "Iteration 320, loss = 0.47534921\n",
      "Iteration 232, loss = 0.50291269\n",
      "Iteration 130, loss = 0.28392231\n",
      "Iteration 323, loss = 0.49508955\n",
      "Iteration 203, loss = 0.20957110\n",
      "Iteration 323, loss = 0.57716872\n",
      "Iteration 156, loss = 0.07281842\n",
      "Iteration 319, loss = 0.47737362\n",
      "Iteration 330, loss = 0.52027678\n",
      "Iteration 320, loss = 0.05142182\n",
      "Iteration 153, loss = 0.16274133\n",
      "Iteration 321, loss = 0.47492244\n",
      "Iteration 202, loss = 0.15674084\n",
      "Iteration 324, loss = 0.49475540\n",
      "Iteration 324, loss = 0.57733661\n",
      "Iteration 155, loss = 0.16807638\n",
      "Iteration 320, loss = 0.47762954\n",
      "Iteration 233, loss = 0.50298927\n",
      "Iteration 331, loss = 0.52074806\n",
      "Iteration 127, loss = 0.37963951\n",
      "Iteration 321, loss = 0.05136040\n",
      "Iteration 204, loss = 0.20938681\n",
      "Iteration 322, loss = 0.47450208\n",
      "Iteration 325, loss = 0.49449781\n",
      "Iteration 92, loss = 0.42297712\n",
      "Iteration 325, loss = 0.57746012\n",
      "Iteration 332, loss = 0.51998940\n",
      "Iteration 321, loss = 0.47726281\n",
      "Iteration 322, loss = 0.05114089\n",
      "Iteration 157, loss = 0.07272158\n",
      "Iteration 203, loss = 0.15639135\n",
      "Iteration 326, loss = 0.49429050\n",
      "Iteration 131, loss = 0.28313523\n",
      "Iteration 323, loss = 0.47500644\n",
      "Iteration 154, loss = 0.16295421\n",
      "Iteration 234, loss = 0.50287223\n",
      "Iteration 326, loss = 0.57680334\n",
      "Iteration 333, loss = 0.52018415\n",
      "Iteration 323, loss = 0.05112677\n",
      "Iteration 322, loss = 0.47723381\n",
      "Iteration 205, loss = 0.20912384\n",
      "Iteration 156, loss = 0.16772879\n",
      "Iteration 327, loss = 0.49429032\n",
      "Iteration 324, loss = 0.47459788\n",
      "Iteration 327, loss = 0.57604536\n",
      "Iteration 204, loss = 0.15620228\n",
      "Iteration 334, loss = 0.51964589\n",
      "Iteration 324, loss = 0.05119355\n",
      "Iteration 323, loss = 0.47747386\n",
      "Iteration 128, loss = 0.37904246\n",
      "Iteration 328, loss = 0.49416544\n",
      "Iteration 235, loss = 0.50291368\n",
      "Iteration 325, loss = 0.47481240\n",
      "Iteration 158, loss = 0.07240868\n",
      "Iteration 328, loss = 0.57682526\n",
      "Iteration 206, loss = 0.20860993\n",
      "Iteration 335, loss = 0.52035816\n",
      "Iteration 325, loss = 0.05113190\n",
      "Iteration 324, loss = 0.47678391\n",
      "Iteration 155, loss = 0.16269479\n",
      "Iteration 329, loss = 0.49412901\n",
      "Iteration 326, loss = 0.47438134\n",
      "Iteration 329, loss = 0.57651641\n",
      "Iteration 132, loss = 0.28280704\n",
      "Iteration 157, loss = 0.16750120\n",
      "Iteration 205, loss = 0.15613633\n",
      "Iteration 336, loss = 0.52000053\n",
      "Iteration 326, loss = 0.05099807\n",
      "Iteration 325, loss = 0.47681905\n",
      "Iteration 236, loss = 0.50294127\n",
      "Iteration 330, loss = 0.49464454\n",
      "Iteration 330, loss = 0.57635503\n",
      "Iteration 327, loss = 0.47452971\n",
      "Iteration 207, loss = 0.20881838\n",
      "Iteration 337, loss = 0.52016825\n",
      "Iteration 327, loss = 0.05094484\n",
      "Iteration 326, loss = 0.47672507\n",
      "Iteration 159, loss = 0.07232190\n",
      "Iteration 129, loss = 0.37885755\n",
      "Iteration 206, loss = 0.15579983\n",
      "Iteration 331, loss = 0.49420549\n",
      "Iteration 93, loss = 0.42289984\n",
      "Iteration 328, loss = 0.47447121\n",
      "Iteration 331, loss = 0.57607929\n",
      "Iteration 156, loss = 0.16225717\n",
      "Iteration 338, loss = 0.51918348\n",
      "Iteration 237, loss = 0.50252196\n",
      "Iteration 328, loss = 0.05089307\n",
      "Iteration 327, loss = 0.47697102\n",
      "Iteration 158, loss = 0.16766148\n",
      "Iteration 208, loss = 0.20873330\n",
      "Iteration 332, loss = 0.49385913\n",
      "Iteration 332, loss = 0.57616589\n",
      "Iteration 329, loss = 0.47444431\n",
      "Iteration 339, loss = 0.51922568\n",
      "Iteration 133, loss = 0.28238497\n",
      "Iteration 329, loss = 0.05071897\n",
      "Iteration 328, loss = 0.47652530\n",
      "Iteration 207, loss = 0.15567578\n",
      "Iteration 333, loss = 0.49401345\n",
      "Iteration 160, loss = 0.07202489\n",
      "Iteration 333, loss = 0.57588872\n",
      "Iteration 330, loss = 0.47373232\n",
      "Iteration 238, loss = 0.50230192\n",
      "Iteration 340, loss = 0.51884361\n",
      "Iteration 330, loss = 0.05075108\n",
      "Iteration 329, loss = 0.47673597\n",
      "Iteration 157, loss = 0.16189981\n",
      "Iteration 209, loss = 0.20828013\n",
      "Iteration 159, loss = 0.16714653\n",
      "Iteration 130, loss = 0.37822939\n",
      "Iteration 334, loss = 0.49397287\n",
      "Iteration 334, loss = 0.57637922\n",
      "Iteration 331, loss = 0.47461474\n",
      "Iteration 341, loss = 0.51897294\n",
      "Iteration 208, loss = 0.15554226\n",
      "Iteration 331, loss = 0.05059303\n",
      "Iteration 330, loss = 0.47625567\n",
      "Iteration 239, loss = 0.50245706\n",
      "Iteration 335, loss = 0.57602848\n",
      "Iteration 335, loss = 0.49456007\n",
      "Iteration 332, loss = 0.47381357\n",
      "Iteration 342, loss = 0.51896379\n",
      "Iteration 161, loss = 0.07178440\n",
      "Iteration 210, loss = 0.20820319\n",
      "Iteration 134, loss = 0.28202872\n",
      "Iteration 332, loss = 0.05093877\n",
      "Iteration 331, loss = 0.47638851\n",
      "Iteration 336, loss = 0.57540849\n",
      "Iteration 336, loss = 0.49402691\n",
      "Iteration 158, loss = 0.16197187\n",
      "Iteration 333, loss = 0.47355659\n",
      "Iteration 343, loss = 0.51886316\n",
      "Iteration 209, loss = 0.15537735\n",
      "Iteration 160, loss = 0.16704710\n",
      "Iteration 333, loss = 0.05085054\n",
      "Iteration 240, loss = 0.50214244\n",
      "Iteration 332, loss = 0.47626705\n",
      "Iteration 337, loss = 0.57551946\n",
      "Iteration 94, loss = 0.42229585\n",
      "Iteration 337, loss = 0.49368913\n",
      "Iteration 334, loss = 0.47360671\n",
      "Iteration 344, loss = 0.51865687\n",
      "Iteration 131, loss = 0.37828843\n",
      "Iteration 211, loss = 0.20799857\n",
      "Iteration 334, loss = 0.05056038\n",
      "Iteration 333, loss = 0.47620068\n",
      "Iteration 162, loss = 0.07166010\n",
      "Iteration 210, loss = 0.15526966\n",
      "Iteration 338, loss = 0.57561666\n",
      "Iteration 338, loss = 0.49403588\n",
      "Iteration 345, loss = 0.51870513\n",
      "Iteration 335, loss = 0.47352679\n",
      "Iteration 241, loss = 0.50253552\n",
      "Iteration 335, loss = 0.05052308\n",
      "Iteration 135, loss = 0.28192174\n",
      "Iteration 159, loss = 0.16163187\n",
      "Iteration 334, loss = 0.47589754\n",
      "Iteration 161, loss = 0.16696316\n",
      "Iteration 339, loss = 0.49350471\n",
      "Iteration 339, loss = 0.57559878\n",
      "Iteration 212, loss = 0.20788405\n",
      "Iteration 346, loss = 0.51837586\n",
      "Iteration 336, loss = 0.47419926\n",
      "Iteration 211, loss = 0.15491293\n",
      "Iteration 336, loss = 0.05020671\n",
      "Iteration 335, loss = 0.47589837\n",
      "Iteration 340, loss = 0.49371031\n",
      "Iteration 340, loss = 0.57513116\n",
      "Iteration 347, loss = 0.51840059\n",
      "Iteration 242, loss = 0.50188813\n",
      "Iteration 337, loss = 0.47332647\n",
      "Iteration 132, loss = 0.37771536\n",
      "Iteration 163, loss = 0.07134084\n",
      "Iteration 337, loss = 0.05063400\n",
      "Iteration 336, loss = 0.47583536\n",
      "Iteration 213, loss = 0.20769401\n",
      "Iteration 341, loss = 0.49374707\n",
      "Iteration 341, loss = 0.57556515\n",
      "Iteration 348, loss = 0.51796682\n",
      "Iteration 160, loss = 0.16136177\n",
      "Iteration 338, loss = 0.47350139\n",
      "Iteration 162, loss = 0.16662812\n",
      "Iteration 212, loss = 0.15484069\n",
      "Iteration 338, loss = 0.05029639\n",
      "Iteration 337, loss = 0.47548808\n",
      "Iteration 136, loss = 0.28130154\n",
      "Iteration 243, loss = 0.50185709\n",
      "Iteration 342, loss = 0.49378059\n",
      "Iteration 342, loss = 0.57513335\n",
      "Iteration 349, loss = 0.51784844\n",
      "Iteration 339, loss = 0.47326609\n",
      "Iteration 339, loss = 0.05027064\n",
      "Iteration 338, loss = 0.47561528\n",
      "Iteration 214, loss = 0.20767027\n",
      "Iteration 164, loss = 0.07135992\n",
      "Iteration 343, loss = 0.49356006\n",
      "Iteration 95, loss = 0.42179297\n",
      "Iteration 343, loss = 0.57556442\n",
      "Iteration 350, loss = 0.51787318\n",
      "Iteration 213, loss = 0.15476603\n",
      "Iteration 340, loss = 0.47337735\n",
      "Iteration 244, loss = 0.50185116\n",
      "Iteration 340, loss = 0.05023687\n",
      "Iteration 161, loss = 0.16107986\n",
      "Iteration 339, loss = 0.47586321\n",
      "Iteration 133, loss = 0.37744114\n",
      "Iteration 163, loss = 0.16647241\n",
      "Iteration 344, loss = 0.57514833\n",
      "Iteration 344, loss = 0.49346829\n",
      "Iteration 351, loss = 0.51765358\n",
      "Iteration 341, loss = 0.47329850\n",
      "Iteration 215, loss = 0.20726392\n",
      "Iteration 341, loss = 0.05024171\n",
      "Iteration 340, loss = 0.47585759\n",
      "Iteration 345, loss = 0.57535102\n",
      "Iteration 345, loss = 0.49338777\n",
      "Iteration 137, loss = 0.28074850\n",
      "Iteration 214, loss = 0.15428328\n",
      "Iteration 352, loss = 0.51807837\n",
      "Iteration 245, loss = 0.50242255\n",
      "Iteration 342, loss = 0.47302018\n",
      "Iteration 165, loss = 0.07110189\n",
      "Iteration 342, loss = 0.05006015\n",
      "Iteration 341, loss = 0.47512715\n",
      "Iteration 353, loss = 0.51869921\n",
      "Iteration 346, loss = 0.57486649\n",
      "Iteration 346, loss = 0.49331472\n",
      "Iteration 162, loss = 0.16090695\n",
      "Iteration 216, loss = 0.20732768\n",
      "Iteration 164, loss = 0.16667540\n",
      "Iteration 343, loss = 0.47343464\n",
      "Iteration 343, loss = 0.05001171\n",
      "Iteration 342, loss = 0.47523744\n",
      "Iteration 215, loss = 0.15438350\n",
      "Iteration 134, loss = 0.37682358\n",
      "Iteration 246, loss = 0.50161441\n",
      "Iteration 354, loss = 0.51732369\n",
      "Iteration 347, loss = 0.57454022\n",
      "Iteration 347, loss = 0.49362664\n",
      "Iteration 344, loss = 0.47304559\n",
      "Iteration 344, loss = 0.05012460\n",
      "Iteration 343, loss = 0.47502621\n",
      "Iteration 355, loss = 0.51732029\n",
      "Iteration 348, loss = 0.57480079Iteration 217, loss = 0.20725423\n",
      "\n",
      "Iteration 166, loss = 0.07081103\n",
      "Iteration 348, loss = 0.49335788\n",
      "Iteration 138, loss = 0.28052515\n",
      "Iteration 345, loss = 0.47318472\n",
      "Iteration 345, loss = 0.05010560\n",
      "Iteration 216, loss = 0.15404008\n",
      "Iteration 163, loss = 0.16082294\n",
      "Iteration 165, loss = 0.16643653\n",
      "Iteration 247, loss = 0.50141752\n",
      "Iteration 344, loss = 0.47497156\n",
      "Iteration 349, loss = 0.57464368\n",
      "Iteration 96, loss = 0.42104846\n",
      "Iteration 356, loss = 0.51714802\n",
      "Iteration 349, loss = 0.49314704\n",
      "Iteration 346, loss = 0.47277382\n",
      "Iteration 346, loss = 0.05042374\n",
      "Iteration 218, loss = 0.20723047\n",
      "Iteration 345, loss = 0.47500551\n",
      "Iteration 357, loss = 0.51727971\n",
      "Iteration 350, loss = 0.57440287\n",
      "Iteration 135, loss = 0.37695284\n",
      "Iteration 350, loss = 0.49319962\n",
      "Iteration 217, loss = 0.15411902\n",
      "Iteration 347, loss = 0.04977735\n",
      "Iteration 347, loss = 0.47285188\n",
      "Iteration 248, loss = 0.50148793\n",
      "Iteration 167, loss = 0.07083212\n",
      "Iteration 358, loss = 0.51683627\n",
      "Iteration 346, loss = 0.47492703\n",
      "Iteration 351, loss = 0.57448604\n",
      "Iteration 164, loss = 0.16062519\n",
      "Iteration 351, loss = 0.49309338\n",
      "Iteration 166, loss = 0.16576752\n",
      "Iteration 139, loss = 0.28003613\n",
      "Iteration 219, loss = 0.20708870\n",
      "Iteration 348, loss = 0.47313972\n",
      "Iteration 348, loss = 0.04961528\n",
      "Iteration 359, loss = 0.51688376\n",
      "Iteration 347, loss = 0.47468356\n",
      "Iteration 352, loss = 0.57440093\n",
      "Iteration 218, loss = 0.15362941\n",
      "Iteration 352, loss = 0.49299590\n",
      "Iteration 249, loss = 0.50134615\n",
      "Iteration 349, loss = 0.47249225\n",
      "Iteration 349, loss = 0.04979555\n",
      "Iteration 360, loss = 0.51758402\n",
      "Iteration 348, loss = 0.47478601\n",
      "Iteration 353, loss = 0.57415015\n",
      "Iteration 168, loss = 0.07066530\n",
      "Iteration 353, loss = 0.49293763\n",
      "Iteration 220, loss = 0.20662633\n",
      "Iteration 136, loss = 0.37623193\n",
      "Iteration 350, loss = 0.47254135\n",
      "Iteration 350, loss = 0.04954092\n",
      "Iteration 165, loss = 0.16035965\n",
      "Iteration 167, loss = 0.16557177\n",
      "Iteration 219, loss = 0.15382562\n",
      "Iteration 361, loss = 0.51652371\n",
      "Iteration 250, loss = 0.50134665\n",
      "Iteration 354, loss = 0.57410302\n",
      "Iteration 349, loss = 0.47474405\n",
      "Iteration 354, loss = 0.49294015\n",
      "Iteration 351, loss = 0.47279087\n",
      "Iteration 351, loss = 0.04962712\n",
      "Iteration 140, loss = 0.27980474\n",
      "Iteration 362, loss = 0.51640628\n",
      "Iteration 355, loss = 0.57404635\n",
      "Iteration 350, loss = 0.47491234\n",
      "Iteration 97, loss = 0.42061322\n",
      "Iteration 221, loss = 0.20654335\n",
      "Iteration 355, loss = 0.49295973\n",
      "Iteration 352, loss = 0.47242514\n",
      "Iteration 220, loss = 0.15311817\n",
      "Iteration 169, loss = 0.07052346\n",
      "Iteration 352, loss = 0.04949515\n",
      "Iteration 251, loss = 0.50136025\n",
      "Iteration 363, loss = 0.51663254\n",
      "Iteration 356, loss = 0.57394156\n",
      "Iteration 351, loss = 0.47437302\n",
      "Iteration 166, loss = 0.15986736\n",
      "Iteration 168, loss = 0.16542802\n",
      "Iteration 356, loss = 0.49269503\n",
      "Iteration 353, loss = 0.47257125\n",
      "Iteration 137, loss = 0.37587647\n",
      "Iteration 353, loss = 0.04966592\n",
      "Iteration 364, loss = 0.51662843\n",
      "Iteration 222, loss = 0.20624724\n",
      "Iteration 357, loss = 0.57375117\n",
      "Iteration 357, loss = 0.49281516\n",
      "Iteration 352, loss = 0.47459186\n",
      "Iteration 221, loss = 0.15339576\n",
      "Iteration 354, loss = 0.47281640\n",
      "Iteration 252, loss = 0.50150306\n",
      "Iteration 354, loss = 0.04940370\n",
      "Iteration 365, loss = 0.51642483\n",
      "Iteration 141, loss = 0.27956548\n",
      "Iteration 170, loss = 0.07023785\n",
      "Iteration 358, loss = 0.57396966\n",
      "Iteration 358, loss = 0.49278577\n",
      "Iteration 353, loss = 0.47437739\n",
      "Iteration 355, loss = 0.47242267\n",
      "Iteration 167, loss = 0.15966714\n",
      "Iteration 169, loss = 0.16561897\n",
      "Iteration 355, loss = 0.04947680\n",
      "Iteration 366, loss = 0.51645048\n",
      "Iteration 223, loss = 0.20629360\n",
      "Iteration 222, loss = 0.15303454\n",
      "Iteration 354, loss = 0.47403581\n",
      "Iteration 359, loss = 0.49289360\n",
      "Iteration 359, loss = 0.57328916\n",
      "Iteration 253, loss = 0.50111124\n",
      "Iteration 356, loss = 0.47210728\n",
      "Iteration 356, loss = 0.04955595\n",
      "Iteration 367, loss = 0.51652324\n",
      "Iteration 138, loss = 0.37557206\n",
      "Iteration 355, loss = 0.47413644\n",
      "Iteration 360, loss = 0.49243502\n",
      "Iteration 360, loss = 0.57359730\n",
      "Iteration 357, loss = 0.47217170\n",
      "Iteration 224, loss = 0.20606874\n",
      "Iteration 171, loss = 0.06999241\n",
      "Iteration 357, loss = 0.04918536\n",
      "Iteration 368, loss = 0.51625200\n",
      "Iteration 223, loss = 0.15296709\n",
      "Iteration 254, loss = 0.50087003\n",
      "Iteration 170, loss = 0.16511025\n",
      "Iteration 168, loss = 0.15975038\n",
      "Iteration 356, loss = 0.47378243\n",
      "Iteration 142, loss = 0.27923053\n",
      "Iteration 98, loss = 0.42014933\n",
      "Iteration 361, loss = 0.49267420\n",
      "Iteration 361, loss = 0.57313194\n",
      "Iteration 358, loss = 0.47236035\n",
      "Iteration 369, loss = 0.51579536\n",
      "Iteration 358, loss = 0.04917926\n",
      "Iteration 357, loss = 0.47422075\n",
      "Iteration 362, loss = 0.49255048\n",
      "Iteration 362, loss = 0.57350225\n",
      "Iteration 225, loss = 0.20610064\n",
      "Iteration 359, loss = 0.47198617\n",
      "Iteration 224, loss = 0.15268001\n",
      "Iteration 370, loss = 0.51606791\n",
      "Iteration 359, loss = 0.04920549\n",
      "Iteration 255, loss = 0.50104024\n",
      "Iteration 139, loss = 0.37562743\n",
      "Iteration 363, loss = 0.49239238\n",
      "Iteration 358, loss = 0.47403338\n",
      "Iteration 172, loss = 0.06990996\n",
      "Iteration 363, loss = 0.57366134\n",
      "Iteration 360, loss = 0.47186839\n",
      "Iteration 169, loss = 0.15933000\n",
      "Iteration 171, loss = 0.16535941\n",
      "Iteration 371, loss = 0.51602085\n",
      "Iteration 360, loss = 0.04918239\n",
      "Iteration 226, loss = 0.20569089\n",
      "Iteration 364, loss = 0.49271014\n",
      "Iteration 359, loss = 0.47410747\n",
      "Iteration 225, loss = 0.15264798\n",
      "Iteration 364, loss = 0.57346252\n",
      "Iteration 143, loss = 0.27869934\n",
      "Iteration 361, loss = 0.47174719\n",
      "Iteration 372, loss = 0.51519276\n",
      "Iteration 256, loss = 0.50103140\n",
      "Iteration 361, loss = 0.04923149\n",
      "Iteration 360, loss = 0.47350895\n",
      "Iteration 365, loss = 0.49235060\n",
      "Iteration 365, loss = 0.57295627\n",
      "Iteration 362, loss = 0.47192045\n",
      "Iteration 373, loss = 0.51575645\n",
      "Iteration 173, loss = 0.06968473\n",
      "Iteration 362, loss = 0.04940622\n",
      "Iteration 227, loss = 0.20571053\n",
      "Iteration 226, loss = 0.15243797\n",
      "Iteration 170, loss = 0.15909061\n",
      "Iteration 172, loss = 0.16461296\n",
      "Iteration 361, loss = 0.47367838\n",
      "Iteration 140, loss = 0.37493973\n",
      "Iteration 366, loss = 0.49253427\n",
      "Iteration 366, loss = 0.57330788\n",
      "Iteration 257, loss = 0.50089459\n",
      "Iteration 363, loss = 0.47196929\n",
      "Iteration 374, loss = 0.51562966\n",
      "Iteration 363, loss = 0.04891762\n",
      "Iteration 99, loss = 0.41960956\n",
      "Iteration 367, loss = 0.49225891\n",
      "Iteration 362, loss = 0.47358596\n",
      "Iteration 367, loss = 0.57294273\n",
      "Iteration 364, loss = 0.47198123\n",
      "Iteration 144, loss = 0.27826826\n",
      "Iteration 228, loss = 0.20572632\n",
      "Iteration 375, loss = 0.51607532\n",
      "Iteration 227, loss = 0.15218764\n",
      "Iteration 364, loss = 0.04882171\n",
      "Iteration 258, loss = 0.50103039\n",
      "Iteration 174, loss = 0.06961750\n",
      "Iteration 368, loss = 0.49244250\n",
      "Iteration 363, loss = 0.47327419\n",
      "Iteration 368, loss = 0.57334105\n",
      "Iteration 173, loss = 0.16460409\n",
      "Iteration 365, loss = 0.47191883\n",
      "Iteration 171, loss = 0.15898577\n",
      "Iteration 376, loss = 0.51519542\n",
      "Iteration 365, loss = 0.04884167\n",
      "Iteration 369, loss = 0.57299051\n",
      "Iteration 369, loss = 0.49232313\n",
      "Iteration 364, loss = 0.47350809\n",
      "Iteration 141, loss = 0.37460500\n",
      "Iteration 229, loss = 0.20576423\n",
      "Iteration 366, loss = 0.47235786\n",
      "Iteration 228, loss = 0.15220170\n",
      "Iteration 377, loss = 0.51515459\n",
      "Iteration 366, loss = 0.04893203\n",
      "Iteration 259, loss = 0.50047755\n",
      "Iteration 370, loss = 0.49199014\n",
      "Iteration 370, loss = 0.57259994\n",
      "Iteration 365, loss = 0.47360779\n",
      "Iteration 367, loss = 0.47125246\n",
      "Iteration 378, loss = 0.51507264\n",
      "Iteration 175, loss = 0.06934952\n",
      "Iteration 145, loss = 0.27786124\n",
      "Iteration 367, loss = 0.04888577\n",
      "Iteration 174, loss = 0.16454728\n",
      "Iteration 172, loss = 0.15868244\n",
      "Iteration 230, loss = 0.20557484\n",
      "Iteration 371, loss = 0.49201314\n",
      "Iteration 371, loss = 0.57224661\n",
      "Iteration 229, loss = 0.15192435\n",
      "Iteration 366, loss = 0.47304769\n",
      "Iteration 379, loss = 0.51485928\n",
      "Iteration 368, loss = 0.47116531\n",
      "Iteration 260, loss = 0.50033241\n",
      "Iteration 368, loss = 0.04867343\n",
      "Iteration 372, loss = 0.49200039\n",
      "Iteration 372, loss = 0.57282572\n",
      "Iteration 367, loss = 0.47333851\n",
      "Iteration 380, loss = 0.51503622\n",
      "Iteration 142, loss = 0.37449766\n",
      "Iteration 369, loss = 0.47138270\n",
      "Iteration 369, loss = 0.04870842\n",
      "Iteration 231, loss = 0.20535952\n",
      "Iteration 230, loss = 0.15163434\n",
      "Iteration 100, loss = 0.41934587\n",
      "Iteration 176, loss = 0.06939637\n",
      "Iteration 373, loss = 0.49199532\n",
      "Iteration 373, loss = 0.57243297\n",
      "Iteration 368, loss = 0.47293701\n",
      "Iteration 175, loss = 0.16437864\n",
      "Iteration 381, loss = 0.51475755\n",
      "Iteration 261, loss = 0.50045399\n",
      "Iteration 173, loss = 0.15865426\n",
      "Iteration 370, loss = 0.47147601\n",
      "Iteration 370, loss = 0.04866149\n",
      "Iteration 146, loss = 0.27767068\n",
      "Iteration 374, loss = 0.49222113\n",
      "Iteration 369, loss = 0.47305118\n",
      "Iteration 374, loss = 0.57206806\n",
      "Iteration 382, loss = 0.51483666\n",
      "Iteration 371, loss = 0.47109222\n",
      "Iteration 371, loss = 0.04862245\n",
      "Iteration 231, loss = 0.15176176\n",
      "Iteration 232, loss = 0.20512237\n",
      "Iteration 262, loss = 0.50044783\n",
      "Iteration 375, loss = 0.49204447\n",
      "Iteration 375, loss = 0.57248305\n",
      "Iteration 383, loss = 0.51504030\n",
      "Iteration 370, loss = 0.47287634\n",
      "Iteration 177, loss = 0.06913377\n",
      "Iteration 372, loss = 0.47141456\n",
      "Iteration 372, loss = 0.04870573\n",
      "Iteration 143, loss = 0.37386038\n",
      "Iteration 176, loss = 0.16406483\n",
      "Iteration 174, loss = 0.15836962\n",
      "Iteration 376, loss = 0.49177699\n",
      "Iteration 384, loss = 0.51527824\n",
      "Iteration 232, loss = 0.15123974\n",
      "Iteration 376, loss = 0.57199382\n",
      "Iteration 371, loss = 0.47265790\n",
      "Iteration 233, loss = 0.20519224\n",
      "Iteration 373, loss = 0.47101562\n",
      "Iteration 373, loss = 0.04858263\n",
      "Iteration 263, loss = 0.50036439\n",
      "Iteration 147, loss = 0.27749653\n",
      "Iteration 385, loss = 0.51410190\n",
      "Iteration 377, loss = 0.49152430\n",
      "Iteration 377, loss = 0.57198998\n",
      "Iteration 372, loss = 0.47269313\n",
      "Iteration 374, loss = 0.47169705\n",
      "Iteration 374, loss = 0.04821581\n",
      "Iteration 178, loss = 0.06899312\n",
      "Iteration 233, loss = 0.15100523\n",
      "Iteration 234, loss = 0.20463931\n",
      "Iteration 386, loss = 0.51426766\n",
      "Iteration 378, loss = 0.49222376\n",
      "Iteration 378, loss = 0.57212984\n",
      "Iteration 177, loss = 0.16400415\n",
      "Iteration 373, loss = 0.47227850\n",
      "Iteration 175, loss = 0.15818608\n",
      "Iteration 375, loss = 0.04838718\n",
      "Iteration 264, loss = 0.49996986\n",
      "Iteration 375, loss = 0.47138184\n",
      "Iteration 144, loss = 0.37364876\n",
      "Iteration 101, loss = 0.41897890\n",
      "Iteration 379, loss = 0.49140120\n",
      "Iteration 387, loss = 0.51456661\n",
      "Iteration 374, loss = 0.47246752\n",
      "Iteration 379, loss = 0.57195840\n",
      "Iteration 376, loss = 0.47101009\n",
      "Iteration 376, loss = 0.04860883\n",
      "Iteration 234, loss = 0.15082146\n",
      "Iteration 235, loss = 0.20474800\n",
      "Iteration 375, loss = 0.47246211\n",
      "Iteration 380, loss = 0.49159910\n",
      "Iteration 388, loss = 0.51421100\n",
      "Iteration 380, loss = 0.57228167\n",
      "Iteration 148, loss = 0.27714908\n",
      "Iteration 179, loss = 0.06878634\n",
      "Iteration 265, loss = 0.49985202\n",
      "Iteration 377, loss = 0.47069209\n",
      "Iteration 178, loss = 0.16396551\n",
      "Iteration 176, loss = 0.15816946\n",
      "Iteration 377, loss = 0.04823895\n",
      "Iteration 381, loss = 0.49183692\n",
      "Iteration 376, loss = 0.47226887\n",
      "Iteration 389, loss = 0.51452094\n",
      "Iteration 381, loss = 0.57224283\n",
      "Iteration 235, loss = 0.15102161\n",
      "Iteration 236, loss = 0.20466183\n",
      "Iteration 378, loss = 0.04816195\n",
      "Iteration 378, loss = 0.47068632\n",
      "Iteration 145, loss = 0.37331957\n",
      "Iteration 266, loss = 0.50009502\n",
      "Iteration 382, loss = 0.49148802\n",
      "Iteration 390, loss = 0.51414179\n",
      "Iteration 382, loss = 0.57154636\n",
      "Iteration 377, loss = 0.47235135\n",
      "Iteration 379, loss = 0.04832531\n",
      "Iteration 379, loss = 0.47108480\n",
      "Iteration 180, loss = 0.06866454\n",
      "Iteration 179, loss = 0.16380551\n",
      "Iteration 391, loss = 0.51375313\n",
      "Iteration 378, loss = 0.47269295\n",
      "Iteration 383, loss = 0.49154219\n",
      "Iteration 177, loss = 0.15780999\n",
      "Iteration 383, loss = 0.57179086\n",
      "Iteration 236, loss = 0.15071258\n",
      "Iteration 237, loss = 0.20414705\n",
      "Iteration 149, loss = 0.27656598\n",
      "Iteration 380, loss = 0.47090771\n",
      "Iteration 380, loss = 0.04825240\n",
      "Iteration 267, loss = 0.49995302\n",
      "Iteration 392, loss = 0.51399607\n",
      "Iteration 384, loss = 0.49133788\n",
      "Iteration 384, loss = 0.57156322\n",
      "Iteration 379, loss = 0.47212000\n",
      "Iteration 381, loss = 0.47049823\n",
      "Iteration 381, loss = 0.04806334\n",
      "Iteration 102, loss = 0.41851958\n",
      "Iteration 393, loss = 0.51410596\n",
      "Iteration 237, loss = 0.15051137\n",
      "Iteration 385, loss = 0.49170255\n",
      "Iteration 385, loss = 0.57120527\n",
      "Iteration 238, loss = 0.20435854\n",
      "Iteration 380, loss = 0.47210336\n",
      "Iteration 181, loss = 0.06837319\n",
      "Iteration 146, loss = 0.37292408\n",
      "Iteration 180, loss = 0.16354668\n",
      "Iteration 268, loss = 0.50024295\n",
      "Iteration 178, loss = 0.15773491\n",
      "Iteration 382, loss = 0.04794680\n",
      "Iteration 382, loss = 0.47064022\n",
      "Iteration 394, loss = 0.51387187\n",
      "Iteration 386, loss = 0.57154546\n",
      "Iteration 386, loss = 0.49188528\n",
      "Iteration 381, loss = 0.47202439\n",
      "Iteration 383, loss = 0.04824042\n",
      "Iteration 383, loss = 0.47142931\n",
      "Iteration 150, loss = 0.27654024\n",
      "Iteration 238, loss = 0.15033342\n",
      "Iteration 395, loss = 0.51394851\n",
      "Iteration 239, loss = 0.20429639\n",
      "Iteration 387, loss = 0.49113623\n",
      "Iteration 387, loss = 0.57115321\n",
      "Iteration 382, loss = 0.47226006\n",
      "Iteration 269, loss = 0.49963541\n",
      "Iteration 384, loss = 0.04790577\n",
      "Iteration 384, loss = 0.47069092\n",
      "Iteration 182, loss = 0.06849729\n",
      "Iteration 396, loss = 0.51349127\n",
      "Iteration 388, loss = 0.49126364\n",
      "Iteration 181, loss = 0.16339481\n",
      "Iteration 388, loss = 0.57154547\n",
      "Iteration 383, loss = 0.47204434\n",
      "Iteration 179, loss = 0.15735275\n",
      "Iteration 147, loss = 0.37265243\n",
      "Iteration 239, loss = 0.15027784\n",
      "Iteration 385, loss = 0.04796788\n",
      "Iteration 385, loss = 0.47054473\n",
      "Iteration 240, loss = 0.20427113\n",
      "Iteration 397, loss = 0.51360647\n",
      "Iteration 389, loss = 0.49118744\n",
      "Iteration 270, loss = 0.49953599\n",
      "Iteration 389, loss = 0.57067471\n",
      "Iteration 384, loss = 0.47146209\n",
      "Iteration 386, loss = 0.04783450\n",
      "Iteration 386, loss = 0.47029644\n",
      "Iteration 398, loss = 0.51353504\n",
      "Iteration 151, loss = 0.27576756\n",
      "Iteration 390, loss = 0.49083226\n",
      "Iteration 390, loss = 0.57082445\n",
      "Iteration 385, loss = 0.47175449\n",
      "Iteration 183, loss = 0.06832936\n",
      "Iteration 240, loss = 0.15027073\n",
      "Iteration 182, loss = 0.16330808\n",
      "Iteration 241, loss = 0.20403266\n",
      "Iteration 387, loss = 0.47038587\n",
      "Iteration 387, loss = 0.04775570\n",
      "Iteration 180, loss = 0.15716708\n",
      "Iteration 271, loss = 0.49949158\n",
      "Iteration 399, loss = 0.51343366\n",
      "Iteration 103, loss = 0.41788940\n",
      "Iteration 391, loss = 0.57112782\n",
      "Iteration 391, loss = 0.49097338\n",
      "Iteration 386, loss = 0.47162039\n",
      "Iteration 388, loss = 0.47026465\n",
      "Iteration 148, loss = 0.37264727\n",
      "Iteration 388, loss = 0.04795655\n",
      "Iteration 400, loss = 0.51413008\n",
      "Iteration 241, loss = 0.15001623\n",
      "Iteration 392, loss = 0.57092350\n",
      "Iteration 392, loss = 0.49116018\n",
      "Iteration 387, loss = 0.47118462\n",
      "Iteration 242, loss = 0.20375050\n",
      "Iteration 389, loss = 0.47050173\n",
      "Iteration 272, loss = 0.49942499\n",
      "Iteration 184, loss = 0.06809017\n",
      "Iteration 389, loss = 0.04773684\n",
      "Iteration 401, loss = 0.51316859\n",
      "Iteration 183, loss = 0.16314070\n",
      "Iteration 393, loss = 0.49115110\n",
      "Iteration 393, loss = 0.57107578\n",
      "Iteration 152, loss = 0.27602097\n",
      "Iteration 388, loss = 0.47140188\n",
      "Iteration 181, loss = 0.15704844\n",
      "Iteration 390, loss = 0.46977130\n",
      "Iteration 390, loss = 0.04764488\n",
      "Iteration 402, loss = 0.51288694\n",
      "Iteration 242, loss = 0.15020523\n",
      "Iteration 243, loss = 0.20360825\n",
      "Iteration 394, loss = 0.57062686\n",
      "Iteration 394, loss = 0.49120061\n",
      "Iteration 389, loss = 0.47139268\n",
      "Iteration 273, loss = 0.49928653\n",
      "Iteration 391, loss = 0.47015299\n",
      "Iteration 391, loss = 0.04799670\n",
      "Iteration 403, loss = 0.51355347\n",
      "Iteration 149, loss = 0.37214615\n",
      "Iteration 395, loss = 0.57102872\n",
      "Iteration 185, loss = 0.06785204\n",
      "Iteration 395, loss = 0.49117183\n",
      "Iteration 390, loss = 0.47151511\n",
      "Iteration 184, loss = 0.16285388\n",
      "Iteration 243, loss = 0.14981201\n",
      "Iteration 392, loss = 0.47006811\n",
      "Iteration 392, loss = 0.04751247\n",
      "Iteration 182, loss = 0.15680636\n",
      "Iteration 244, loss = 0.20352093\n",
      "Iteration 404, loss = 0.51287818\n",
      "Iteration 274, loss = 0.49942601\n",
      "Iteration 396, loss = 0.57061897\n",
      "Iteration 396, loss = 0.49098993\n",
      "Iteration 153, loss = 0.27591323\n",
      "Iteration 391, loss = 0.47090802\n",
      "Iteration 393, loss = 0.47010054\n",
      "Iteration 393, loss = 0.04750076\n",
      "Iteration 405, loss = 0.51333933\n",
      "Iteration 104, loss = 0.41775327\n",
      "Iteration 397, loss = 0.57049826\n",
      "Iteration 244, loss = 0.14949908\n",
      "Iteration 392, loss = 0.47107949\n",
      "Iteration 397, loss = 0.49081081\n",
      "Iteration 245, loss = 0.20343842\n",
      "Iteration 186, loss = 0.06747979\n",
      "Iteration 394, loss = 0.47011642\n",
      "Iteration 406, loss = 0.51281917\n",
      "Iteration 394, loss = 0.04758522\n",
      "Iteration 185, loss = 0.16274468\n",
      "Iteration 275, loss = 0.49936835\n",
      "Iteration 150, loss = 0.37175863\n",
      "Iteration 183, loss = 0.15676708\n",
      "Iteration 398, loss = 0.49091206\n",
      "Iteration 398, loss = 0.57027005\n",
      "Iteration 393, loss = 0.47087029\n",
      "Iteration 395, loss = 0.47012853\n",
      "Iteration 407, loss = 0.51269303\n",
      "Iteration 395, loss = 0.04747909\n",
      "Iteration 245, loss = 0.14948152\n",
      "Iteration 399, loss = 0.49084490\n",
      "Iteration 399, loss = 0.57042155\n",
      "Iteration 394, loss = 0.47075674\n",
      "Iteration 246, loss = 0.20316736\n",
      "Iteration 154, loss = 0.27531919\n",
      "Iteration 276, loss = 0.49896028\n",
      "Iteration 396, loss = 0.46996646\n",
      "Iteration 408, loss = 0.51336089\n",
      "Iteration 396, loss = 0.04751938\n",
      "Iteration 187, loss = 0.06752964\n",
      "Iteration 400, loss = 0.49068490\n",
      "Iteration 400, loss = 0.57026437\n",
      "Iteration 395, loss = 0.47072626\n",
      "Iteration 186, loss = 0.16267102\n",
      "Iteration 184, loss = 0.15662020\n",
      "Iteration 246, loss = 0.14916551\n",
      "Iteration 397, loss = 0.47002976\n",
      "Iteration 409, loss = 0.51292996\n",
      "Iteration 397, loss = 0.04766039\n",
      "Iteration 247, loss = 0.20320616\n",
      "Iteration 151, loss = 0.37157907\n",
      "Iteration 401, loss = 0.49068536\n",
      "Iteration 401, loss = 0.57070835\n",
      "Iteration 396, loss = 0.47090902\n",
      "Iteration 277, loss = 0.49907887\n",
      "Iteration 410, loss = 0.51229747\n",
      "Iteration 398, loss = 0.46974506\n",
      "Iteration 398, loss = 0.04733592\n",
      "Iteration 402, loss = 0.49059362\n",
      "Iteration 402, loss = 0.56992749\n",
      "Iteration 397, loss = 0.47039863\n",
      "Iteration 247, loss = 0.14894058\n",
      "Iteration 188, loss = 0.06735995\n",
      "Iteration 155, loss = 0.27469886\n",
      "Iteration 248, loss = 0.20295953\n",
      "Iteration 187, loss = 0.16229646\n",
      "Iteration 411, loss = 0.51240847\n",
      "Iteration 105, loss = 0.41726311\n",
      "Iteration 399, loss = 0.46974897\n",
      "Iteration 399, loss = 0.04718835\n",
      "Iteration 185, loss = 0.15624382\n",
      "Iteration 403, loss = 0.49027332\n",
      "Iteration 278, loss = 0.49939380\n",
      "Iteration 403, loss = 0.56992426\n",
      "Iteration 398, loss = 0.47080330\n",
      "Iteration 412, loss = 0.51241674\n",
      "Iteration 400, loss = 0.46962547\n",
      "Iteration 400, loss = 0.04726958\n",
      "Iteration 404, loss = 0.49051176\n",
      "Iteration 404, loss = 0.57003145\n",
      "Iteration 248, loss = 0.14878447\n",
      "Iteration 399, loss = 0.47059194\n",
      "Iteration 152, loss = 0.37150188\n",
      "Iteration 249, loss = 0.20312321\n",
      "Iteration 413, loss = 0.51306100\n",
      "Iteration 279, loss = 0.49901150\n",
      "Iteration 401, loss = 0.46931649\n",
      "Iteration 189, loss = 0.06722886\n",
      "Iteration 401, loss = 0.04717982\n",
      "Iteration 405, loss = 0.49071290\n",
      "Iteration 188, loss = 0.16217430\n",
      "Iteration 405, loss = 0.57032876\n",
      "Iteration 400, loss = 0.47036306\n",
      "Iteration 186, loss = 0.15626696\n",
      "Iteration 156, loss = 0.27429472\n",
      "Iteration 414, loss = 0.51214988\n",
      "Iteration 402, loss = 0.46955737\n",
      "Iteration 249, loss = 0.14899676\n",
      "Iteration 402, loss = 0.04726996\n",
      "Iteration 406, loss = 0.49109049\n",
      "Iteration 406, loss = 0.56960455\n",
      "Iteration 250, loss = 0.20283245\n",
      "Iteration 401, loss = 0.47102729\n",
      "Iteration 280, loss = 0.49907311\n",
      "Iteration 415, loss = 0.51210386\n",
      "Iteration 403, loss = 0.46933943\n",
      "Iteration 403, loss = 0.04712378\n",
      "Iteration 407, loss = 0.49064510\n",
      "Iteration 407, loss = 0.56991868\n",
      "Iteration 402, loss = 0.47010540\n",
      "Iteration 190, loss = 0.06706423\n",
      "Iteration 189, loss = 0.16189904\n",
      "Iteration 153, loss = 0.37084224\n",
      "Iteration 250, loss = 0.14872813\n",
      "Iteration 416, loss = 0.51197944\n",
      "Iteration 404, loss = 0.46984256\n",
      "Iteration 404, loss = 0.04721647\n",
      "Iteration 408, loss = 0.49010310\n",
      "Iteration 251, loss = 0.20244106\n",
      "Iteration 187, loss = 0.15604704\n",
      "Iteration 408, loss = 0.56945033\n",
      "Iteration 403, loss = 0.47029577\n",
      "Iteration 281, loss = 0.49877311\n",
      "Iteration 405, loss = 0.46960022\n",
      "Iteration 417, loss = 0.51203882\n",
      "Iteration 106, loss = 0.41687732\n",
      "Iteration 157, loss = 0.27423740\n",
      "Iteration 409, loss = 0.49025589\n",
      "Iteration 405, loss = 0.04709544\n",
      "Iteration 409, loss = 0.56939798\n",
      "Iteration 404, loss = 0.47003255\n",
      "Iteration 251, loss = 0.14851118\n",
      "Iteration 252, loss = 0.20270258\n",
      "Iteration 406, loss = 0.46930661\n",
      "Iteration 418, loss = 0.51184625\n",
      "Iteration 406, loss = 0.04706017\n",
      "Iteration 191, loss = 0.06712276\n",
      "Iteration 410, loss = 0.49025372\n",
      "Iteration 190, loss = 0.16171095\n",
      "Iteration 282, loss = 0.49893850\n",
      "Iteration 410, loss = 0.56970391\n",
      "Iteration 188, loss = 0.15578638\n",
      "Iteration 405, loss = 0.47069374\n",
      "Iteration 407, loss = 0.46947239\n",
      "Iteration 419, loss = 0.51242437\n",
      "Iteration 154, loss = 0.37044233\n",
      "Iteration 407, loss = 0.04699244\n",
      "Iteration 411, loss = 0.49020631\n",
      "Iteration 411, loss = 0.56985047\n",
      "Iteration 252, loss = 0.14833089\n",
      "Iteration 253, loss = 0.20224315\n",
      "Iteration 406, loss = 0.47004774\n",
      "Iteration 420, loss = 0.51233615\n",
      "Iteration 408, loss = 0.46907871\n",
      "Iteration 283, loss = 0.49870084\n",
      "Iteration 408, loss = 0.04689964\n",
      "Iteration 158, loss = 0.27382586\n",
      "Iteration 412, loss = 0.49043158\n",
      "Iteration 412, loss = 0.56975648\n",
      "Iteration 192, loss = 0.06693240\n",
      "Iteration 407, loss = 0.46993356\n",
      "Iteration 191, loss = 0.16171353\n",
      "Iteration 421, loss = 0.51177035\n",
      "Iteration 409, loss = 0.46917848\n",
      "Iteration 189, loss = 0.15568195\n",
      "Iteration 253, loss = 0.14820261\n",
      "Iteration 413, loss = 0.49026963\n",
      "Iteration 409, loss = 0.04704570\n",
      "Iteration 254, loss = 0.20241315\n",
      "Iteration 413, loss = 0.56893317\n",
      "Iteration 284, loss = 0.49874917\n",
      "Iteration 408, loss = 0.47013165\n",
      "Iteration 422, loss = 0.51181816\n",
      "Iteration 410, loss = 0.46927726\n",
      "Iteration 155, loss = 0.37029691\n",
      "Iteration 414, loss = 0.49042191\n",
      "Iteration 410, loss = 0.04681690\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 414, loss = 0.56886980\n",
      "Iteration 409, loss = 0.46991023\n",
      "Iteration 423, loss = 0.51166522\n",
      "Iteration 254, loss = 0.14824085\n",
      "Iteration 107, loss = 0.41615847\n",
      "Iteration 411, loss = 0.46918569\n",
      "Iteration 193, loss = 0.06667497\n",
      "Iteration 415, loss = 0.49008221\n",
      "Iteration 255, loss = 0.20233561\n",
      "Iteration 192, loss = 0.16172697\n",
      "Iteration 159, loss = 0.27370952\n",
      "Iteration 285, loss = 0.49876375\n",
      "Iteration 415, loss = 0.56910053\n",
      "Iteration 190, loss = 0.15569183\n",
      "Iteration 424, loss = 0.51139702\n",
      "Iteration 410, loss = 0.46978828\n",
      "Iteration 412, loss = 0.46873056\n",
      "Iteration 416, loss = 0.49016928\n",
      "Iteration 416, loss = 0.56959979\n",
      "Iteration 255, loss = 0.14801337\n",
      "Iteration 425, loss = 0.51184813\n",
      "Iteration 411, loss = 0.46972917\n",
      "Iteration 413, loss = 0.46907817\n",
      "Iteration 256, loss = 0.20200258\n",
      "Iteration 417, loss = 0.49003045\n",
      "Iteration 286, loss = 0.49866366\n",
      "Iteration 156, loss = 0.36986390\n",
      "Iteration 417, loss = 0.56913017\n",
      "Iteration 194, loss = 0.06648890\n",
      "Iteration 193, loss = 0.16127769\n",
      "Iteration 426, loss = 0.51153279\n",
      "Iteration 412, loss = 0.46962467\n",
      "Iteration 414, loss = 0.46890696\n",
      "Iteration 418, loss = 0.49018373\n",
      "Iteration 191, loss = 0.15567768\n",
      "Iteration 256, loss = 0.14785564\n",
      "Iteration 160, loss = 0.27341576\n",
      "Iteration 418, loss = 0.56912288\n",
      "Iteration 427, loss = 0.51123889\n",
      "Iteration 257, loss = 0.20235494\n",
      "Iteration 413, loss = 0.46919956\n",
      "Iteration 287, loss = 0.49835648\n",
      "Iteration 415, loss = 0.46900309\n",
      "Iteration 419, loss = 0.48965781\n",
      "Iteration 419, loss = 0.56922053\n",
      "Iteration 428, loss = 0.51103176\n",
      "Iteration 420, loss = 0.48973541\n",
      "Iteration 414, loss = 0.46929893\n",
      "Iteration 416, loss = 0.46867593\n",
      "Iteration 195, loss = 0.06637706\n",
      "Iteration 257, loss = 0.14763993\n",
      "Iteration 194, loss = 0.16133014\n",
      "Iteration 420, loss = 0.56883601\n",
      "Iteration 258, loss = 0.20173799\n",
      "Iteration 192, loss = 0.15509037\n",
      "Iteration 288, loss = 0.49804121\n",
      "Iteration 157, loss = 0.36995918\n",
      "Iteration 429, loss = 0.51108870\n",
      "Iteration 415, loss = 0.46915637\n",
      "Iteration 108, loss = 0.41618053\n",
      "Iteration 421, loss = 0.48974952\n",
      "Iteration 417, loss = 0.46863871\n",
      "Iteration 421, loss = 0.56871296\n",
      "Iteration 161, loss = 0.27298519\n",
      "Iteration 430, loss = 0.51179025\n",
      "Iteration 422, loss = 0.48987908\n",
      "Iteration 416, loss = 0.46914125\n",
      "Iteration 258, loss = 0.14766524\n",
      "Iteration 418, loss = 0.46889631\n",
      "Iteration 259, loss = 0.20167227\n",
      "Iteration 289, loss = 0.49805244\n",
      "Iteration 196, loss = 0.06652933\n",
      "Iteration 422, loss = 0.56880474\n",
      "Iteration 195, loss = 0.16127217\n",
      "Iteration 431, loss = 0.51106627\n",
      "Iteration 417, loss = 0.46918243\n",
      "Iteration 423, loss = 0.48948535\n",
      "Iteration 419, loss = 0.46825803\n",
      "Iteration 193, loss = 0.15496002\n",
      "Iteration 423, loss = 0.56894823\n",
      "Iteration 432, loss = 0.51077516\n",
      "Iteration 259, loss = 0.14730804\n",
      "Iteration 424, loss = 0.49028717\n",
      "Iteration 158, loss = 0.36940928\n",
      "Iteration 418, loss = 0.46905695\n",
      "Iteration 420, loss = 0.46880010\n",
      "Iteration 290, loss = 0.49824088\n",
      "Iteration 260, loss = 0.20162029\n",
      "Iteration 424, loss = 0.56896992\n",
      "Iteration 433, loss = 0.51104555\n",
      "Iteration 425, loss = 0.48940443\n",
      "Iteration 419, loss = 0.46934091\n",
      "Iteration 197, loss = 0.06613289\n",
      "Iteration 162, loss = 0.27257747\n",
      "Iteration 421, loss = 0.46840773\n",
      "Iteration 196, loss = 0.16095153\n",
      "Iteration 425, loss = 0.56835418\n",
      "Iteration 260, loss = 0.14754100\n",
      "Iteration 194, loss = 0.15484165\n",
      "Iteration 434, loss = 0.51115253\n",
      "Iteration 291, loss = 0.49792683\n",
      "Iteration 261, loss = 0.20167388\n",
      "Iteration 420, loss = 0.46913360\n",
      "Iteration 426, loss = 0.48974748\n",
      "Iteration 422, loss = 0.46943181\n",
      "Iteration 426, loss = 0.56824068\n",
      "Iteration 435, loss = 0.51101038\n",
      "Iteration 109, loss = 0.41559377\n",
      "Iteration 421, loss = 0.46912817\n",
      "Iteration 427, loss = 0.48954976\n",
      "Iteration 423, loss = 0.46825428\n",
      "Iteration 159, loss = 0.36918915\n",
      "Iteration 261, loss = 0.14711299\n",
      "Iteration 198, loss = 0.06605921\n",
      "Iteration 427, loss = 0.56802566\n",
      "Iteration 436, loss = 0.51034885\n",
      "Iteration 197, loss = 0.16079251\n",
      "Iteration 262, loss = 0.20133637\n",
      "Iteration 292, loss = 0.49848247\n",
      "Iteration 428, loss = 0.48967634\n",
      "Iteration 422, loss = 0.46847860\n",
      "Iteration 424, loss = 0.46803132\n",
      "Iteration 195, loss = 0.15468131\n",
      "Iteration 163, loss = 0.27233844\n",
      "Iteration 428, loss = 0.56825779\n",
      "Iteration 437, loss = 0.51104680\n",
      "Iteration 262, loss = 0.14741837\n",
      "Iteration 429, loss = 0.48946713\n",
      "Iteration 423, loss = 0.46881908\n",
      "Iteration 425, loss = 0.46860099\n",
      "Iteration 293, loss = 0.49816887\n",
      "Iteration 429, loss = 0.56842376\n",
      "Iteration 263, loss = 0.20123530\n",
      "Iteration 438, loss = 0.51076244\n",
      "Iteration 199, loss = 0.06571724\n",
      "Iteration 430, loss = 0.48957526\n",
      "Iteration 424, loss = 0.46883010\n",
      "Iteration 198, loss = 0.16082076\n",
      "Iteration 426, loss = 0.46813773\n",
      "Iteration 160, loss = 0.36868143\n",
      "Iteration 430, loss = 0.56777487\n",
      "Iteration 439, loss = 0.51065906\n",
      "Iteration 196, loss = 0.15434346\n",
      "Iteration 263, loss = 0.14665898\n",
      "Iteration 431, loss = 0.48949482\n",
      "Iteration 425, loss = 0.46836705\n",
      "Iteration 294, loss = 0.49809656\n",
      "Iteration 427, loss = 0.46852496\n",
      "Iteration 264, loss = 0.20089195\n",
      "Iteration 431, loss = 0.56850258\n",
      "Iteration 164, loss = 0.27209993\n",
      "Iteration 440, loss = 0.51050948\n",
      "Iteration 432, loss = 0.48955439\n",
      "Iteration 426, loss = 0.46892213\n",
      "Iteration 428, loss = 0.46829892\n",
      "Iteration 264, loss = 0.14689410\n",
      "Iteration 200, loss = 0.06582156\n",
      "Iteration 432, loss = 0.56786255\n",
      "Iteration 199, loss = 0.16062782\n",
      "Iteration 441, loss = 0.51018849\n",
      "Iteration 295, loss = 0.49775187\n",
      "Iteration 433, loss = 0.48924818\n",
      "Iteration 110, loss = 0.41519032\n",
      "Iteration 265, loss = 0.20114675\n",
      "Iteration 427, loss = 0.46893381\n",
      "Iteration 429, loss = 0.46817710\n",
      "Iteration 197, loss = 0.15458857\n",
      "Iteration 442, loss = 0.51051213\n",
      "Iteration 433, loss = 0.56796895\n",
      "Iteration 161, loss = 0.36889234\n",
      "Iteration 434, loss = 0.48920026\n",
      "Iteration 428, loss = 0.46824404\n",
      "Iteration 430, loss = 0.46805448\n",
      "Iteration 265, loss = 0.14644313\n",
      "Iteration 296, loss = 0.49763474\n",
      "Iteration 443, loss = 0.51026565\n",
      "Iteration 434, loss = 0.56813588\n",
      "Iteration 165, loss = 0.27162975\n",
      "Iteration 266, loss = 0.20081173\n",
      "Iteration 201, loss = 0.06562719\n",
      "Iteration 435, loss = 0.48913767\n",
      "Iteration 200, loss = 0.16039392\n",
      "Iteration 429, loss = 0.46811074\n",
      "Iteration 431, loss = 0.46770955\n",
      "Iteration 444, loss = 0.51056730\n",
      "Iteration 435, loss = 0.56788964\n",
      "Iteration 198, loss = 0.15417918\n",
      "Iteration 266, loss = 0.14659784\n",
      "Iteration 436, loss = 0.48903125\n",
      "Iteration 432, loss = 0.46792325\n",
      "Iteration 297, loss = 0.49743591\n",
      "Iteration 430, loss = 0.46831197\n",
      "Iteration 445, loss = 0.50998366\n",
      "Iteration 267, loss = 0.20105440\n",
      "Iteration 436, loss = 0.56761723\n",
      "Iteration 162, loss = 0.36845604\n",
      "Iteration 437, loss = 0.48921020\n",
      "Iteration 433, loss = 0.46836041\n",
      "Iteration 431, loss = 0.46782434\n",
      "Iteration 202, loss = 0.06552941\n",
      "Iteration 446, loss = 0.50977097\n",
      "Iteration 437, loss = 0.56701207\n",
      "Iteration 267, loss = 0.14622966\n",
      "Iteration 201, loss = 0.16051315\n",
      "Iteration 166, loss = 0.27191605\n",
      "Iteration 438, loss = 0.48911211\n",
      "Iteration 298, loss = 0.49789525\n",
      "Iteration 434, loss = 0.46802132\n",
      "Iteration 199, loss = 0.15389575\n",
      "Iteration 447, loss = 0.51041281\n",
      "Iteration 432, loss = 0.46828475\n",
      "Iteration 268, loss = 0.20125408\n",
      "Iteration 438, loss = 0.56772903\n",
      "Iteration 439, loss = 0.48955144\n",
      "Iteration 111, loss = 0.41502893\n",
      "Iteration 435, loss = 0.46781052\n",
      "Iteration 448, loss = 0.50972331\n",
      "Iteration 433, loss = 0.46875432\n",
      "Iteration 268, loss = 0.14633286\n",
      "Iteration 299, loss = 0.49759884\n",
      "Iteration 439, loss = 0.56733962\n",
      "Iteration 203, loss = 0.06544725\n",
      "Iteration 440, loss = 0.48891956\n",
      "Iteration 269, loss = 0.20044812\n",
      "Iteration 163, loss = 0.36796595\n",
      "Iteration 436, loss = 0.46789276\n",
      "Iteration 202, loss = 0.16020368\n",
      "Iteration 449, loss = 0.50971803\n",
      "Iteration 434, loss = 0.46791990\n",
      "Iteration 440, loss = 0.56747091\n",
      "Iteration 200, loss = 0.15386881\n",
      "Iteration 441, loss = 0.48870532\n",
      "Iteration 167, loss = 0.27092753\n",
      "Iteration 269, loss = 0.14609970\n",
      "Iteration 437, loss = 0.46838043\n",
      "Iteration 450, loss = 0.50982902\n",
      "Iteration 300, loss = 0.49744954\n",
      "Iteration 435, loss = 0.46756914\n",
      "Iteration 441, loss = 0.56747089\n",
      "Iteration 270, loss = 0.20034618\n",
      "Iteration 442, loss = 0.48889085\n",
      "Iteration 451, loss = 0.50965109\n",
      "Iteration 438, loss = 0.46813303\n",
      "Iteration 204, loss = 0.06536517\n",
      "Iteration 436, loss = 0.46741080\n",
      "Iteration 203, loss = 0.15976806\n",
      "Iteration 442, loss = 0.56733360\n",
      "Iteration 443, loss = 0.48888431\n",
      "Iteration 270, loss = 0.14587598\n",
      "Iteration 301, loss = 0.49735226\n",
      "Iteration 439, loss = 0.46766447\n",
      "Iteration 452, loss = 0.51012600\n",
      "Iteration 201, loss = 0.15373681\n",
      "Iteration 164, loss = 0.36754226\n",
      "Iteration 437, loss = 0.46773774\n",
      "Iteration 271, loss = 0.20036669\n",
      "Iteration 443, loss = 0.56728406\n",
      "Iteration 444, loss = 0.48878013\n",
      "Iteration 440, loss = 0.46762922\n",
      "Iteration 453, loss = 0.50941321\n",
      "Iteration 168, loss = 0.27095806\n",
      "Iteration 438, loss = 0.46757477\n",
      "Iteration 271, loss = 0.14571967\n",
      "Iteration 444, loss = 0.56740675\n",
      "Iteration 302, loss = 0.49772006\n",
      "Iteration 205, loss = 0.06523155\n",
      "Iteration 445, loss = 0.48875871\n",
      "Iteration 204, loss = 0.15984274\n",
      "Iteration 112, loss = 0.41449664\n",
      "Iteration 441, loss = 0.46751901\n",
      "Iteration 454, loss = 0.50973129\n",
      "Iteration 439, loss = 0.46774092\n",
      "Iteration 272, loss = 0.20019083\n",
      "Iteration 445, loss = 0.56680018\n",
      "Iteration 202, loss = 0.15331535\n",
      "Iteration 446, loss = 0.48878014\n",
      "Iteration 455, loss = 0.50932475\n",
      "Iteration 442, loss = 0.46755828\n",
      "Iteration 440, loss = 0.46718619\n",
      "Iteration 272, loss = 0.14584353\n",
      "Iteration 165, loss = 0.36728079\n",
      "Iteration 303, loss = 0.49738341\n",
      "Iteration 446, loss = 0.56719128\n",
      "Iteration 447, loss = 0.48865521\n",
      "Iteration 273, loss = 0.20009753\n",
      "Iteration 456, loss = 0.50910021\n",
      "Iteration 443, loss = 0.46748124\n",
      "Iteration 206, loss = 0.06537850\n",
      "Iteration 441, loss = 0.46787392\n",
      "Iteration 169, loss = 0.27082983\n",
      "Iteration 205, loss = 0.15974603\n",
      "Iteration 447, loss = 0.56672863\n",
      "Iteration 448, loss = 0.48854589\n",
      "Iteration 457, loss = 0.50909112\n",
      "Iteration 273, loss = 0.14552958\n",
      "Iteration 444, loss = 0.46720908\n",
      "Iteration 304, loss = 0.49688615\n",
      "Iteration 442, loss = 0.46733967\n",
      "Iteration 203, loss = 0.15368755\n",
      "Iteration 448, loss = 0.56732940\n",
      "Iteration 449, loss = 0.48860351\n",
      "Iteration 274, loss = 0.19970633\n",
      "Iteration 458, loss = 0.50898054\n",
      "Iteration 445, loss = 0.46757433\n",
      "Iteration 443, loss = 0.46754139\n",
      "Iteration 449, loss = 0.56652527\n",
      "Iteration 166, loss = 0.36723623\n",
      "Iteration 207, loss = 0.06479319\n",
      "Iteration 450, loss = 0.48841246\n",
      "Iteration 206, loss = 0.15962091\n",
      "Iteration 274, loss = 0.14521123\n",
      "Iteration 459, loss = 0.50924963\n",
      "Iteration 305, loss = 0.49708568\n",
      "Iteration 446, loss = 0.46738375\n",
      "Iteration 444, loss = 0.46720203\n",
      "Iteration 170, loss = 0.27045683\n",
      "Iteration 450, loss = 0.56692776\n",
      "Iteration 275, loss = 0.19989295\n",
      "Iteration 451, loss = 0.48816403\n",
      "Iteration 204, loss = 0.15346499\n",
      "Iteration 113, loss = 0.41404767\n",
      "Iteration 460, loss = 0.50920340\n",
      "Iteration 447, loss = 0.46759775\n",
      "Iteration 445, loss = 0.46718318\n",
      "Iteration 451, loss = 0.56644331\n",
      "Iteration 275, loss = 0.14537861\n",
      "Iteration 452, loss = 0.48866071\n",
      "Iteration 306, loss = 0.49724195\n",
      "Iteration 461, loss = 0.50998035\n",
      "Iteration 208, loss = 0.06472241\n",
      "Iteration 448, loss = 0.46746720\n",
      "Iteration 446, loss = 0.46689950\n",
      "Iteration 276, loss = 0.19990820\n",
      "Iteration 207, loss = 0.15952281\n",
      "Iteration 452, loss = 0.56654051\n",
      "Iteration 453, loss = 0.48830038\n",
      "Iteration 167, loss = 0.36703119\n",
      "Iteration 462, loss = 0.50874972\n",
      "Iteration 449, loss = 0.46765756\n",
      "Iteration 447, loss = 0.46692824\n",
      "Iteration 205, loss = 0.15296970\n",
      "Iteration 276, loss = 0.14516014\n",
      "Iteration 307, loss = 0.49690581\n",
      "Iteration 453, loss = 0.56681240\n",
      "Iteration 171, loss = 0.26998992\n",
      "Iteration 454, loss = 0.48863238\n",
      "Iteration 463, loss = 0.50863326\n",
      "Iteration 277, loss = 0.19978814\n",
      "Iteration 450, loss = 0.46713198\n",
      "Iteration 448, loss = 0.46684992\n",
      "Iteration 209, loss = 0.06483975\n",
      "Iteration 454, loss = 0.56630033\n",
      "Iteration 455, loss = 0.48840950\n",
      "Iteration 464, loss = 0.50884158\n",
      "Iteration 208, loss = 0.15931139\n",
      "Iteration 451, loss = 0.46725026\n",
      "Iteration 277, loss = 0.14514752\n",
      "Iteration 308, loss = 0.49665435\n",
      "Iteration 449, loss = 0.46706392\n",
      "Iteration 455, loss = 0.56633221\n",
      "Iteration 206, loss = 0.15273513\n",
      "Iteration 465, loss = 0.50916478\n",
      "Iteration 456, loss = 0.48835797\n",
      "Iteration 278, loss = 0.19952767\n",
      "Iteration 168, loss = 0.36639021\n",
      "Iteration 452, loss = 0.46701392\n",
      "Iteration 450, loss = 0.46696634\n",
      "Iteration 456, loss = 0.56639935\n",
      "Iteration 172, loss = 0.26961949\n",
      "Iteration 466, loss = 0.50857618\n",
      "Iteration 457, loss = 0.48809488\n",
      "Iteration 114, loss = 0.41391252\n",
      "Iteration 278, loss = 0.14497546\n",
      "Iteration 210, loss = 0.06451530\n",
      "Iteration 309, loss = 0.49671980\n",
      "Iteration 453, loss = 0.46706709\n",
      "Iteration 209, loss = 0.15931097\n",
      "Iteration 451, loss = 0.46638468\n",
      "Iteration 279, loss = 0.19954358\n",
      "Iteration 457, loss = 0.56654590\n",
      "Iteration 458, loss = 0.48820108\n",
      "Iteration 467, loss = 0.50857935\n",
      "Iteration 454, loss = 0.46697637\n",
      "Iteration 207, loss = 0.15271811\n",
      "Iteration 452, loss = 0.46683198\n",
      "Iteration 279, loss = 0.14491972\n",
      "Iteration 458, loss = 0.56645913\n",
      "Iteration 468, loss = 0.50873775\n",
      "Iteration 459, loss = 0.48843032\n",
      "Iteration 310, loss = 0.49692878\n",
      "Iteration 455, loss = 0.46707666\n",
      "Iteration 169, loss = 0.36645153\n",
      "Iteration 211, loss = 0.06448817\n",
      "Iteration 280, loss = 0.19926956\n",
      "Iteration 453, loss = 0.46666455\n",
      "Iteration 459, loss = 0.56615585\n",
      "Iteration 469, loss = 0.50885067\n",
      "Iteration 210, loss = 0.15906619\n",
      "Iteration 460, loss = 0.48796949\n",
      "Iteration 173, loss = 0.26936158\n",
      "Iteration 456, loss = 0.46684537\n",
      "Iteration 280, loss = 0.14496737\n",
      "Iteration 454, loss = 0.46684034\n",
      "Iteration 311, loss = 0.49653265\n",
      "Iteration 460, loss = 0.56652793\n",
      "Iteration 470, loss = 0.50855754\n",
      "Iteration 208, loss = 0.15259071\n",
      "Iteration 461, loss = 0.48819488\n",
      "Iteration 281, loss = 0.19923933\n",
      "Iteration 457, loss = 0.46675938\n",
      "Iteration 455, loss = 0.46676767\n",
      "Iteration 461, loss = 0.56639373\n",
      "Iteration 471, loss = 0.50819264\n",
      "Iteration 212, loss = 0.06432482\n",
      "Iteration 462, loss = 0.48804519\n",
      "Iteration 281, loss = 0.14473712\n",
      "Iteration 211, loss = 0.15881497\n",
      "Iteration 458, loss = 0.46678767\n",
      "Iteration 312, loss = 0.49668193\n",
      "Iteration 170, loss = 0.36599315\n",
      "Iteration 456, loss = 0.46670616\n",
      "Iteration 462, loss = 0.56607302\n",
      "Iteration 472, loss = 0.50890965\n",
      "Iteration 115, loss = 0.41313868\n",
      "Iteration 282, loss = 0.19919218\n",
      "Iteration 463, loss = 0.48792480\n",
      "Iteration 174, loss = 0.26899379\n",
      "Iteration 209, loss = 0.15231075\n",
      "Iteration 459, loss = 0.46709393\n",
      "Iteration 457, loss = 0.46642864\n",
      "Iteration 463, loss = 0.56567857\n",
      "Iteration 473, loss = 0.50783370\n",
      "Iteration 282, loss = 0.14435043\n",
      "Iteration 464, loss = 0.48826103\n",
      "Iteration 313, loss = 0.49652716\n",
      "Iteration 213, loss = 0.06431511\n",
      "Iteration 460, loss = 0.46660712\n",
      "Iteration 458, loss = 0.46625674\n",
      "Iteration 283, loss = 0.19938087\n",
      "Iteration 212, loss = 0.15882616\n",
      "Iteration 464, loss = 0.56624866\n",
      "Iteration 474, loss = 0.50867457\n",
      "Iteration 465, loss = 0.48824278\n",
      "Iteration 461, loss = 0.46670642\n",
      "Iteration 171, loss = 0.36611128\n",
      "Iteration 283, loss = 0.14449609\n",
      "Iteration 459, loss = 0.46606369\n",
      "Iteration 465, loss = 0.56551913\n",
      "Iteration 475, loss = 0.50813808\n",
      "Iteration 210, loss = 0.15212047\n",
      "Iteration 314, loss = 0.49657835\n",
      "Iteration 466, loss = 0.48808366\n",
      "Iteration 175, loss = 0.26883649\n",
      "Iteration 284, loss = 0.19898855\n",
      "Iteration 462, loss = 0.46653096\n",
      "Iteration 460, loss = 0.46625652\n",
      "Iteration 466, loss = 0.56561081\n",
      "Iteration 214, loss = 0.06421707\n",
      "Iteration 476, loss = 0.50823463\n",
      "Iteration 213, loss = 0.15861863\n",
      "Iteration 467, loss = 0.48787674\n",
      "Iteration 284, loss = 0.14442370\n",
      "Iteration 463, loss = 0.46651448\n",
      "Iteration 477, loss = 0.50810892\n",
      "Iteration 461, loss = 0.46604876\n",
      "Iteration 315, loss = 0.49613289\n",
      "Iteration 467, loss = 0.56570868\n",
      "Iteration 285, loss = 0.19884541\n",
      "Iteration 211, loss = 0.15190071\n",
      "Iteration 468, loss = 0.48821128\n",
      "Iteration 464, loss = 0.46642947\n",
      "Iteration 478, loss = 0.50860962\n",
      "Iteration 172, loss = 0.36545453\n",
      "Iteration 462, loss = 0.46628814\n",
      "Iteration 116, loss = 0.41292599\n",
      "Iteration 468, loss = 0.56613411\n",
      "Iteration 285, loss = 0.14389072\n",
      "Iteration 469, loss = 0.48795237\n",
      "Iteration 215, loss = 0.06399239\n",
      "Iteration 176, loss = 0.26852863\n",
      "Iteration 316, loss = 0.49610335\n",
      "Iteration 465, loss = 0.46625240\n",
      "Iteration 479, loss = 0.50811307\n",
      "Iteration 214, loss = 0.15856827\n",
      "Iteration 463, loss = 0.46601120\n",
      "Iteration 469, loss = 0.56522804\n",
      "Iteration 286, loss = 0.19862164\n",
      "Iteration 470, loss = 0.48786153\n",
      "Iteration 480, loss = 0.50818130\n",
      "Iteration 466, loss = 0.46640794\n",
      "Iteration 212, loss = 0.15194941\n",
      "Iteration 464, loss = 0.46608375\n",
      "Iteration 470, loss = 0.56522253\n",
      "Iteration 286, loss = 0.14375395\n",
      "Iteration 471, loss = 0.48746296\n",
      "Iteration 317, loss = 0.49585893\n",
      "Iteration 481, loss = 0.50769046\n",
      "Iteration 467, loss = 0.46704469\n",
      "Iteration 287, loss = 0.19859941\n",
      "Iteration 216, loss = 0.06385285\n",
      "Iteration 465, loss = 0.46606985\n",
      "Iteration 471, loss = 0.56584714\n",
      "Iteration 173, loss = 0.36501796\n",
      "Iteration 215, loss = 0.15831793\n",
      "Iteration 472, loss = 0.48749796\n",
      "Iteration 482, loss = 0.50731085\n",
      "Iteration 468, loss = 0.46623323\n",
      "Iteration 177, loss = 0.26800504\n",
      "Iteration 287, loss = 0.14385020\n",
      "Iteration 466, loss = 0.46583282\n",
      "Iteration 318, loss = 0.49621668\n",
      "Iteration 472, loss = 0.56531083\n",
      "Iteration 213, loss = 0.15169835\n",
      "Iteration 473, loss = 0.48787204\n",
      "Iteration 469, loss = 0.46604738\n",
      "Iteration 288, loss = 0.19892855\n",
      "Iteration 483, loss = 0.50828025\n",
      "Iteration 467, loss = 0.46587240\n",
      "Iteration 473, loss = 0.56498375\n",
      "Iteration 474, loss = 0.48749883\n",
      "Iteration 217, loss = 0.06378863\n",
      "Iteration 470, loss = 0.46598481\n",
      "Iteration 484, loss = 0.50809159\n",
      "Iteration 288, loss = 0.14378337\n",
      "Iteration 216, loss = 0.15811222\n",
      "Iteration 117, loss = 0.41294200\n",
      "Iteration 319, loss = 0.49643114\n",
      "Iteration 468, loss = 0.46561264\n",
      "Iteration 474, loss = 0.56485266\n",
      "Iteration 475, loss = 0.48767583\n",
      "Iteration 174, loss = 0.36501689\n",
      "Iteration 289, loss = 0.19847463\n",
      "Iteration 471, loss = 0.46602702\n",
      "Iteration 485, loss = 0.50760722\n",
      "Iteration 214, loss = 0.15146382\n",
      "Iteration 178, loss = 0.26827165\n",
      "Iteration 469, loss = 0.46577345\n",
      "Iteration 476, loss = 0.48762672\n",
      "Iteration 475, loss = 0.56495621\n",
      "Iteration 289, loss = 0.14329419\n",
      "Iteration 472, loss = 0.46679351\n",
      "Iteration 320, loss = 0.49592425\n",
      "Iteration 486, loss = 0.50746451\n",
      "Iteration 218, loss = 0.06358736\n",
      "Iteration 470, loss = 0.46602583\n",
      "Iteration 290, loss = 0.19811790\n",
      "Iteration 477, loss = 0.48782276\n",
      "Iteration 217, loss = 0.15814495\n",
      "Iteration 476, loss = 0.56536552\n",
      "Iteration 473, loss = 0.46590248\n",
      "Iteration 487, loss = 0.50785527\n",
      "Iteration 471, loss = 0.46600583\n",
      "Iteration 290, loss = 0.14333253\n",
      "Iteration 478, loss = 0.48725246\n",
      "Iteration 215, loss = 0.15159933\n",
      "Iteration 477, loss = 0.56495967\n",
      "Iteration 175, loss = 0.36474110\n",
      "Iteration 321, loss = 0.49584344\n",
      "Iteration 474, loss = 0.46576546\n",
      "Iteration 488, loss = 0.50758000\n",
      "Iteration 291, loss = 0.19824936\n",
      "Iteration 179, loss = 0.26766858\n",
      "Iteration 472, loss = 0.46535574\n",
      "Iteration 479, loss = 0.48739511\n",
      "Iteration 219, loss = 0.06363749\n",
      "Iteration 478, loss = 0.56480890\n",
      "Iteration 475, loss = 0.46581257\n",
      "Iteration 218, loss = 0.15804552\n",
      "Iteration 489, loss = 0.50746675\n",
      "Iteration 291, loss = 0.14321710\n",
      "Iteration 322, loss = 0.49591168\n",
      "Iteration 473, loss = 0.46552812\n",
      "Iteration 480, loss = 0.48763157\n",
      "Iteration 479, loss = 0.56457144\n",
      "Iteration 476, loss = 0.46621951\n",
      "Iteration 490, loss = 0.50759040\n",
      "Iteration 118, loss = 0.41246489\n",
      "Iteration 292, loss = 0.19817627\n",
      "Iteration 216, loss = 0.15119474\n",
      "Iteration 481, loss = 0.48761809\n",
      "Iteration 474, loss = 0.46562388\n",
      "Iteration 480, loss = 0.56447227\n",
      "Iteration 477, loss = 0.46558186\n",
      "Iteration 176, loss = 0.36448192\n",
      "Iteration 491, loss = 0.50754601\n",
      "Iteration 220, loss = 0.06363471\n",
      "Iteration 292, loss = 0.14319137\n",
      "Iteration 323, loss = 0.49615267\n",
      "Iteration 219, loss = 0.15796603\n",
      "Iteration 482, loss = 0.48712674\n",
      "Iteration 180, loss = 0.26727325\n",
      "Iteration 475, loss = 0.46511078\n",
      "Iteration 481, loss = 0.56516059\n",
      "Iteration 478, loss = 0.46623025\n",
      "Iteration 293, loss = 0.19805453\n",
      "Iteration 492, loss = 0.50693406\n",
      "Iteration 483, loss = 0.48725574\n",
      "Iteration 217, loss = 0.15110235\n",
      "Iteration 476, loss = 0.46582776\n",
      "Iteration 482, loss = 0.56532541\n",
      "Iteration 479, loss = 0.46568728\n",
      "Iteration 293, loss = 0.14318738\n",
      "Iteration 324, loss = 0.49564913\n",
      "Iteration 493, loss = 0.50719289\n",
      "Iteration 484, loss = 0.48772969\n",
      "Iteration 221, loss = 0.06348226\n",
      "Iteration 477, loss = 0.46559276\n",
      "Iteration 483, loss = 0.56458032\n",
      "Iteration 294, loss = 0.19793700\n",
      "Iteration 220, loss = 0.15790488\n",
      "Iteration 480, loss = 0.46572216\n",
      "Iteration 494, loss = 0.50703907\n",
      "Iteration 177, loss = 0.36426720\n",
      "Iteration 485, loss = 0.48712520\n",
      "Iteration 478, loss = 0.46513409\n",
      "Iteration 325, loss = 0.49587479\n",
      "Iteration 484, loss = 0.56463298\n",
      "Iteration 294, loss = 0.14279433\n",
      "Iteration 181, loss = 0.26734337\n",
      "Iteration 481, loss = 0.46565147\n",
      "Iteration 495, loss = 0.50729891\n",
      "Iteration 218, loss = 0.15103038\n",
      "Iteration 295, loss = 0.19784528\n",
      "Iteration 486, loss = 0.48682600\n",
      "Iteration 479, loss = 0.46544975\n",
      "Iteration 485, loss = 0.56480432\n",
      "Iteration 482, loss = 0.46560129\n",
      "Iteration 496, loss = 0.50704325\n",
      "Iteration 222, loss = 0.06322502\n",
      "Iteration 119, loss = 0.41201554\n",
      "Iteration 221, loss = 0.15753726\n",
      "Iteration 326, loss = 0.49573043\n",
      "Iteration 295, loss = 0.14260783\n",
      "Iteration 487, loss = 0.48713225\n",
      "Iteration 480, loss = 0.46507078\n",
      "Iteration 486, loss = 0.56424470\n",
      "Iteration 483, loss = 0.46542187\n",
      "Iteration 497, loss = 0.50727410\n",
      "Iteration 296, loss = 0.19823971\n",
      "Iteration 178, loss = 0.36420177\n",
      "Iteration 488, loss = 0.48712451\n",
      "Iteration 219, loss = 0.15071961\n",
      "Iteration 487, loss = 0.56454233\n",
      "Iteration 481, loss = 0.46546732\n",
      "Iteration 484, loss = 0.46556247\n",
      "Iteration 498, loss = 0.50678550\n",
      "Iteration 182, loss = 0.26692923\n",
      "Iteration 327, loss = 0.49549636\n",
      "Iteration 296, loss = 0.14271040\n",
      "Iteration 223, loss = 0.06303072\n",
      "Iteration 222, loss = 0.15731705\n",
      "Iteration 489, loss = 0.48709370\n",
      "Iteration 482, loss = 0.46492211\n",
      "Iteration 488, loss = 0.56451447\n",
      "Iteration 499, loss = 0.50664238\n",
      "Iteration 485, loss = 0.46615867\n",
      "Iteration 297, loss = 0.19758257\n",
      "Iteration 490, loss = 0.48692736\n",
      "Iteration 483, loss = 0.46508166\n",
      "Iteration 489, loss = 0.56410416\n",
      "Iteration 328, loss = 0.49526282\n",
      "Iteration 297, loss = 0.14290974\n",
      "Iteration 500, loss = 0.50677474\n",
      "Iteration 486, loss = 0.46528766\n",
      "Iteration 220, loss = 0.15080908\n",
      "Iteration 179, loss = 0.36394059\n",
      "Iteration 491, loss = 0.48679518\n",
      "Iteration 484, loss = 0.46481661\n",
      "Iteration 490, loss = 0.56437825\n",
      "Iteration 298, loss = 0.19778516\n",
      "Iteration 224, loss = 0.06301033\n",
      "Iteration 501, loss = 0.50691477\n",
      "Iteration 487, loss = 0.46542854\n",
      "Iteration 223, loss = 0.15754869\n",
      "Iteration 183, loss = 0.26708141\n",
      "Iteration 329, loss = 0.49563462\n",
      "Iteration 298, loss = 0.14259265\n",
      "Iteration 485, loss = 0.46481692\n",
      "Iteration 492, loss = 0.48657661\n",
      "Iteration 491, loss = 0.56401868\n",
      "Iteration 502, loss = 0.50703462\n",
      "Iteration 120, loss = 0.41154385\n",
      "Iteration 488, loss = 0.46536596\n",
      "Iteration 221, loss = 0.15050278\n",
      "Iteration 299, loss = 0.19736055\n",
      "Iteration 486, loss = 0.46526253\n",
      "Iteration 493, loss = 0.48675121\n",
      "Iteration 492, loss = 0.56434278\n",
      "Iteration 503, loss = 0.50674115\n",
      "Iteration 489, loss = 0.46539804\n",
      "Iteration 225, loss = 0.06305484\n",
      "Iteration 330, loss = 0.49514619\n",
      "Iteration 299, loss = 0.14227921\n",
      "Iteration 224, loss = 0.15726207\n",
      "Iteration 180, loss = 0.36354772\n",
      "Iteration 487, loss = 0.46442473\n",
      "Iteration 494, loss = 0.48660845\n",
      "Iteration 493, loss = 0.56415456\n",
      "Iteration 504, loss = 0.50645569\n",
      "Iteration 490, loss = 0.46522231\n",
      "Iteration 184, loss = 0.26612547\n",
      "Iteration 300, loss = 0.19755166\n",
      "Iteration 488, loss = 0.46461392\n",
      "Iteration 495, loss = 0.48680827\n",
      "Iteration 494, loss = 0.56397635\n",
      "Iteration 505, loss = 0.50697580\n",
      "Iteration 331, loss = 0.49524390\n",
      "Iteration 300, loss = 0.14210785\n",
      "Iteration 222, loss = 0.15026339\n",
      "Iteration 491, loss = 0.46532221\n",
      "Iteration 496, loss = 0.48694299\n",
      "Iteration 489, loss = 0.46452494\n",
      "Iteration 226, loss = 0.06287118\n",
      "Iteration 225, loss = 0.15701836\n",
      "Iteration 506, loss = 0.50683588\n",
      "Iteration 495, loss = 0.56401276\n",
      "Iteration 301, loss = 0.19708017\n",
      "Iteration 492, loss = 0.46504329\n",
      "Iteration 497, loss = 0.48684782\n",
      "Iteration 490, loss = 0.46460334\n",
      "Iteration 332, loss = 0.49575069\n",
      "Iteration 181, loss = 0.36349137\n",
      "Iteration 301, loss = 0.14200587\n",
      "Iteration 507, loss = 0.50603764\n",
      "Iteration 496, loss = 0.56375975\n",
      "Iteration 493, loss = 0.46528259\n",
      "Iteration 185, loss = 0.26631764\n",
      "Iteration 498, loss = 0.48642792\n",
      "Iteration 223, loss = 0.15009365\n",
      "Iteration 491, loss = 0.46423564\n",
      "Iteration 302, loss = 0.19736444\n",
      "Iteration 121, loss = 0.41118466\n",
      "Iteration 508, loss = 0.50665101\n",
      "Iteration 497, loss = 0.56348225\n",
      "Iteration 494, loss = 0.46556930\n",
      "Iteration 227, loss = 0.06275765\n",
      "Iteration 226, loss = 0.15710582\n",
      "Iteration 499, loss = 0.48651357\n",
      "Iteration 333, loss = 0.49528923\n",
      "Iteration 302, loss = 0.14191766\n",
      "Iteration 492, loss = 0.46433883\n",
      "Iteration 498, loss = 0.56359081\n",
      "Iteration 509, loss = 0.50635288\n",
      "Iteration 495, loss = 0.46473775\n",
      "Iteration 500, loss = 0.48664001\n",
      "Iteration 303, loss = 0.19708482\n",
      "Iteration 493, loss = 0.46448427\n",
      "Iteration 499, loss = 0.56307936\n",
      "Iteration 510, loss = 0.50691225\n",
      "Iteration 182, loss = 0.36286376\n",
      "Iteration 224, loss = 0.15014500\n",
      "Iteration 496, loss = 0.46585157\n",
      "Iteration 334, loss = 0.49482336\n",
      "Iteration 303, loss = 0.14186826\n",
      "Iteration 501, loss = 0.48660567\n",
      "Iteration 186, loss = 0.26603960\n",
      "Iteration 228, loss = 0.06254512\n",
      "Iteration 494, loss = 0.46438977\n",
      "Iteration 227, loss = 0.15699290\n",
      "Iteration 511, loss = 0.50677098\n",
      "Iteration 500, loss = 0.56327827\n",
      "Iteration 497, loss = 0.46507975\n",
      "Iteration 502, loss = 0.48640125\n",
      "Iteration 304, loss = 0.19718887\n",
      "Iteration 495, loss = 0.46439867\n",
      "Iteration 335, loss = 0.49481836\n",
      "Iteration 512, loss = 0.50614938\n",
      "Iteration 501, loss = 0.56372322\n",
      "Iteration 304, loss = 0.14160516\n",
      "Iteration 498, loss = 0.46483532\n",
      "Iteration 503, loss = 0.48702161\n",
      "Iteration 225, loss = 0.15004621\n",
      "Iteration 496, loss = 0.46398251\n",
      "Iteration 502, loss = 0.56319801\n",
      "Iteration 513, loss = 0.50697690\n",
      "Iteration 305, loss = 0.19721760\n",
      "Iteration 229, loss = 0.06257823\n",
      "Iteration 183, loss = 0.36291756\n",
      "Iteration 499, loss = 0.46476468\n",
      "Iteration 228, loss = 0.15688747\n",
      "Iteration 504, loss = 0.48638831\n",
      "Iteration 336, loss = 0.49513915\n",
      "Iteration 187, loss = 0.26570336\n",
      "Iteration 305, loss = 0.14155670\n",
      "Iteration 497, loss = 0.46425386\n",
      "Iteration 122, loss = 0.41078359\n",
      "Iteration 503, loss = 0.56358710\n",
      "Iteration 514, loss = 0.50594942\n",
      "Iteration 500, loss = 0.46490590\n",
      "Iteration 505, loss = 0.48612360\n",
      "Iteration 504, loss = 0.56380870\n",
      "Iteration 498, loss = 0.46382580\n",
      "Iteration 226, loss = 0.14979058\n",
      "Iteration 515, loss = 0.50614245\n",
      "Iteration 306, loss = 0.19670041\n",
      "Iteration 501, loss = 0.46494977\n",
      "Iteration 506, loss = 0.48666001\n",
      "Iteration 337, loss = 0.49501228\n",
      "Iteration 306, loss = 0.14163204\n",
      "Iteration 230, loss = 0.06236734\n",
      "Iteration 229, loss = 0.15661973\n",
      "Iteration 505, loss = 0.56314460\n",
      "Iteration 499, loss = 0.46446735\n",
      "Iteration 516, loss = 0.50590944\n",
      "Iteration 507, loss = 0.48690880\n",
      "Iteration 502, loss = 0.46492882\n",
      "Iteration 184, loss = 0.36256685\n",
      "Iteration 307, loss = 0.19676597\n",
      "Iteration 506, loss = 0.56278794\n",
      "Iteration 500, loss = 0.46408077\n",
      "Iteration 188, loss = 0.26555898\n",
      "Iteration 517, loss = 0.50570042\n",
      "Iteration 338, loss = 0.49506743\n",
      "Iteration 508, loss = 0.48613945\n",
      "Iteration 503, loss = 0.46516293\n",
      "Iteration 227, loss = 0.14981663\n",
      "Iteration 307, loss = 0.14135289\n",
      "Iteration 507, loss = 0.56296148\n",
      "Iteration 501, loss = 0.46359643\n",
      "Iteration 518, loss = 0.50561458\n",
      "Iteration 231, loss = 0.06222531\n",
      "Iteration 230, loss = 0.15668820\n",
      "Iteration 509, loss = 0.48626763\n",
      "Iteration 504, loss = 0.46499237\n",
      "Iteration 308, loss = 0.19657603\n",
      "Iteration 508, loss = 0.56347965\n",
      "Iteration 519, loss = 0.50624318\n",
      "Iteration 502, loss = 0.46372535\n",
      "Iteration 339, loss = 0.49486761\n",
      "Iteration 308, loss = 0.14130594\n",
      "Iteration 510, loss = 0.48583602\n",
      "Iteration 505, loss = 0.46465029\n",
      "Iteration 185, loss = 0.36283099\n",
      "Iteration 509, loss = 0.56321785\n",
      "Iteration 123, loss = 0.41067136\n",
      "Iteration 228, loss = 0.14926716\n",
      "Iteration 520, loss = 0.50594783\n",
      "Iteration 503, loss = 0.46395115\n",
      "Iteration 189, loss = 0.26498309\n",
      "Iteration 309, loss = 0.19633393\n",
      "Iteration 511, loss = 0.48590971\n",
      "Iteration 506, loss = 0.46456036\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 232, loss = 0.06229516\n",
      "Iteration 231, loss = 0.15652981\n",
      "Iteration 340, loss = 0.49456395\n",
      "Iteration 510, loss = 0.56330179\n",
      "Iteration 521, loss = 0.50597618\n",
      "Iteration 504, loss = 0.46395884\n",
      "Iteration 309, loss = 0.14161750\n",
      "Iteration 512, loss = 0.48596425\n",
      "Iteration 511, loss = 0.56319719\n",
      "Iteration 522, loss = 0.50589673\n",
      "Iteration 505, loss = 0.46376107\n",
      "Iteration 310, loss = 0.19645346\n",
      "Iteration 229, loss = 0.14944288\n",
      "Iteration 513, loss = 0.48643323\n",
      "Iteration 341, loss = 0.49472628\n",
      "Iteration 310, loss = 0.14090983\n",
      "Iteration 512, loss = 0.56227469\n",
      "Iteration 523, loss = 0.50623109\n",
      "Iteration 186, loss = 0.36160882\n",
      "Iteration 506, loss = 0.46348729\n",
      "Iteration 233, loss = 0.06215130\n",
      "Iteration 232, loss = 0.15637219\n",
      "Iteration 190, loss = 0.26497279\n",
      "Iteration 514, loss = 0.48598289\n",
      "Iteration 524, loss = 0.50549182\n",
      "Iteration 513, loss = 0.56294718\n",
      "Iteration 311, loss = 0.19687478\n",
      "Iteration 507, loss = 0.46362860\n",
      "Iteration 342, loss = 0.49446654\n",
      "Iteration 311, loss = 0.14081747\n",
      "Iteration 515, loss = 0.48609944\n",
      "Iteration 525, loss = 0.50594701\n",
      "Iteration 230, loss = 0.14957619\n",
      "Iteration 514, loss = 0.56251410\n",
      "Iteration 508, loss = 0.46370352\n",
      "Iteration 516, loss = 0.48567733\n",
      "Iteration 312, loss = 0.19653682\n",
      "Iteration 234, loss = 0.06209784\n",
      "Iteration 233, loss = 0.15626815\n",
      "Iteration 526, loss = 0.50568827\n",
      "Iteration 124, loss = 0.41015540\n",
      "Iteration 343, loss = 0.49468718\n",
      "Iteration 515, loss = 0.56290032\n",
      "Iteration 187, loss = 0.36190210\n",
      "Iteration 509, loss = 0.46393314\n",
      "Iteration 312, loss = 0.14087195\n",
      "Iteration 517, loss = 0.48570995\n",
      "Iteration 191, loss = 0.26429603\n",
      "Iteration 527, loss = 0.50555007\n",
      "Iteration 516, loss = 0.56296836\n",
      "Iteration 510, loss = 0.46318627\n",
      "Iteration 231, loss = 0.14911413\n",
      "Iteration 313, loss = 0.19629738\n",
      "Iteration 518, loss = 0.48585045\n",
      "Iteration 344, loss = 0.49452129\n",
      "Iteration 528, loss = 0.50557149\n",
      "Iteration 517, loss = 0.56259264\n",
      "Iteration 235, loss = 0.06197804\n",
      "Iteration 313, loss = 0.14052172\n",
      "Iteration 234, loss = 0.15599576\n",
      "Iteration 511, loss = 0.46372964\n",
      "Iteration 519, loss = 0.48584702\n",
      "Iteration 529, loss = 0.50561390\n",
      "Iteration 518, loss = 0.56201304\n",
      "Iteration 314, loss = 0.19609868\n",
      "Iteration 512, loss = 0.46323810\n",
      "Iteration 188, loss = 0.36175361\n",
      "Iteration 345, loss = 0.49437799\n",
      "Iteration 520, loss = 0.48568565\n",
      "Iteration 232, loss = 0.14918718\n",
      "Iteration 192, loss = 0.26457870\n",
      "Iteration 314, loss = 0.14077645\n",
      "Iteration 530, loss = 0.50485247\n",
      "Iteration 519, loss = 0.56273006\n",
      "Iteration 513, loss = 0.46303058\n",
      "Iteration 236, loss = 0.06164749\n",
      "Iteration 235, loss = 0.15581195\n",
      "Iteration 521, loss = 0.48585555\n",
      "Iteration 531, loss = 0.50470567\n",
      "Iteration 315, loss = 0.19604673\n",
      "Iteration 520, loss = 0.56185726\n",
      "Iteration 346, loss = 0.49445961\n",
      "Iteration 514, loss = 0.46347356\n",
      "Iteration 522, loss = 0.48570753\n",
      "Iteration 315, loss = 0.14067201\n",
      "Iteration 532, loss = 0.50482713\n",
      "Iteration 125, loss = 0.41010835\n",
      "Iteration 521, loss = 0.56259199\n",
      "Iteration 233, loss = 0.14870378\n",
      "Iteration 515, loss = 0.46287321\n",
      "Iteration 523, loss = 0.48634558\n",
      "Iteration 189, loss = 0.36107187\n",
      "Iteration 316, loss = 0.19609917\n",
      "Iteration 533, loss = 0.50498367\n",
      "Iteration 347, loss = 0.49472409\n",
      "Iteration 236, loss = 0.15573789\n",
      "Iteration 237, loss = 0.06165635\n",
      "Iteration 193, loss = 0.26430211\n",
      "Iteration 522, loss = 0.56251171\n",
      "Iteration 516, loss = 0.46291435\n",
      "Iteration 316, loss = 0.14026055\n",
      "Iteration 524, loss = 0.48557312\n",
      "Iteration 534, loss = 0.50516594\n",
      "Iteration 523, loss = 0.56233453\n",
      "Iteration 517, loss = 0.46292211\n",
      "Iteration 317, loss = 0.19602205\n",
      "Iteration 525, loss = 0.48577159\n",
      "Iteration 348, loss = 0.49447906\n",
      "Iteration 234, loss = 0.14863395\n",
      "Iteration 535, loss = 0.50544239\n",
      "Iteration 317, loss = 0.14001929\n",
      "Iteration 524, loss = 0.56233285\n",
      "Iteration 518, loss = 0.46311498\n",
      "Iteration 237, loss = 0.15579434\n",
      "Iteration 238, loss = 0.06155238\n",
      "Iteration 526, loss = 0.48552197\n",
      "Iteration 190, loss = 0.36154879\n",
      "Iteration 536, loss = 0.50507003\n",
      "Iteration 525, loss = 0.56184331\n",
      "Iteration 519, loss = 0.46339547\n",
      "Iteration 318, loss = 0.19575612\n",
      "Iteration 194, loss = 0.26388739\n",
      "Iteration 349, loss = 0.49398428\n",
      "Iteration 527, loss = 0.48542098\n",
      "Iteration 537, loss = 0.50595138\n",
      "Iteration 318, loss = 0.13980921\n",
      "Iteration 235, loss = 0.14890771\n",
      "Iteration 526, loss = 0.56303703\n",
      "Iteration 520, loss = 0.46268982\n",
      "Iteration 528, loss = 0.48549405\n",
      "Iteration 538, loss = 0.50492163\n",
      "Iteration 319, loss = 0.19620119\n",
      "Iteration 126, loss = 0.40965570\n",
      "Iteration 239, loss = 0.06144598\n",
      "Iteration 238, loss = 0.15550356\n",
      "Iteration 350, loss = 0.49405256\n",
      "Iteration 527, loss = 0.56165633\n",
      "Iteration 521, loss = 0.46254988\n",
      "Iteration 529, loss = 0.48520991\n",
      "Iteration 319, loss = 0.13997358\n",
      "Iteration 539, loss = 0.50481960\n",
      "Iteration 528, loss = 0.56132957\n",
      "Iteration 191, loss = 0.36094796\n",
      "Iteration 522, loss = 0.46274602\n",
      "Iteration 195, loss = 0.26404836\n",
      "Iteration 530, loss = 0.48533209\n",
      "Iteration 236, loss = 0.14858789\n",
      "Iteration 320, loss = 0.19530533\n",
      "Iteration 540, loss = 0.50463535\n",
      "Iteration 351, loss = 0.49370413\n",
      "Iteration 529, loss = 0.56182991\n",
      "Iteration 320, loss = 0.14013411\n",
      "Iteration 239, loss = 0.15567621\n",
      "Iteration 523, loss = 0.46256545\n",
      "Iteration 240, loss = 0.06141774\n",
      "Iteration 531, loss = 0.48533222\n",
      "Iteration 541, loss = 0.50454105\n",
      "Iteration 530, loss = 0.56240805\n",
      "Iteration 321, loss = 0.19549474\n",
      "Iteration 524, loss = 0.46243435\n",
      "Iteration 352, loss = 0.49442101\n",
      "Iteration 532, loss = 0.48512165\n",
      "Iteration 542, loss = 0.50490780\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 531, loss = 0.56185040\n",
      "Iteration 237, loss = 0.14837163\n",
      "Iteration 321, loss = 0.13997566\n",
      "Iteration 192, loss = 0.36095340\n",
      "Iteration 525, loss = 0.46230274\n",
      "Iteration 196, loss = 0.26355490\n",
      "Iteration 533, loss = 0.48520567\n",
      "Iteration 240, loss = 0.15536289\n",
      "Iteration 241, loss = 0.06127353\n",
      "Iteration 532, loss = 0.56177467\n",
      "Iteration 322, loss = 0.19555186\n",
      "Iteration 353, loss = 0.49366499\n",
      "Iteration 526, loss = 0.46245230\n",
      "Iteration 534, loss = 0.48523240\n",
      "Iteration 322, loss = 0.13963149\n",
      "Iteration 127, loss = 0.40919121\n",
      "Iteration 533, loss = 0.56170641\n",
      "Iteration 527, loss = 0.46225535\n",
      "Iteration 238, loss = 0.14819853\n",
      "Iteration 535, loss = 0.48508983\n",
      "Iteration 534, loss = 0.56206022\n",
      "Iteration 354, loss = 0.49350553\n",
      "Iteration 323, loss = 0.19545825\n",
      "Iteration 241, loss = 0.15517223\n",
      "Iteration 242, loss = 0.06121099\n",
      "Iteration 528, loss = 0.46238846\n",
      "Iteration 193, loss = 0.36059847\n",
      "Iteration 536, loss = 0.48532259\n",
      "Iteration 323, loss = 0.13970995\n",
      "Iteration 197, loss = 0.26307825\n",
      "Iteration 535, loss = 0.56163199\n",
      "Iteration 529, loss = 0.46207073\n",
      "Iteration 355, loss = 0.49353693\n",
      "Iteration 537, loss = 0.48522097\n",
      "Iteration 324, loss = 0.19545141\n",
      "Iteration 239, loss = 0.14799525\n",
      "Iteration 536, loss = 0.56185589\n",
      "Iteration 530, loss = 0.46244518\n",
      "Iteration 324, loss = 0.13972063\n",
      "Iteration 538, loss = 0.48531918\n",
      "Iteration 242, loss = 0.15516889\n",
      "Iteration 243, loss = 0.06122379\n",
      "Iteration 537, loss = 0.56197326\n",
      "Iteration 356, loss = 0.49353981\n",
      "Iteration 325, loss = 0.19541149\n",
      "Iteration 531, loss = 0.46230552\n",
      "Iteration 539, loss = 0.48557008\n",
      "Iteration 194, loss = 0.36013672\n",
      "Iteration 198, loss = 0.26293807\n",
      "Iteration 538, loss = 0.56122856\n",
      "Iteration 325, loss = 0.13943498\n",
      "Iteration 532, loss = 0.46233285\n",
      "Iteration 240, loss = 0.14788729\n",
      "Iteration 540, loss = 0.48535946\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 357, loss = 0.49359532\n",
      "Iteration 128, loss = 0.40898531\n",
      "Iteration 539, loss = 0.56158247\n",
      "Iteration 326, loss = 0.19520874\n",
      "Iteration 243, loss = 0.15496286\n",
      "Iteration 244, loss = 0.06082589\n",
      "Iteration 533, loss = 0.46244648\n",
      "Iteration 326, loss = 0.13943326\n",
      "Iteration 540, loss = 0.56116515\n",
      "Iteration 534, loss = 0.46188147\n",
      "Iteration 358, loss = 0.49346070\n",
      "Iteration 195, loss = 0.36001599\n",
      "Iteration 241, loss = 0.14760983\n",
      "Iteration 541, loss = 0.56143183\n",
      "Iteration 199, loss = 0.26265362\n",
      "Iteration 327, loss = 0.19514673\n",
      "Iteration 535, loss = 0.46217882\n",
      "Iteration 244, loss = 0.15528948\n",
      "Iteration 327, loss = 0.13914072\n",
      "Iteration 245, loss = 0.06087968\n",
      "Iteration 542, loss = 0.56181724\n",
      "Iteration 536, loss = 0.46212259\n",
      "Iteration 359, loss = 0.49339959\n",
      "Iteration 328, loss = 0.19519404\n",
      "Iteration 543, loss = 0.56129551\n",
      "Iteration 537, loss = 0.46202555\n",
      "Iteration 242, loss = 0.14782980\n",
      "Iteration 328, loss = 0.13895260\n",
      "Iteration 544, loss = 0.56099085\n",
      "Iteration 196, loss = 0.35990651\n",
      "Iteration 245, loss = 0.15470601\n",
      "Iteration 360, loss = 0.49331195\n",
      "Iteration 200, loss = 0.26275906\n",
      "Iteration 538, loss = 0.46158801\n",
      "Iteration 246, loss = 0.06066518\n",
      "Iteration 329, loss = 0.19500366\n",
      "Iteration 545, loss = 0.56109472\n",
      "Iteration 129, loss = 0.40882763\n",
      "Iteration 329, loss = 0.13902423\n",
      "Iteration 539, loss = 0.46160373\n",
      "Iteration 546, loss = 0.56036917\n",
      "Iteration 243, loss = 0.14733218\n",
      "Iteration 361, loss = 0.49339606\n",
      "Iteration 540, loss = 0.46145806\n",
      "Iteration 330, loss = 0.19481342\n",
      "Iteration 246, loss = 0.15490833\n",
      "Iteration 547, loss = 0.56081774\n",
      "Iteration 330, loss = 0.13874222\n",
      "Iteration 247, loss = 0.06078769\n",
      "Iteration 197, loss = 0.35953983\n",
      "Iteration 541, loss = 0.46166862\n",
      "Iteration 201, loss = 0.26235291\n",
      "Iteration 362, loss = 0.49344189\n",
      "Iteration 548, loss = 0.56202629\n",
      "Iteration 331, loss = 0.19487943\n",
      "Iteration 244, loss = 0.14770871\n",
      "Iteration 542, loss = 0.46158986\n",
      "Iteration 331, loss = 0.13885115\n",
      "Iteration 549, loss = 0.56082801\n",
      "Iteration 247, loss = 0.15454138\n",
      "Iteration 363, loss = 0.49332468\n",
      "Iteration 543, loss = 0.46168054\n",
      "Iteration 248, loss = 0.06046543\n",
      "Iteration 550, loss = 0.56094640\n",
      "Iteration 332, loss = 0.19478497\n",
      "Iteration 332, loss = 0.13887281\n",
      "Iteration 544, loss = 0.46180679\n",
      "Iteration 198, loss = 0.35931143\n",
      "Iteration 202, loss = 0.26184340\n",
      "Iteration 551, loss = 0.56149665\n",
      "Iteration 245, loss = 0.14746259\n",
      "Iteration 364, loss = 0.49298579\n",
      "Iteration 130, loss = 0.40836605\n",
      "Iteration 545, loss = 0.46165620\n",
      "Iteration 248, loss = 0.15467860\n",
      "Iteration 552, loss = 0.56085343\n",
      "Iteration 333, loss = 0.19466744\n",
      "Iteration 249, loss = 0.06055899\n",
      "Iteration 333, loss = 0.13849097\n",
      "Iteration 546, loss = 0.46209297\n",
      "Iteration 553, loss = 0.56016843\n",
      "Iteration 365, loss = 0.49292941\n",
      "Iteration 334, loss = 0.19450898\n",
      "Iteration 246, loss = 0.14727498\n",
      "Iteration 547, loss = 0.46152222\n",
      "Iteration 554, loss = 0.56071275\n",
      "Iteration 199, loss = 0.35916433\n",
      "Iteration 203, loss = 0.26190656\n",
      "Iteration 334, loss = 0.13873857\n",
      "Iteration 249, loss = 0.15429357\n",
      "Iteration 548, loss = 0.46195867\n",
      "Iteration 250, loss = 0.06052504\n",
      "Iteration 366, loss = 0.49316577\n",
      "Iteration 555, loss = 0.56069172\n",
      "Iteration 335, loss = 0.19454929\n",
      "Iteration 549, loss = 0.46101678\n",
      "Iteration 335, loss = 0.13838485\n",
      "Iteration 556, loss = 0.56078553\n",
      "Iteration 247, loss = 0.14715589\n",
      "Iteration 367, loss = 0.49322857\n",
      "Iteration 250, loss = 0.15426786\n",
      "Iteration 550, loss = 0.46134577\n",
      "Iteration 557, loss = 0.56089878\n",
      "Iteration 251, loss = 0.06035928\n",
      "Iteration 200, loss = 0.35892436\n",
      "Iteration 204, loss = 0.26155429\n",
      "Iteration 336, loss = 0.19459708\n",
      "Iteration 336, loss = 0.13838449\n",
      "Iteration 131, loss = 0.40808587\n",
      "Iteration 551, loss = 0.46115795\n",
      "Iteration 558, loss = 0.56023934\n",
      "Iteration 368, loss = 0.49311852\n",
      "Iteration 248, loss = 0.14678619\n",
      "Iteration 552, loss = 0.46131336\n",
      "Iteration 559, loss = 0.56049966\n",
      "Iteration 337, loss = 0.19427344\n",
      "Iteration 251, loss = 0.15428910\n",
      "Iteration 337, loss = 0.13823753\n",
      "Iteration 553, loss = 0.46122250\n",
      "Iteration 252, loss = 0.06021152\n",
      "Iteration 560, loss = 0.56045748\n",
      "Iteration 369, loss = 0.49288010\n",
      "Iteration 205, loss = 0.26110814\n",
      "Iteration 201, loss = 0.35863253\n",
      "Iteration 554, loss = 0.46096861\n",
      "Iteration 338, loss = 0.19432881\n",
      "Iteration 561, loss = 0.56063977\n",
      "Iteration 338, loss = 0.13794066\n",
      "Iteration 249, loss = 0.14686511\n",
      "Iteration 252, loss = 0.15407376\n",
      "Iteration 555, loss = 0.46209369\n",
      "Iteration 370, loss = 0.49301477\n",
      "Iteration 562, loss = 0.56049729\n",
      "Iteration 253, loss = 0.06021081\n",
      "Iteration 556, loss = 0.46113374\n",
      "Iteration 339, loss = 0.19433832\n",
      "Iteration 339, loss = 0.13808408\n",
      "Iteration 563, loss = 0.56030044\n",
      "Iteration 371, loss = 0.49285663\n",
      "Iteration 206, loss = 0.26147722\n",
      "Iteration 132, loss = 0.40805114\n",
      "Iteration 557, loss = 0.46043312\n",
      "Iteration 250, loss = 0.14670485\n",
      "Iteration 202, loss = 0.35831967\n",
      "Iteration 564, loss = 0.55956109\n",
      "Iteration 253, loss = 0.15407840\n",
      "Iteration 340, loss = 0.19415198\n",
      "Iteration 340, loss = 0.13792990\n",
      "Iteration 558, loss = 0.46066824\n",
      "Iteration 254, loss = 0.05995752\n",
      "Iteration 565, loss = 0.56046326\n",
      "Iteration 372, loss = 0.49250491\n",
      "Iteration 559, loss = 0.46084649\n",
      "Iteration 566, loss = 0.56073380\n",
      "Iteration 341, loss = 0.19402141\n",
      "Iteration 341, loss = 0.13768820\n",
      "Iteration 251, loss = 0.14641729\n",
      "Iteration 254, loss = 0.15372898\n",
      "Iteration 560, loss = 0.46062754\n",
      "Iteration 207, loss = 0.26096919\n",
      "Iteration 203, loss = 0.35816617\n",
      "Iteration 567, loss = 0.55969921\n",
      "Iteration 373, loss = 0.49264287\n",
      "Iteration 255, loss = 0.05992519\n",
      "Iteration 561, loss = 0.46044141\n",
      "Iteration 568, loss = 0.56016290\n",
      "Iteration 342, loss = 0.13771498\n",
      "Iteration 342, loss = 0.19382538\n",
      "Iteration 374, loss = 0.49282834\n",
      "Iteration 562, loss = 0.46096399\n",
      "Iteration 569, loss = 0.56019243\n",
      "Iteration 252, loss = 0.14681787\n",
      "Iteration 255, loss = 0.15393795\n",
      "Iteration 563, loss = 0.46125465\n",
      "Iteration 256, loss = 0.05998234\n",
      "Iteration 343, loss = 0.13797171\n",
      "Iteration 570, loss = 0.55965799\n",
      "Iteration 133, loss = 0.40771031\n",
      "Iteration 343, loss = 0.19411363\n",
      "Iteration 208, loss = 0.26030434\n",
      "Iteration 204, loss = 0.35770891\n",
      "Iteration 375, loss = 0.49222957\n",
      "Iteration 564, loss = 0.46062669\n",
      "Iteration 571, loss = 0.55988780\n",
      "Iteration 253, loss = 0.14638814\n",
      "Iteration 256, loss = 0.15374254\n",
      "Iteration 344, loss = 0.13758479\n",
      "Iteration 344, loss = 0.19380700\n",
      "Iteration 565, loss = 0.46089281\n",
      "Iteration 572, loss = 0.56001011\n",
      "Iteration 376, loss = 0.49259606\n",
      "Iteration 257, loss = 0.05997644\n",
      "Iteration 566, loss = 0.46035172\n",
      "Iteration 573, loss = 0.55983705\n",
      "Iteration 209, loss = 0.26070504\n",
      "Iteration 345, loss = 0.13742416\n",
      "Iteration 205, loss = 0.35762169\n",
      "Iteration 345, loss = 0.19370974\n",
      "Iteration 567, loss = 0.46033863\n",
      "Iteration 257, loss = 0.15384063\n",
      "Iteration 377, loss = 0.49244022\n",
      "Iteration 574, loss = 0.55959465\n",
      "Iteration 254, loss = 0.14644519\n",
      "Iteration 258, loss = 0.05956845\n",
      "Iteration 568, loss = 0.46057989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 575, loss = 0.55974916\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 346, loss = 0.13732952\n",
      "Iteration 346, loss = 0.19363810\n",
      "Iteration 378, loss = 0.49211443\n",
      "Iteration 134, loss = 0.40693534\n",
      "Iteration 258, loss = 0.15351826\n",
      "Iteration 210, loss = 0.26012039\n",
      "Iteration 255, loss = 0.14630081\n",
      "Iteration 206, loss = 0.35772781\n",
      "Iteration 347, loss = 0.13723098\n",
      "Iteration 347, loss = 0.19354356\n",
      "Iteration 259, loss = 0.05955498\n",
      "Iteration 379, loss = 0.49235669\n",
      "Iteration 348, loss = 0.13719644\n",
      "Iteration 348, loss = 0.19366683\n",
      "Iteration 259, loss = 0.15345004\n",
      "Iteration 256, loss = 0.14619724\n",
      "Iteration 380, loss = 0.49252882\n",
      "Iteration 211, loss = 0.26003211\n",
      "Iteration 260, loss = 0.05955282\n",
      "Iteration 207, loss = 0.35703773\n",
      "Iteration 349, loss = 0.13697734\n",
      "Iteration 349, loss = 0.19355445\n",
      "Iteration 381, loss = 0.49221183\n",
      "Iteration 260, loss = 0.15336011\n",
      "Iteration 257, loss = 0.14575631\n",
      "Iteration 350, loss = 0.13712078\n",
      "Iteration 261, loss = 0.05952409\n",
      "Iteration 135, loss = 0.40709350\n",
      "Iteration 350, loss = 0.19336622\n",
      "Iteration 382, loss = 0.49203555\n",
      "Iteration 212, loss = 0.25983089\n",
      "Iteration 208, loss = 0.35735338\n",
      "Iteration 351, loss = 0.13724470\n",
      "Iteration 261, loss = 0.15322495\n",
      "Iteration 258, loss = 0.14599757\n",
      "Iteration 351, loss = 0.19341613\n",
      "Iteration 383, loss = 0.49192777\n",
      "Iteration 262, loss = 0.05932090\n",
      "Iteration 352, loss = 0.13674642\n",
      "Iteration 352, loss = 0.19318734\n",
      "Iteration 213, loss = 0.25981174\n",
      "Iteration 209, loss = 0.35684302\n",
      "Iteration 384, loss = 0.49249576\n",
      "Iteration 262, loss = 0.15335800\n",
      "Iteration 259, loss = 0.14585901\n",
      "Iteration 353, loss = 0.13659249\n",
      "Iteration 263, loss = 0.05925744\n",
      "Iteration 353, loss = 0.19300160\n",
      "Iteration 385, loss = 0.49245522\n",
      "Iteration 136, loss = 0.40653323\n",
      "Iteration 263, loss = 0.15294645\n",
      "Iteration 354, loss = 0.13656903\n",
      "Iteration 260, loss = 0.14558501\n",
      "Iteration 214, loss = 0.25934590\n",
      "Iteration 210, loss = 0.35678216\n",
      "Iteration 354, loss = 0.19321065\n",
      "Iteration 386, loss = 0.49210154\n",
      "Iteration 264, loss = 0.05940631\n",
      "Iteration 355, loss = 0.13665104\n",
      "Iteration 355, loss = 0.19304068\n",
      "Iteration 387, loss = 0.49193756\n",
      "Iteration 264, loss = 0.15301639\n",
      "Iteration 261, loss = 0.14532926\n",
      "Iteration 265, loss = 0.05910636\n",
      "Iteration 211, loss = 0.35661849\n",
      "Iteration 215, loss = 0.25898667\n",
      "Iteration 356, loss = 0.13642018\n",
      "Iteration 388, loss = 0.49160267\n",
      "Iteration 356, loss = 0.19302405\n",
      "Iteration 265, loss = 0.15295271\n",
      "Iteration 262, loss = 0.14526411\n",
      "Iteration 357, loss = 0.13614908\n",
      "Iteration 389, loss = 0.49161750\n",
      "Iteration 137, loss = 0.40637816\n",
      "Iteration 266, loss = 0.05902006\n",
      "Iteration 357, loss = 0.19298514\n",
      "Iteration 212, loss = 0.35640015\n",
      "Iteration 216, loss = 0.25900193\n",
      "Iteration 358, loss = 0.13619936\n",
      "Iteration 266, loss = 0.15268428\n",
      "Iteration 390, loss = 0.49178557\n",
      "Iteration 263, loss = 0.14542165\n",
      "Iteration 358, loss = 0.19335086\n",
      "Iteration 267, loss = 0.05883644\n",
      "Iteration 359, loss = 0.13620353\n",
      "Iteration 391, loss = 0.49180321\n",
      "Iteration 359, loss = 0.19285079\n",
      "Iteration 267, loss = 0.15271341\n",
      "Iteration 213, loss = 0.35628731\n",
      "Iteration 264, loss = 0.14518596\n",
      "Iteration 217, loss = 0.25897858\n",
      "Iteration 268, loss = 0.05887968\n",
      "Iteration 360, loss = 0.13617203\n",
      "Iteration 392, loss = 0.49192582\n",
      "Iteration 360, loss = 0.19296699\n",
      "Iteration 138, loss = 0.40620002\n",
      "Iteration 268, loss = 0.15247169\n",
      "Iteration 361, loss = 0.13596829\n",
      "Iteration 393, loss = 0.49142139\n",
      "Iteration 265, loss = 0.14491971\n",
      "Iteration 361, loss = 0.19281332\n",
      "Iteration 269, loss = 0.05867984\n",
      "Iteration 214, loss = 0.35574042\n",
      "Iteration 218, loss = 0.25852932\n",
      "Iteration 362, loss = 0.13575057\n",
      "Iteration 394, loss = 0.49137060\n",
      "Iteration 362, loss = 0.19250343\n",
      "Iteration 269, loss = 0.15271813\n",
      "Iteration 266, loss = 0.14477099\n",
      "Iteration 270, loss = 0.05879783\n",
      "Iteration 363, loss = 0.13603696\n",
      "Iteration 395, loss = 0.49130141\n",
      "Iteration 363, loss = 0.19278027\n",
      "Iteration 215, loss = 0.35535869\n",
      "Iteration 219, loss = 0.25810190\n",
      "Iteration 270, loss = 0.15231270\n",
      "Iteration 267, loss = 0.14484843\n",
      "Iteration 364, loss = 0.13564030\n",
      "Iteration 396, loss = 0.49136706\n",
      "Iteration 139, loss = 0.40576600\n",
      "Iteration 271, loss = 0.05859973\n",
      "Iteration 364, loss = 0.19266568\n",
      "Iteration 365, loss = 0.13553845\n",
      "Iteration 397, loss = 0.49101926\n",
      "Iteration 271, loss = 0.15219538\n",
      "Iteration 216, loss = 0.35550657\n",
      "Iteration 220, loss = 0.25841905\n",
      "Iteration 365, loss = 0.19253111\n",
      "Iteration 268, loss = 0.14466134\n",
      "Iteration 272, loss = 0.05845919\n",
      "Iteration 366, loss = 0.13564678\n",
      "Iteration 398, loss = 0.49130018\n",
      "Iteration 366, loss = 0.19238254\n",
      "Iteration 272, loss = 0.15216939\n",
      "Iteration 269, loss = 0.14450674\n",
      "Iteration 367, loss = 0.13551883\n",
      "Iteration 399, loss = 0.49128474\n",
      "Iteration 217, loss = 0.35485706\n",
      "Iteration 221, loss = 0.25785574\n",
      "Iteration 273, loss = 0.05847205\n",
      "Iteration 367, loss = 0.19242272\n",
      "Iteration 140, loss = 0.40569524\n",
      "Iteration 273, loss = 0.15200287\n",
      "Iteration 368, loss = 0.13539620\n",
      "Iteration 400, loss = 0.49110783\n",
      "Iteration 270, loss = 0.14427431\n",
      "Iteration 368, loss = 0.19250834\n",
      "Iteration 274, loss = 0.05837666\n",
      "Iteration 218, loss = 0.35494324\n",
      "Iteration 222, loss = 0.25781176\n",
      "Iteration 369, loss = 0.13545167\n",
      "Iteration 401, loss = 0.49095645\n",
      "Iteration 369, loss = 0.19236465\n",
      "Iteration 274, loss = 0.15210302\n",
      "Iteration 271, loss = 0.14436040\n",
      "Iteration 275, loss = 0.05835800\n",
      "Iteration 370, loss = 0.13527160\n",
      "Iteration 402, loss = 0.49095939\n",
      "Iteration 370, loss = 0.19194810\n",
      "Iteration 219, loss = 0.35490636\n",
      "Iteration 223, loss = 0.25737921\n",
      "Iteration 275, loss = 0.15186420\n",
      "Iteration 371, loss = 0.13576898\n",
      "Iteration 403, loss = 0.49137775\n",
      "Iteration 272, loss = 0.14427060\n",
      "Iteration 141, loss = 0.40536712\n",
      "Iteration 276, loss = 0.05842145\n",
      "Iteration 371, loss = 0.19208486\n",
      "Iteration 372, loss = 0.13500329\n",
      "Iteration 404, loss = 0.49086841\n",
      "Iteration 276, loss = 0.15200959\n",
      "Iteration 372, loss = 0.19183390\n",
      "Iteration 220, loss = 0.35461709\n",
      "Iteration 224, loss = 0.25706285\n",
      "Iteration 273, loss = 0.14408945\n",
      "Iteration 277, loss = 0.05802620\n",
      "Iteration 373, loss = 0.13500703\n",
      "Iteration 405, loss = 0.49087816\n",
      "Iteration 373, loss = 0.19195373\n",
      "Iteration 277, loss = 0.15199435\n",
      "Iteration 374, loss = 0.13485524\n",
      "Iteration 406, loss = 0.49107287\n",
      "Iteration 274, loss = 0.14408342\n",
      "Iteration 278, loss = 0.05824132\n",
      "Iteration 221, loss = 0.35453755\n",
      "Iteration 225, loss = 0.25681876\n",
      "Iteration 374, loss = 0.19186580\n",
      "Iteration 142, loss = 0.40496807\n",
      "Iteration 375, loss = 0.13486949\n",
      "Iteration 407, loss = 0.49091038\n",
      "Iteration 278, loss = 0.15209111\n",
      "Iteration 375, loss = 0.19182370\n",
      "Iteration 275, loss = 0.14391032\n",
      "Iteration 279, loss = 0.05798984\n",
      "Iteration 376, loss = 0.13459101\n",
      "Iteration 408, loss = 0.49073675\n",
      "Iteration 222, loss = 0.35406438\n",
      "Iteration 226, loss = 0.25705999\n",
      "Iteration 376, loss = 0.19163455\n",
      "Iteration 279, loss = 0.15154765\n",
      "Iteration 276, loss = 0.14373065\n",
      "Iteration 377, loss = 0.13481948\n",
      "Iteration 409, loss = 0.49052103\n",
      "Iteration 280, loss = 0.05792689\n",
      "Iteration 377, loss = 0.19171421\n",
      "Iteration 378, loss = 0.13479614\n",
      "Iteration 280, loss = 0.15174354\n",
      "Iteration 223, loss = 0.35401960\n",
      "Iteration 410, loss = 0.49119278\n",
      "Iteration 227, loss = 0.25659776\n",
      "Iteration 143, loss = 0.40493798\n",
      "Iteration 277, loss = 0.14386971\n",
      "Iteration 281, loss = 0.05788674\n",
      "Iteration 378, loss = 0.19209832\n",
      "Iteration 379, loss = 0.13479247\n",
      "Iteration 411, loss = 0.49054170\n",
      "Iteration 281, loss = 0.15154604\n",
      "Iteration 379, loss = 0.19192451\n",
      "Iteration 278, loss = 0.14355318\n",
      "Iteration 224, loss = 0.35363509\n",
      "Iteration 282, loss = 0.05780536\n",
      "Iteration 228, loss = 0.25654814\n",
      "Iteration 380, loss = 0.13450609\n",
      "Iteration 412, loss = 0.49030317\n",
      "Iteration 380, loss = 0.19162410\n",
      "Iteration 282, loss = 0.15166005\n",
      "Iteration 413, loss = 0.49085926\n",
      "Iteration 381, loss = 0.13410574\n",
      "Iteration 279, loss = 0.14391245\n",
      "Iteration 283, loss = 0.05769411\n",
      "Iteration 381, loss = 0.19139604\n",
      "Iteration 225, loss = 0.35354918\n",
      "Iteration 229, loss = 0.25637906\n",
      "Iteration 144, loss = 0.40455665\n",
      "Iteration 382, loss = 0.13460235\n",
      "Iteration 414, loss = 0.49068677\n",
      "Iteration 283, loss = 0.15151944\n",
      "Iteration 382, loss = 0.19160409\n",
      "Iteration 280, loss = 0.14314978\n",
      "Iteration 284, loss = 0.05781718\n",
      "Iteration 383, loss = 0.13414506\n",
      "Iteration 415, loss = 0.49002337\n",
      "Iteration 226, loss = 0.35328702\n",
      "Iteration 383, loss = 0.19122419\n",
      "Iteration 230, loss = 0.25620851\n",
      "Iteration 284, loss = 0.15145057\n",
      "Iteration 384, loss = 0.13418368\n",
      "Iteration 416, loss = 0.49032069\n",
      "Iteration 281, loss = 0.14330470\n",
      "Iteration 285, loss = 0.05763386\n",
      "Iteration 384, loss = 0.19150798\n",
      "Iteration 385, loss = 0.13462598\n",
      "Iteration 417, loss = 0.49055233\n",
      "Iteration 285, loss = 0.15127440\n",
      "Iteration 231, loss = 0.25575017\n",
      "Iteration 282, loss = 0.14314231\n",
      "Iteration 227, loss = 0.35298311\n",
      "Iteration 145, loss = 0.40448704\n",
      "Iteration 286, loss = 0.05755444\n",
      "Iteration 385, loss = 0.19140075\n",
      "Iteration 386, loss = 0.13389288\n",
      "Iteration 418, loss = 0.49035252\n",
      "Iteration 286, loss = 0.15094560\n",
      "Iteration 386, loss = 0.19104298\n",
      "Iteration 387, loss = 0.13387239\n",
      "Iteration 419, loss = 0.49086045\n",
      "Iteration 283, loss = 0.14298454\n",
      "Iteration 287, loss = 0.05740364\n",
      "Iteration 228, loss = 0.35283227\n",
      "Iteration 232, loss = 0.25556127\n",
      "Iteration 387, loss = 0.19135105\n",
      "Iteration 287, loss = 0.15093502\n",
      "Iteration 420, loss = 0.49020168\n",
      "Iteration 388, loss = 0.13376019\n",
      "Iteration 288, loss = 0.05742923\n",
      "Iteration 284, loss = 0.14304911\n",
      "Iteration 388, loss = 0.19104194\n",
      "Iteration 389, loss = 0.13410478\n",
      "Iteration 421, loss = 0.49069351\n",
      "Iteration 146, loss = 0.40399014\n",
      "Iteration 229, loss = 0.35245266\n",
      "Iteration 233, loss = 0.25563048\n",
      "Iteration 288, loss = 0.15119436\n",
      "Iteration 389, loss = 0.19089081\n",
      "Iteration 289, loss = 0.05727615\n",
      "Iteration 285, loss = 0.14353753\n",
      "Iteration 390, loss = 0.13394276\n",
      "Iteration 422, loss = 0.49007741\n",
      "Iteration 390, loss = 0.19081527\n",
      "Iteration 289, loss = 0.15099189\n",
      "Iteration 391, loss = 0.13363241\n",
      "Iteration 423, loss = 0.48996611\n",
      "Iteration 230, loss = 0.35241239\n",
      "Iteration 234, loss = 0.25533811\n",
      "Iteration 290, loss = 0.05721582\n",
      "Iteration 286, loss = 0.14301994\n",
      "Iteration 391, loss = 0.19085567\n",
      "Iteration 424, loss = 0.49003939\n",
      "Iteration 392, loss = 0.13356007\n",
      "Iteration 290, loss = 0.15077101\n",
      "Iteration 147, loss = 0.40373458\n",
      "Iteration 291, loss = 0.05717489\n",
      "Iteration 287, loss = 0.14268508\n",
      "Iteration 392, loss = 0.19093544\n",
      "Iteration 235, loss = 0.25504036\n",
      "Iteration 231, loss = 0.35223607\n",
      "Iteration 425, loss = 0.49003623\n",
      "Iteration 393, loss = 0.13361500\n",
      "Iteration 291, loss = 0.15086760\n",
      "Iteration 393, loss = 0.19083715\n",
      "Iteration 394, loss = 0.13339211\n",
      "Iteration 426, loss = 0.49017564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 292, loss = 0.05706060\n",
      "Iteration 288, loss = 0.14278711\n",
      "Iteration 232, loss = 0.35153819\n",
      "Iteration 236, loss = 0.25497956\n",
      "Iteration 394, loss = 0.19079344\n",
      "Iteration 395, loss = 0.13354960\n",
      "Iteration 292, loss = 0.15060423\n",
      "Iteration 293, loss = 0.05708463\n",
      "Iteration 289, loss = 0.14255708\n",
      "Iteration 395, loss = 0.19059915\n",
      "Iteration 396, loss = 0.13353399\n",
      "Iteration 148, loss = 0.40365408\n",
      "Iteration 293, loss = 0.15029728\n",
      "Iteration 237, loss = 0.25494755\n",
      "Iteration 233, loss = 0.35179483\n",
      "Iteration 396, loss = 0.19088333\n",
      "Iteration 397, loss = 0.13324289\n",
      "Iteration 294, loss = 0.05687176\n",
      "Iteration 290, loss = 0.14215639\n",
      "Iteration 397, loss = 0.19065891\n",
      "Iteration 398, loss = 0.13327284\n",
      "Iteration 294, loss = 0.15053254\n",
      "Iteration 238, loss = 0.25430205\n",
      "Iteration 295, loss = 0.05670897\n",
      "Iteration 234, loss = 0.35165291\n",
      "Iteration 291, loss = 0.14257045\n",
      "Iteration 398, loss = 0.19082158\n",
      "Iteration 399, loss = 0.13336023\n",
      "Iteration 295, loss = 0.15029030\n",
      "Iteration 149, loss = 0.40331748\n",
      "Iteration 296, loss = 0.05666130\n",
      "Iteration 400, loss = 0.13321715\n",
      "Iteration 399, loss = 0.19058501\n",
      "Iteration 292, loss = 0.14228517\n",
      "Iteration 239, loss = 0.25435812\n",
      "Iteration 235, loss = 0.35119269\n",
      "Iteration 296, loss = 0.15025071\n",
      "Iteration 401, loss = 0.13297460\n",
      "Iteration 400, loss = 0.19039927\n",
      "Iteration 297, loss = 0.05685134\n",
      "Iteration 293, loss = 0.14193689\n",
      "Iteration 402, loss = 0.13291492\n",
      "Iteration 401, loss = 0.19044509\n",
      "Iteration 240, loss = 0.25439001\n",
      "Iteration 236, loss = 0.35139248\n",
      "Iteration 297, loss = 0.15027113\n",
      "Iteration 298, loss = 0.05673852\n",
      "Iteration 403, loss = 0.13305268\n",
      "Iteration 402, loss = 0.19020528\n",
      "Iteration 294, loss = 0.14208399\n",
      "Iteration 150, loss = 0.40298109\n",
      "Iteration 298, loss = 0.15038694\n",
      "Iteration 404, loss = 0.13276780\n",
      "Iteration 403, loss = 0.19022348\n",
      "Iteration 241, loss = 0.25360949\n",
      "Iteration 237, loss = 0.35074817\n",
      "Iteration 299, loss = 0.05658402\n",
      "Iteration 295, loss = 0.14203191\n",
      "Iteration 405, loss = 0.13270476\n",
      "Iteration 404, loss = 0.19015637\n",
      "Iteration 299, loss = 0.15027851\n",
      "Iteration 300, loss = 0.05658699\n",
      "Iteration 242, loss = 0.25377551\n",
      "Iteration 296, loss = 0.14196714\n",
      "Iteration 406, loss = 0.13260693\n",
      "Iteration 405, loss = 0.19015880Iteration 238, loss = 0.35098976\n",
      "\n",
      "Iteration 300, loss = 0.14997421\n",
      "Iteration 151, loss = 0.40287065\n",
      "Iteration 407, loss = 0.13256816\n",
      "Iteration 406, loss = 0.19022221\n",
      "Iteration 301, loss = 0.05638119\n",
      "Iteration 297, loss = 0.14170315\n",
      "Iteration 243, loss = 0.25338177\n",
      "Iteration 301, loss = 0.14997879\n",
      "Iteration 239, loss = 0.35064674\n",
      "Iteration 408, loss = 0.13251809\n",
      "Iteration 407, loss = 0.19015262\n",
      "Iteration 302, loss = 0.05645990\n",
      "Iteration 298, loss = 0.14195454\n",
      "Iteration 409, loss = 0.13246304\n",
      "Iteration 408, loss = 0.19007186\n",
      "Iteration 302, loss = 0.14983402\n",
      "Iteration 244, loss = 0.25320702\n",
      "Iteration 240, loss = 0.35027003\n",
      "Iteration 303, loss = 0.05635959\n",
      "Iteration 410, loss = 0.13233744\n",
      "Iteration 409, loss = 0.19020395\n",
      "Iteration 299, loss = 0.14146239\n",
      "Iteration 152, loss = 0.40235216\n",
      "Iteration 303, loss = 0.14993551\n",
      "Iteration 411, loss = 0.13218489\n",
      "Iteration 410, loss = 0.19003749\n",
      "Iteration 245, loss = 0.25352726\n",
      "Iteration 304, loss = 0.05623568\n",
      "Iteration 241, loss = 0.35015876\n",
      "Iteration 300, loss = 0.14163691\n",
      "Iteration 412, loss = 0.13263341\n",
      "Iteration 304, loss = 0.14992855\n",
      "Iteration 411, loss = 0.19008939\n",
      "Iteration 305, loss = 0.05596858\n",
      "Iteration 413, loss = 0.13242492\n",
      "Iteration 301, loss = 0.14125251\n",
      "Iteration 412, loss = 0.18970681\n",
      "Iteration 246, loss = 0.25293759\n",
      "Iteration 242, loss = 0.34968925\n",
      "Iteration 305, loss = 0.14989412\n",
      "Iteration 153, loss = 0.40204538\n",
      "Iteration 414, loss = 0.13186338\n",
      "Iteration 413, loss = 0.18978793\n",
      "Iteration 306, loss = 0.05613849\n",
      "Iteration 302, loss = 0.14113605\n",
      "Iteration 415, loss = 0.13240065\n",
      "Iteration 306, loss = 0.14963175\n",
      "Iteration 247, loss = 0.25272311\n",
      "Iteration 414, loss = 0.18978701\n",
      "Iteration 243, loss = 0.34947013\n",
      "Iteration 307, loss = 0.05611290\n",
      "Iteration 416, loss = 0.13180746\n",
      "Iteration 303, loss = 0.14092576\n",
      "Iteration 415, loss = 0.18975899\n",
      "Iteration 307, loss = 0.14957613\n",
      "Iteration 248, loss = 0.25250503\n",
      "Iteration 417, loss = 0.13167632\n",
      "Iteration 308, loss = 0.05593410\n",
      "Iteration 244, loss = 0.34962206\n",
      "Iteration 416, loss = 0.18958735\n",
      "Iteration 304, loss = 0.14097387\n",
      "Iteration 154, loss = 0.40199146\n",
      "Iteration 308, loss = 0.14940668\n",
      "Iteration 418, loss = 0.13198126\n",
      "Iteration 417, loss = 0.18959752\n",
      "Iteration 309, loss = 0.05586954\n",
      "Iteration 249, loss = 0.25249583\n",
      "Iteration 305, loss = 0.14098820\n",
      "Iteration 419, loss = 0.13186641\n",
      "Iteration 245, loss = 0.34928889\n",
      "Iteration 309, loss = 0.14935775\n",
      "Iteration 418, loss = 0.18984093\n",
      "Iteration 310, loss = 0.05580850\n",
      "Iteration 420, loss = 0.13185619\n",
      "Iteration 419, loss = 0.18961043\n",
      "Iteration 306, loss = 0.14081898\n",
      "Iteration 250, loss = 0.25243353\n",
      "Iteration 310, loss = 0.14937988\n",
      "Iteration 246, loss = 0.34937581\n",
      "Iteration 421, loss = 0.13204461\n",
      "Iteration 155, loss = 0.40182088\n",
      "Iteration 420, loss = 0.18945718\n",
      "Iteration 311, loss = 0.05585998\n",
      "Iteration 307, loss = 0.14067431\n",
      "Iteration 422, loss = 0.13143381\n",
      "Iteration 311, loss = 0.14904300\n",
      "Iteration 421, loss = 0.18936844\n",
      "Iteration 251, loss = 0.25229683\n",
      "Iteration 247, loss = 0.34882885\n",
      "Iteration 312, loss = 0.05583718\n",
      "Iteration 423, loss = 0.13149943\n",
      "Iteration 422, loss = 0.18960498\n",
      "Iteration 308, loss = 0.14070779\n",
      "Iteration 312, loss = 0.14927087\n",
      "Iteration 424, loss = 0.13171417\n",
      "Iteration 313, loss = 0.05556073\n",
      "Iteration 252, loss = 0.25198818\n",
      "Iteration 423, loss = 0.18909691\n",
      "Iteration 248, loss = 0.34830484\n",
      "Iteration 156, loss = 0.40158908\n",
      "Iteration 309, loss = 0.14073945\n",
      "Iteration 313, loss = 0.14926227\n",
      "Iteration 425, loss = 0.13135822\n",
      "Iteration 424, loss = 0.18923046\n",
      "Iteration 314, loss = 0.05551194\n",
      "Iteration 426, loss = 0.13160978\n",
      "Iteration 253, loss = 0.25201404\n",
      "Iteration 310, loss = 0.14073890\n",
      "Iteration 425, loss = 0.18915843\n",
      "Iteration 314, loss = 0.14918161\n",
      "Iteration 249, loss = 0.34860414\n",
      "Iteration 427, loss = 0.13175465\n",
      "Iteration 315, loss = 0.05537559\n",
      "Iteration 426, loss = 0.18897560\n",
      "Iteration 311, loss = 0.14046840\n",
      "Iteration 315, loss = 0.14909621\n",
      "Iteration 428, loss = 0.13145234\n",
      "Iteration 254, loss = 0.25183330\n",
      "Iteration 250, loss = 0.34830578\n",
      "Iteration 157, loss = 0.40122434\n",
      "Iteration 427, loss = 0.18914809\n",
      "Iteration 316, loss = 0.05574985\n",
      "Iteration 429, loss = 0.13148789\n",
      "Iteration 316, loss = 0.14912306\n",
      "Iteration 312, loss = 0.14027988\n",
      "Iteration 428, loss = 0.18925783\n",
      "Iteration 255, loss = 0.25150021\n",
      "Iteration 430, loss = 0.13081371\n",
      "Iteration 317, loss = 0.05554341\n",
      "Iteration 251, loss = 0.34790891\n",
      "Iteration 429, loss = 0.18927076\n",
      "Iteration 317, loss = 0.14892306\n",
      "Iteration 313, loss = 0.14008698\n",
      "Iteration 431, loss = 0.13122824\n",
      "Iteration 318, loss = 0.05526709\n",
      "Iteration 430, loss = 0.18904508\n",
      "Iteration 256, loss = 0.25128076\n",
      "Iteration 158, loss = 0.40132904\n",
      "Iteration 432, loss = 0.13102604\n",
      "Iteration 318, loss = 0.14931355\n",
      "Iteration 314, loss = 0.14012740\n",
      "Iteration 252, loss = 0.34739458\n",
      "Iteration 431, loss = 0.18896577\n",
      "Iteration 319, loss = 0.05527802\n",
      "Iteration 433, loss = 0.13097770\n",
      "Iteration 315, loss = 0.13995262\n",
      "Iteration 319, loss = 0.14891788\n",
      "Iteration 432, loss = 0.18905313\n",
      "Iteration 257, loss = 0.25117389\n",
      "Iteration 253, loss = 0.34778619\n",
      "Iteration 434, loss = 0.13090831\n",
      "Iteration 320, loss = 0.05538133\n",
      "Iteration 433, loss = 0.18904486\n",
      "Iteration 316, loss = 0.14012862\n",
      "Iteration 320, loss = 0.14889892\n",
      "Iteration 435, loss = 0.13114104\n",
      "Iteration 258, loss = 0.25108299\n",
      "Iteration 159, loss = 0.40099772\n",
      "Iteration 434, loss = 0.18878639\n",
      "Iteration 321, loss = 0.05519975\n",
      "Iteration 254, loss = 0.34698750\n",
      "Iteration 436, loss = 0.13077940\n",
      "Iteration 321, loss = 0.14894480\n",
      "Iteration 317, loss = 0.13987761\n",
      "Iteration 435, loss = 0.18899402\n",
      "Iteration 437, loss = 0.13061685\n",
      "Iteration 259, loss = 0.25061571\n",
      "Iteration 322, loss = 0.05504010\n",
      "Iteration 436, loss = 0.18871536\n",
      "Iteration 255, loss = 0.34664583\n",
      "Iteration 318, loss = 0.13978958\n",
      "Iteration 322, loss = 0.14877757\n",
      "Iteration 438, loss = 0.13091846\n",
      "Iteration 323, loss = 0.05517119\n",
      "Iteration 437, loss = 0.18858909\n",
      "Iteration 160, loss = 0.40072647\n",
      "Iteration 439, loss = 0.13037667\n",
      "Iteration 260, loss = 0.25066591\n",
      "Iteration 323, loss = 0.14855623\n",
      "Iteration 319, loss = 0.13978825\n",
      "Iteration 256, loss = 0.34658387\n",
      "Iteration 438, loss = 0.18891936\n",
      "Iteration 324, loss = 0.05484003\n",
      "Iteration 440, loss = 0.13086791\n",
      "Iteration 439, loss = 0.18861668\n",
      "Iteration 320, loss = 0.13990570\n",
      "Iteration 324, loss = 0.14848195\n",
      "Iteration 261, loss = 0.25027067\n",
      "Iteration 441, loss = 0.13073116\n",
      "Iteration 257, loss = 0.34622375\n",
      "Iteration 325, loss = 0.05484798\n",
      "Iteration 440, loss = 0.18831782\n",
      "Iteration 442, loss = 0.13033481\n",
      "Iteration 321, loss = 0.13957031\n",
      "Iteration 325, loss = 0.14825924\n",
      "Iteration 161, loss = 0.40060710\n",
      "Iteration 441, loss = 0.18848993\n",
      "Iteration 262, loss = 0.25052824\n",
      "Iteration 326, loss = 0.05482996\n",
      "Iteration 443, loss = 0.13024099\n",
      "Iteration 258, loss = 0.34641602\n",
      "Iteration 322, loss = 0.13942988\n",
      "Iteration 326, loss = 0.14839050\n",
      "Iteration 442, loss = 0.18839636\n",
      "Iteration 444, loss = 0.13019043\n",
      "Iteration 327, loss = 0.05476353\n",
      "Iteration 263, loss = 0.25037625\n",
      "Iteration 443, loss = 0.18850176\n",
      "Iteration 323, loss = 0.13930774\n",
      "Iteration 445, loss = 0.13028128\n",
      "Iteration 327, loss = 0.14862822\n",
      "Iteration 259, loss = 0.34585421\n",
      "Iteration 328, loss = 0.05459703\n",
      "Iteration 444, loss = 0.18840587\n",
      "Iteration 446, loss = 0.13007004\n",
      "Iteration 162, loss = 0.40013239\n",
      "Iteration 324, loss = 0.13910183\n",
      "Iteration 264, loss = 0.25015771\n",
      "Iteration 328, loss = 0.14837024\n",
      "Iteration 445, loss = 0.18866574\n",
      "Iteration 447, loss = 0.13029352\n",
      "Iteration 260, loss = 0.34561507\n",
      "Iteration 329, loss = 0.05450791\n",
      "Iteration 325, loss = 0.13901248\n",
      "Iteration 446, loss = 0.18857361\n",
      "Iteration 329, loss = 0.14819082\n",
      "Iteration 448, loss = 0.13022037\n",
      "Iteration 265, loss = 0.24984779\n",
      "Iteration 330, loss = 0.05473035\n",
      "Iteration 261, loss = 0.34495603\n",
      "Iteration 447, loss = 0.18819840\n",
      "Iteration 449, loss = 0.13036933\n",
      "Iteration 326, loss = 0.13915932\n",
      "Iteration 330, loss = 0.14808790\n",
      "Iteration 163, loss = 0.40013785\n",
      "Iteration 448, loss = 0.18808052\n",
      "Iteration 450, loss = 0.12971375\n",
      "Iteration 331, loss = 0.05446906\n",
      "Iteration 266, loss = 0.25001341\n",
      "Iteration 327, loss = 0.13910477\n",
      "Iteration 262, loss = 0.34540087\n",
      "Iteration 331, loss = 0.14806974\n",
      "Iteration 449, loss = 0.18819155\n",
      "Iteration 451, loss = 0.13016979\n",
      "Iteration 332, loss = 0.05426860\n",
      "Iteration 452, loss = 0.13000065\n",
      "Iteration 450, loss = 0.18847431\n",
      "Iteration 267, loss = 0.24950367\n",
      "Iteration 328, loss = 0.13876651\n",
      "Iteration 332, loss = 0.14780485\n",
      "Iteration 263, loss = 0.34446669\n",
      "Iteration 333, loss = 0.05444792\n",
      "Iteration 453, loss = 0.12952376\n",
      "Iteration 451, loss = 0.18800425\n",
      "Iteration 164, loss = 0.39976149\n",
      "Iteration 329, loss = 0.13873407\n",
      "Iteration 333, loss = 0.14809821\n",
      "Iteration 268, loss = 0.24913684\n",
      "Iteration 454, loss = 0.13004786\n",
      "Iteration 452, loss = 0.18795035\n",
      "Iteration 334, loss = 0.05423930\n",
      "Iteration 264, loss = 0.34426754\n",
      "Iteration 455, loss = 0.12971944\n",
      "Iteration 453, loss = 0.18820966\n",
      "Iteration 330, loss = 0.13887787\n",
      "Iteration 334, loss = 0.14808240\n",
      "Iteration 335, loss = 0.05435604\n",
      "Iteration 269, loss = 0.24898111\n",
      "Iteration 456, loss = 0.12961341\n",
      "Iteration 454, loss = 0.18811324\n",
      "Iteration 265, loss = 0.34431378\n",
      "Iteration 335, loss = 0.14783016\n",
      "Iteration 331, loss = 0.13846965\n",
      "Iteration 165, loss = 0.39954174\n",
      "Iteration 457, loss = 0.12954351\n",
      "Iteration 455, loss = 0.18801293\n",
      "Iteration 336, loss = 0.05420483\n",
      "Iteration 270, loss = 0.24896803\n",
      "Iteration 336, loss = 0.14775911\n",
      "Iteration 332, loss = 0.13844967\n",
      "Iteration 458, loss = 0.12968850\n",
      "Iteration 456, loss = 0.18815687\n",
      "Iteration 266, loss = 0.34416559\n",
      "Iteration 337, loss = 0.05423008\n",
      "Iteration 459, loss = 0.12931011\n",
      "Iteration 457, loss = 0.18805753\n",
      "Iteration 337, loss = 0.14761756\n",
      "Iteration 271, loss = 0.24878652\n",
      "Iteration 333, loss = 0.13843011\n",
      "Iteration 460, loss = 0.12942691\n",
      "Iteration 267, loss = 0.34360734\n",
      "Iteration 338, loss = 0.05418850\n",
      "Iteration 458, loss = 0.18765463\n",
      "Iteration 166, loss = 0.39937978\n",
      "Iteration 338, loss = 0.14775814\n",
      "Iteration 334, loss = 0.13825655\n",
      "Iteration 461, loss = 0.12937210\n",
      "Iteration 459, loss = 0.18787099\n",
      "Iteration 272, loss = 0.24851526\n",
      "Iteration 339, loss = 0.05412577\n",
      "Iteration 268, loss = 0.34340579\n",
      "Iteration 462, loss = 0.12926142\n",
      "Iteration 460, loss = 0.18779291\n",
      "Iteration 339, loss = 0.14745631\n",
      "Iteration 335, loss = 0.13833797\n",
      "Iteration 340, loss = 0.05400367\n",
      "Iteration 463, loss = 0.12938078\n",
      "Iteration 461, loss = 0.18772268\n",
      "Iteration 273, loss = 0.24875888\n",
      "Iteration 269, loss = 0.34317130\n",
      "Iteration 340, loss = 0.14779080\n",
      "Iteration 336, loss = 0.13823319\n",
      "Iteration 464, loss = 0.12943166\n",
      "Iteration 167, loss = 0.39934665\n",
      "Iteration 462, loss = 0.18751045\n",
      "Iteration 341, loss = 0.05404701\n",
      "Iteration 465, loss = 0.12888275\n",
      "Iteration 341, loss = 0.14748244\n",
      "Iteration 274, loss = 0.24809351\n",
      "Iteration 463, loss = 0.18757999\n",
      "Iteration 337, loss = 0.13781917\n",
      "Iteration 270, loss = 0.34319079\n",
      "Iteration 342, loss = 0.05395157\n",
      "Iteration 466, loss = 0.12904094\n",
      "Iteration 464, loss = 0.18742296\n",
      "Iteration 342, loss = 0.14759566\n",
      "Iteration 338, loss = 0.13769371\n",
      "Iteration 467, loss = 0.12895402\n",
      "Iteration 275, loss = 0.24826628\n",
      "Iteration 343, loss = 0.05391858\n",
      "Iteration 465, loss = 0.18758774\n",
      "Iteration 271, loss = 0.34252736\n",
      "Iteration 168, loss = 0.39891391\n",
      "Iteration 468, loss = 0.12875005\n",
      "Iteration 343, loss = 0.14711704\n",
      "Iteration 339, loss = 0.13794866\n",
      "Iteration 466, loss = 0.18739187\n",
      "Iteration 344, loss = 0.05398868\n",
      "Iteration 276, loss = 0.24766330\n",
      "Iteration 469, loss = 0.12877535\n",
      "Iteration 467, loss = 0.18718320\n",
      "Iteration 272, loss = 0.34254082\n",
      "Iteration 344, loss = 0.14724882\n",
      "Iteration 340, loss = 0.13779578\n",
      "Iteration 470, loss = 0.12890230\n",
      "Iteration 345, loss = 0.05371371\n",
      "Iteration 468, loss = 0.18758377\n",
      "Iteration 277, loss = 0.24823361\n",
      "Iteration 345, loss = 0.14730271\n",
      "Iteration 471, loss = 0.12869830\n",
      "Iteration 341, loss = 0.13761657\n",
      "Iteration 273, loss = 0.34235554\n",
      "Iteration 169, loss = 0.39891215\n",
      "Iteration 469, loss = 0.18770122\n",
      "Iteration 346, loss = 0.05362070\n",
      "Iteration 472, loss = 0.12926276\n",
      "Iteration 346, loss = 0.14720463\n",
      "Iteration 470, loss = 0.18697561\n",
      "Iteration 342, loss = 0.13753247\n",
      "Iteration 278, loss = 0.24808769\n",
      "Iteration 347, loss = 0.05356396\n",
      "Iteration 473, loss = 0.12875941\n",
      "Iteration 274, loss = 0.34191477\n",
      "Iteration 471, loss = 0.18729077\n",
      "Iteration 347, loss = 0.14690007\n",
      "Iteration 343, loss = 0.13732831\n",
      "Iteration 474, loss = 0.12866102\n",
      "Iteration 348, loss = 0.05354373\n",
      "Iteration 472, loss = 0.18710486\n",
      "Iteration 279, loss = 0.24771810\n",
      "Iteration 275, loss = 0.34180180\n",
      "Iteration 475, loss = 0.12855728\n",
      "Iteration 170, loss = 0.39844983\n",
      "Iteration 348, loss = 0.14684504\n",
      "Iteration 344, loss = 0.13777398\n",
      "Iteration 473, loss = 0.18715163\n",
      "Iteration 349, loss = 0.05353052\n",
      "Iteration 476, loss = 0.12852933\n",
      "Iteration 280, loss = 0.24719193\n",
      "Iteration 474, loss = 0.18722533\n",
      "Iteration 349, loss = 0.14711882\n",
      "Iteration 276, loss = 0.34140824\n",
      "Iteration 345, loss = 0.13726627\n",
      "Iteration 477, loss = 0.12848621\n",
      "Iteration 350, loss = 0.05351695\n",
      "Iteration 475, loss = 0.18719805\n",
      "Iteration 350, loss = 0.14663109\n",
      "Iteration 478, loss = 0.12850805\n",
      "Iteration 346, loss = 0.13723607\n",
      "Iteration 281, loss = 0.24706621\n",
      "Iteration 171, loss = 0.39839854\n",
      "Iteration 277, loss = 0.34103920\n",
      "Iteration 476, loss = 0.18704256\n",
      "Iteration 351, loss = 0.05342986\n",
      "Iteration 479, loss = 0.12856702\n",
      "Iteration 351, loss = 0.14677711\n",
      "Iteration 347, loss = 0.13706707\n",
      "Iteration 477, loss = 0.18718592\n",
      "Iteration 480, loss = 0.12831527\n",
      "Iteration 352, loss = 0.05369562\n",
      "Iteration 282, loss = 0.24699451\n",
      "Iteration 278, loss = 0.34095750\n",
      "Iteration 478, loss = 0.18708525\n",
      "Iteration 352, loss = 0.14652130\n",
      "Iteration 481, loss = 0.12815350\n",
      "Iteration 348, loss = 0.13697560\n",
      "Iteration 353, loss = 0.05333209\n",
      "Iteration 479, loss = 0.18673154\n",
      "Iteration 172, loss = 0.39843084\n",
      "Iteration 482, loss = 0.12844905\n",
      "Iteration 283, loss = 0.24703086\n",
      "Iteration 279, loss = 0.34052192\n",
      "Iteration 353, loss = 0.14692112\n",
      "Iteration 349, loss = 0.13688745\n",
      "Iteration 480, loss = 0.18690130\n",
      "Iteration 354, loss = 0.05331811\n",
      "Iteration 483, loss = 0.12817426\n",
      "Iteration 354, loss = 0.14660591\n",
      "Iteration 481, loss = 0.18641041\n",
      "Iteration 284, loss = 0.24679361\n",
      "Iteration 350, loss = 0.13679155\n",
      "Iteration 484, loss = 0.12815960\n",
      "Iteration 280, loss = 0.34059799\n",
      "Iteration 355, loss = 0.05317199\n",
      "Iteration 482, loss = 0.18680090\n",
      "Iteration 485, loss = 0.12821694\n",
      "Iteration 355, loss = 0.14662867\n",
      "Iteration 351, loss = 0.13698086\n",
      "Iteration 173, loss = 0.39783456\n",
      "Iteration 285, loss = 0.24669139\n",
      "Iteration 356, loss = 0.05327413\n",
      "Iteration 483, loss = 0.18671030\n",
      "Iteration 281, loss = 0.34057108\n",
      "Iteration 486, loss = 0.12780209\n",
      "Iteration 356, loss = 0.14641910\n",
      "Iteration 484, loss = 0.18684957\n",
      "Iteration 352, loss = 0.13685798\n",
      "Iteration 487, loss = 0.12799259\n",
      "Iteration 357, loss = 0.05303404\n",
      "Iteration 286, loss = 0.24648496\n",
      "Iteration 485, loss = 0.18669829\n",
      "Iteration 282, loss = 0.34039213\n",
      "Iteration 357, loss = 0.14668741\n",
      "Iteration 488, loss = 0.12792830\n",
      "Iteration 353, loss = 0.13654617\n",
      "Iteration 358, loss = 0.05337733\n",
      "Iteration 486, loss = 0.18679110\n",
      "Iteration 489, loss = 0.12850855\n",
      "Iteration 174, loss = 0.39777256\n",
      "Iteration 358, loss = 0.14655501\n",
      "Iteration 287, loss = 0.24610536\n",
      "Iteration 283, loss = 0.33966992\n",
      "Iteration 487, loss = 0.18639909\n",
      "Iteration 354, loss = 0.13673996\n",
      "Iteration 490, loss = 0.12776584\n",
      "Iteration 359, loss = 0.05290212\n",
      "Iteration 359, loss = 0.14616873\n",
      "Iteration 488, loss = 0.18638470\n",
      "Iteration 491, loss = 0.12785699\n",
      "Iteration 355, loss = 0.13690427\n",
      "Iteration 288, loss = 0.24615024\n",
      "Iteration 360, loss = 0.05309401\n",
      "Iteration 284, loss = 0.33967547\n",
      "Iteration 489, loss = 0.18653243\n",
      "Iteration 492, loss = 0.12729678\n",
      "Iteration 360, loss = 0.14616502\n",
      "Iteration 175, loss = 0.39752928\n",
      "Iteration 356, loss = 0.13646295\n",
      "Iteration 490, loss = 0.18652026\n",
      "Iteration 493, loss = 0.12768273\n",
      "Iteration 361, loss = 0.05296166\n",
      "Iteration 289, loss = 0.24584037\n",
      "Iteration 285, loss = 0.33963355\n",
      "Iteration 361, loss = 0.14618852\n",
      "Iteration 494, loss = 0.12742070\n",
      "Iteration 491, loss = 0.18632297\n",
      "Iteration 357, loss = 0.13644911\n",
      "Iteration 362, loss = 0.05294172\n",
      "Iteration 495, loss = 0.12790760\n",
      "Iteration 492, loss = 0.18633208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 362, loss = 0.14597104\n",
      "Iteration 290, loss = 0.24557139\n",
      "Iteration 286, loss = 0.33923247\n",
      "Iteration 358, loss = 0.13646636\n",
      "Iteration 496, loss = 0.12754362\n",
      "Iteration 363, loss = 0.05287850\n",
      "Iteration 176, loss = 0.39764305\n",
      "Iteration 363, loss = 0.14608223\n",
      "Iteration 497, loss = 0.12740971\n",
      "Iteration 291, loss = 0.24575875\n",
      "Iteration 359, loss = 0.13626989\n",
      "Iteration 287, loss = 0.33918578\n",
      "Iteration 364, loss = 0.05283441\n",
      "Iteration 498, loss = 0.12740218\n",
      "Iteration 364, loss = 0.14630530\n",
      "Iteration 360, loss = 0.13637743\n",
      "Iteration 365, loss = 0.05267643\n",
      "Iteration 499, loss = 0.12734182\n",
      "Iteration 292, loss = 0.24542443\n",
      "Iteration 288, loss = 0.33868867\n",
      "Iteration 365, loss = 0.14602106\n",
      "Iteration 177, loss = 0.39718661\n",
      "Iteration 500, loss = 0.12751105\n",
      "Iteration 361, loss = 0.13602453\n",
      "Iteration 366, loss = 0.05283372\n",
      "Iteration 293, loss = 0.24503325\n",
      "Iteration 366, loss = 0.14563160\n",
      "Iteration 289, loss = 0.33887088\n",
      "Iteration 501, loss = 0.12696222\n",
      "Iteration 362, loss = 0.13574317\n",
      "Iteration 367, loss = 0.05268636\n",
      "Iteration 502, loss = 0.12729853\n",
      "Iteration 367, loss = 0.14586071\n",
      "Iteration 294, loss = 0.24553010\n",
      "Iteration 290, loss = 0.33864523\n",
      "Iteration 363, loss = 0.13589337\n",
      "Iteration 503, loss = 0.12726599\n",
      "Iteration 368, loss = 0.05253779\n",
      "Iteration 178, loss = 0.39727508\n",
      "Iteration 368, loss = 0.14587592\n",
      "Iteration 504, loss = 0.12700943\n",
      "Iteration 364, loss = 0.13616966\n",
      "Iteration 369, loss = 0.05256001\n",
      "Iteration 295, loss = 0.24482664\n",
      "Iteration 291, loss = 0.33852678\n",
      "Iteration 505, loss = 0.12705605\n",
      "Iteration 369, loss = 0.14582968\n",
      "Iteration 365, loss = 0.13590668\n",
      "Iteration 506, loss = 0.12713503\n",
      "Iteration 370, loss = 0.05260773\n",
      "Iteration 292, loss = 0.33812277\n",
      "Iteration 296, loss = 0.24471930\n",
      "Iteration 370, loss = 0.14613318\n",
      "Iteration 507, loss = 0.12696886\n",
      "Iteration 179, loss = 0.39686448\n",
      "Iteration 366, loss = 0.13569198\n",
      "Iteration 371, loss = 0.05248615\n",
      "Iteration 508, loss = 0.12716668\n",
      "Iteration 371, loss = 0.14571201\n",
      "Iteration 293, loss = 0.33802664\n",
      "Iteration 297, loss = 0.24460698\n",
      "Iteration 367, loss = 0.13560904\n",
      "Iteration 372, loss = 0.05241037\n",
      "Iteration 509, loss = 0.12693814\n",
      "Iteration 372, loss = 0.14578002\n",
      "Iteration 510, loss = 0.12678797\n",
      "Iteration 368, loss = 0.13530401\n",
      "Iteration 294, loss = 0.33763314\n",
      "Iteration 373, loss = 0.05250952\n",
      "Iteration 298, loss = 0.24441483\n",
      "Iteration 180, loss = 0.39659912\n",
      "Iteration 373, loss = 0.14566136\n",
      "Iteration 511, loss = 0.12693234\n",
      "Iteration 369, loss = 0.13543192\n",
      "Iteration 374, loss = 0.05243383\n",
      "Iteration 295, loss = 0.33750592\n",
      "Iteration 512, loss = 0.12707887\n",
      "Iteration 299, loss = 0.24465388\n",
      "Iteration 374, loss = 0.14560921\n",
      "Iteration 513, loss = 0.12669159\n",
      "Iteration 370, loss = 0.13538247\n",
      "Iteration 375, loss = 0.05235145\n",
      "Iteration 375, loss = 0.14547735\n",
      "Iteration 296, loss = 0.33730704\n",
      "Iteration 514, loss = 0.12670760\n",
      "Iteration 300, loss = 0.24431897\n",
      "Iteration 181, loss = 0.39629061\n",
      "Iteration 371, loss = 0.13538940\n",
      "Iteration 376, loss = 0.05230508\n",
      "Iteration 515, loss = 0.12673493\n",
      "Iteration 376, loss = 0.14551987\n",
      "Iteration 297, loss = 0.33716591\n",
      "Iteration 372, loss = 0.13526911\n",
      "Iteration 301, loss = 0.24399943\n",
      "Iteration 516, loss = 0.12642633\n",
      "Iteration 377, loss = 0.05227477\n",
      "Iteration 377, loss = 0.14552674\n",
      "Iteration 517, loss = 0.12655197\n",
      "Iteration 373, loss = 0.13524739\n",
      "Iteration 378, loss = 0.05205628\n",
      "Iteration 298, loss = 0.33671477\n",
      "Iteration 302, loss = 0.24429301\n",
      "Iteration 182, loss = 0.39615819\n",
      "Iteration 378, loss = 0.14519135\n",
      "Iteration 518, loss = 0.12636856\n",
      "Iteration 374, loss = 0.13504055\n",
      "Iteration 379, loss = 0.05216654\n",
      "Iteration 519, loss = 0.12627178\n",
      "Iteration 379, loss = 0.14531036\n",
      "Iteration 303, loss = 0.24408894\n",
      "Iteration 299, loss = 0.33642260\n",
      "Iteration 520, loss = 0.12644720\n",
      "Iteration 375, loss = 0.13488060\n",
      "Iteration 380, loss = 0.05208220\n",
      "Iteration 380, loss = 0.14511715\n",
      "Iteration 521, loss = 0.12650431\n",
      "Iteration 183, loss = 0.39609859\n",
      "Iteration 304, loss = 0.24415355\n",
      "Iteration 300, loss = 0.33641341\n",
      "Iteration 376, loss = 0.13500336\n",
      "Iteration 381, loss = 0.05201184\n",
      "Iteration 522, loss = 0.12601644\n",
      "Iteration 381, loss = 0.14522347\n",
      "Iteration 377, loss = 0.13490915\n",
      "Iteration 523, loss = 0.12602692\n",
      "Iteration 382, loss = 0.05203159\n",
      "Iteration 305, loss = 0.24349141\n",
      "Iteration 301, loss = 0.33617461\n",
      "Iteration 382, loss = 0.14536676\n",
      "Iteration 524, loss = 0.12625723\n",
      "Iteration 378, loss = 0.13474181\n",
      "Iteration 383, loss = 0.05209720\n",
      "Iteration 184, loss = 0.39579796\n",
      "Iteration 383, loss = 0.14560597\n",
      "Iteration 525, loss = 0.12582914\n",
      "Iteration 302, loss = 0.33600179\n",
      "Iteration 306, loss = 0.24326937\n",
      "Iteration 379, loss = 0.13469037\n",
      "Iteration 526, loss = 0.12601152\n",
      "Iteration 384, loss = 0.05198983\n",
      "Iteration 384, loss = 0.14495835\n",
      "Iteration 303, loss = 0.33571189\n",
      "Iteration 527, loss = 0.12600544\n",
      "Iteration 307, loss = 0.24287746\n",
      "Iteration 380, loss = 0.13458534\n",
      "Iteration 385, loss = 0.05180006\n",
      "Iteration 385, loss = 0.14522610\n",
      "Iteration 528, loss = 0.12582040\n",
      "Iteration 185, loss = 0.39588268\n",
      "Iteration 381, loss = 0.13432098\n",
      "Iteration 304, loss = 0.33563241\n",
      "Iteration 386, loss = 0.05169086\n",
      "Iteration 308, loss = 0.24305781\n",
      "Iteration 529, loss = 0.12638274\n",
      "Iteration 386, loss = 0.14491609\n",
      "Iteration 530, loss = 0.12575988\n",
      "Iteration 382, loss = 0.13429572\n",
      "Iteration 387, loss = 0.05174954\n",
      "Iteration 387, loss = 0.14492200\n",
      "Iteration 305, loss = 0.33500004\n",
      "Iteration 309, loss = 0.24268831\n",
      "Iteration 531, loss = 0.12552853\n",
      "Iteration 383, loss = 0.13443485\n",
      "Iteration 388, loss = 0.05190381\n",
      "Iteration 186, loss = 0.39559771\n",
      "Iteration 532, loss = 0.12577699\n",
      "Iteration 388, loss = 0.14499016\n",
      "Iteration 310, loss = 0.24308467\n",
      "Iteration 306, loss = 0.33496123\n",
      "Iteration 533, loss = 0.12591287\n",
      "Iteration 389, loss = 0.05166025\n",
      "Iteration 384, loss = 0.13441774\n",
      "Iteration 389, loss = 0.14474196\n",
      "Iteration 534, loss = 0.12557908\n",
      "Iteration 311, loss = 0.24239479\n",
      "Iteration 307, loss = 0.33502343\n",
      "Iteration 390, loss = 0.05150949\n",
      "Iteration 385, loss = 0.13419391\n",
      "Iteration 535, loss = 0.12574937\n",
      "Iteration 390, loss = 0.14491525\n",
      "Iteration 187, loss = 0.39513156\n",
      "Iteration 536, loss = 0.12537230\n",
      "Iteration 391, loss = 0.05171448\n",
      "Iteration 386, loss = 0.13420305\n",
      "Iteration 312, loss = 0.24257343\n",
      "Iteration 308, loss = 0.33466607\n",
      "Iteration 391, loss = 0.14503879\n",
      "Iteration 537, loss = 0.12555734\n",
      "Iteration 392, loss = 0.05166791\n",
      "Iteration 387, loss = 0.13425025\n",
      "Iteration 538, loss = 0.12533607\n",
      "Iteration 392, loss = 0.14492482\n",
      "Iteration 313, loss = 0.24252617\n",
      "Iteration 309, loss = 0.33425614\n",
      "Iteration 539, loss = 0.12542286\n",
      "Iteration 393, loss = 0.05156206\n",
      "Iteration 388, loss = 0.13409441\n",
      "Iteration 188, loss = 0.39498734\n",
      "Iteration 393, loss = 0.14461281\n",
      "Iteration 540, loss = 0.12560224\n",
      "Iteration 310, loss = 0.33420975\n",
      "Iteration 314, loss = 0.24207380\n",
      "Iteration 394, loss = 0.05151651\n",
      "Iteration 389, loss = 0.13379548\n",
      "Iteration 394, loss = 0.14467609\n",
      "Iteration 541, loss = 0.12535639\n",
      "Iteration 395, loss = 0.05156479\n",
      "Iteration 390, loss = 0.13404821\n",
      "Iteration 542, loss = 0.12519181\n",
      "Iteration 311, loss = 0.33453197\n",
      "Iteration 315, loss = 0.24206795\n",
      "Iteration 395, loss = 0.14444257\n",
      "Iteration 189, loss = 0.39485779\n",
      "Iteration 543, loss = 0.12527867\n",
      "Iteration 396, loss = 0.05142868\n",
      "Iteration 391, loss = 0.13379709\n",
      "Iteration 312, loss = 0.33369065\n",
      "Iteration 396, loss = 0.14447230\n",
      "Iteration 544, loss = 0.12562490\n",
      "Iteration 316, loss = 0.24223178\n",
      "Iteration 397, loss = 0.05172602\n",
      "Iteration 392, loss = 0.13424852\n",
      "Iteration 545, loss = 0.12533657\n",
      "Iteration 397, loss = 0.14466851\n",
      "Iteration 313, loss = 0.33348152\n",
      "Iteration 317, loss = 0.24213546\n",
      "Iteration 546, loss = 0.12513976\n",
      "Iteration 190, loss = 0.39456906\n",
      "Iteration 398, loss = 0.05118851\n",
      "Iteration 393, loss = 0.13367504\n",
      "Iteration 398, loss = 0.14440223\n",
      "Iteration 547, loss = 0.12513851\n",
      "Iteration 399, loss = 0.05111130\n",
      "Iteration 314, loss = 0.33320641\n",
      "Iteration 318, loss = 0.24163682\n",
      "Iteration 394, loss = 0.13356129\n",
      "Iteration 548, loss = 0.12512751\n",
      "Iteration 399, loss = 0.14425509\n",
      "Iteration 549, loss = 0.12520252\n",
      "Iteration 400, loss = 0.05111841\n",
      "Iteration 395, loss = 0.13366814\n",
      "Iteration 315, loss = 0.33362334\n",
      "Iteration 400, loss = 0.14428254\n",
      "Iteration 319, loss = 0.24150851\n",
      "Iteration 191, loss = 0.39461421\n",
      "Iteration 550, loss = 0.12502284\n",
      "Iteration 401, loss = 0.05124014\n",
      "Iteration 396, loss = 0.13338261\n",
      "Iteration 551, loss = 0.12488093\n",
      "Iteration 401, loss = 0.14415999\n",
      "Iteration 316, loss = 0.33272936\n",
      "Iteration 320, loss = 0.24156313\n",
      "Iteration 402, loss = 0.05107372\n",
      "Iteration 552, loss = 0.12507144\n",
      "Iteration 397, loss = 0.13327978\n",
      "Iteration 402, loss = 0.14434135\n",
      "Iteration 553, loss = 0.12500184\n",
      "Iteration 192, loss = 0.39459516\n",
      "Iteration 317, loss = 0.33249805\n",
      "Iteration 403, loss = 0.05105379\n",
      "Iteration 321, loss = 0.24129222\n",
      "Iteration 398, loss = 0.13324337\n",
      "Iteration 403, loss = 0.14400594\n",
      "Iteration 554, loss = 0.12483729\n",
      "Iteration 404, loss = 0.05088792\n",
      "Iteration 399, loss = 0.13303691\n",
      "Iteration 318, loss = 0.33273180\n",
      "Iteration 555, loss = 0.12479352\n",
      "Iteration 322, loss = 0.24128000\n",
      "Iteration 404, loss = 0.14397948\n",
      "Iteration 556, loss = 0.12468130\n",
      "Iteration 405, loss = 0.05115449\n",
      "Iteration 400, loss = 0.13318077\n",
      "Iteration 193, loss = 0.39440169\n",
      "Iteration 405, loss = 0.14417283\n",
      "Iteration 319, loss = 0.33220452\n",
      "Iteration 557, loss = 0.12437716\n",
      "Iteration 323, loss = 0.24087749\n",
      "Iteration 406, loss = 0.05108463\n",
      "Iteration 401, loss = 0.13334555\n",
      "Iteration 558, loss = 0.12482129\n",
      "Iteration 406, loss = 0.14395297\n",
      "Iteration 320, loss = 0.33232227\n",
      "Iteration 559, loss = 0.12464842\n",
      "Iteration 324, loss = 0.24087185\n",
      "Iteration 407, loss = 0.05097368\n",
      "Iteration 402, loss = 0.13298713\n",
      "Iteration 407, loss = 0.14392394\n",
      "Iteration 560, loss = 0.12456686\n",
      "Iteration 194, loss = 0.39381355\n",
      "Iteration 321, loss = 0.33198932\n",
      "Iteration 408, loss = 0.05080800\n",
      "Iteration 403, loss = 0.13313953\n",
      "Iteration 325, loss = 0.24070103\n",
      "Iteration 561, loss = 0.12453086\n",
      "Iteration 408, loss = 0.14412824\n",
      "Iteration 562, loss = 0.12469686\n",
      "Iteration 409, loss = 0.05092013\n",
      "Iteration 404, loss = 0.13285645\n",
      "Iteration 322, loss = 0.33146775\n",
      "Iteration 409, loss = 0.14377394\n",
      "Iteration 326, loss = 0.24019763\n",
      "Iteration 563, loss = 0.12447916\n",
      "Iteration 410, loss = 0.05068778\n",
      "Iteration 405, loss = 0.13282300\n",
      "Iteration 195, loss = 0.39363233\n",
      "Iteration 410, loss = 0.14379730\n",
      "Iteration 564, loss = 0.12452777\n",
      "Iteration 323, loss = 0.33141197\n",
      "Iteration 327, loss = 0.24049228\n",
      "Iteration 411, loss = 0.05077410\n",
      "Iteration 565, loss = 0.12481114\n",
      "Iteration 406, loss = 0.13293246\n",
      "Iteration 411, loss = 0.14363775\n",
      "Iteration 566, loss = 0.12443659\n",
      "Iteration 324, loss = 0.33138398\n",
      "Iteration 412, loss = 0.05069184\n",
      "Iteration 328, loss = 0.24045394\n",
      "Iteration 407, loss = 0.13268576\n",
      "Iteration 412, loss = 0.14377837\n",
      "Iteration 567, loss = 0.12456695\n",
      "Iteration 196, loss = 0.39357745\n",
      "Iteration 413, loss = 0.05080577\n",
      "Iteration 568, loss = 0.12449990\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 325, loss = 0.33111937\n",
      "Iteration 408, loss = 0.13258004\n",
      "Iteration 413, loss = 0.14371077\n",
      "Iteration 329, loss = 0.24030724\n",
      "Iteration 414, loss = 0.05063196\n",
      "Iteration 409, loss = 0.13256456\n",
      "Iteration 414, loss = 0.14359096\n",
      "Iteration 326, loss = 0.33097082\n",
      "Iteration 330, loss = 0.24037614\n",
      "Iteration 415, loss = 0.05061998\n",
      "Iteration 410, loss = 0.13256436\n",
      "Iteration 415, loss = 0.14371572\n",
      "Iteration 197, loss = 0.39330516\n",
      "Iteration 327, loss = 0.33075132\n",
      "Iteration 416, loss = 0.05062754\n",
      "Iteration 331, loss = 0.23971880\n",
      "Iteration 411, loss = 0.13246565\n",
      "Iteration 416, loss = 0.14346261\n",
      "Iteration 417, loss = 0.05073213\n",
      "Iteration 328, loss = 0.33075972\n",
      "Iteration 412, loss = 0.13257003\n",
      "Iteration 417, loss = 0.14371633\n",
      "Iteration 332, loss = 0.24015025\n",
      "Iteration 198, loss = 0.39341662\n",
      "Iteration 418, loss = 0.05055262\n",
      "Iteration 413, loss = 0.13226997\n",
      "Iteration 329, loss = 0.33022452\n",
      "Iteration 418, loss = 0.14369766\n",
      "Iteration 333, loss = 0.23974937\n",
      "Iteration 419, loss = 0.05055237\n",
      "Iteration 414, loss = 0.13209377\n",
      "Iteration 419, loss = 0.14334802\n",
      "Iteration 330, loss = 0.33023354\n",
      "Iteration 334, loss = 0.23968063\n",
      "Iteration 420, loss = 0.05049396\n",
      "Iteration 199, loss = 0.39339773\n",
      "Iteration 415, loss = 0.13231588\n",
      "Iteration 420, loss = 0.14364650\n",
      "Iteration 331, loss = 0.33017287\n",
      "Iteration 421, loss = 0.05061911\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 335, loss = 0.23965039\n",
      "Iteration 416, loss = 0.13196932\n",
      "Iteration 421, loss = 0.14322531\n",
      "Iteration 332, loss = 0.32964584\n",
      "Iteration 417, loss = 0.13200361\n",
      "Iteration 422, loss = 0.14324263\n",
      "Iteration 336, loss = 0.23931965\n",
      "Iteration 200, loss = 0.39310986\n",
      "Iteration 423, loss = 0.14308243\n",
      "Iteration 418, loss = 0.13179948\n",
      "Iteration 333, loss = 0.32975689\n",
      "Iteration 337, loss = 0.23928006\n",
      "Iteration 424, loss = 0.14324806\n",
      "Iteration 419, loss = 0.13209539\n",
      "Iteration 334, loss = 0.32936505\n",
      "Iteration 338, loss = 0.23892961\n",
      "Iteration 425, loss = 0.14321106\n",
      "Iteration 420, loss = 0.13215492\n",
      "Iteration 201, loss = 0.39258699\n",
      "Iteration 335, loss = 0.32906946\n",
      "Iteration 421, loss = 0.13174609\n",
      "Iteration 426, loss = 0.14339361\n",
      "Iteration 339, loss = 0.23875290\n",
      "Iteration 427, loss = 0.14314717\n",
      "Iteration 336, loss = 0.32918628\n",
      "Iteration 422, loss = 0.13156284\n",
      "Iteration 340, loss = 0.23900498\n",
      "Iteration 202, loss = 0.39252327\n",
      "Iteration 423, loss = 0.13187930\n",
      "Iteration 428, loss = 0.14344390\n",
      "Iteration 337, loss = 0.32869292\n",
      "Iteration 341, loss = 0.23887667\n",
      "Iteration 424, loss = 0.13163270\n",
      "Iteration 429, loss = 0.14318697\n",
      "Iteration 338, loss = 0.32822769\n",
      "Iteration 203, loss = 0.39260511\n",
      "Iteration 342, loss = 0.23899452\n",
      "Iteration 425, loss = 0.13139102\n",
      "Iteration 430, loss = 0.14287114\n",
      "Iteration 339, loss = 0.32835756\n",
      "Iteration 426, loss = 0.13145005\n",
      "Iteration 431, loss = 0.14301609\n",
      "Iteration 343, loss = 0.23861658\n",
      "Iteration 427, loss = 0.13139548\n",
      "Iteration 432, loss = 0.14282457\n",
      "Iteration 340, loss = 0.32844785\n",
      "Iteration 204, loss = 0.39217075\n",
      "Iteration 344, loss = 0.23846701\n",
      "Iteration 428, loss = 0.13155358\n",
      "Iteration 433, loss = 0.14268021\n",
      "Iteration 341, loss = 0.32792743\n",
      "Iteration 345, loss = 0.23857133\n",
      "Iteration 434, loss = 0.14272636\n",
      "Iteration 429, loss = 0.13117890\n",
      "Iteration 342, loss = 0.32750040\n",
      "Iteration 205, loss = 0.39205969\n",
      "Iteration 435, loss = 0.14265055\n",
      "Iteration 430, loss = 0.13143253\n",
      "Iteration 346, loss = 0.23828720\n",
      "Iteration 436, loss = 0.14302157\n",
      "Iteration 431, loss = 0.13102941\n",
      "Iteration 343, loss = 0.32771549\n",
      "Iteration 347, loss = 0.23835674\n",
      "Iteration 432, loss = 0.13112223\n",
      "Iteration 437, loss = 0.14248398\n",
      "Iteration 206, loss = 0.39189976\n",
      "Iteration 344, loss = 0.32735721\n",
      "Iteration 348, loss = 0.23800005\n",
      "Iteration 433, loss = 0.13107623\n",
      "Iteration 438, loss = 0.14255522\n",
      "Iteration 345, loss = 0.32752502\n",
      "Iteration 349, loss = 0.23757453\n",
      "Iteration 434, loss = 0.13103371\n",
      "Iteration 439, loss = 0.14249984\n",
      "Iteration 207, loss = 0.39205076\n",
      "Iteration 435, loss = 0.13089790\n",
      "Iteration 440, loss = 0.14241742\n",
      "Iteration 346, loss = 0.32718403\n",
      "Iteration 350, loss = 0.23790790\n",
      "Iteration 441, loss = 0.14266675\n",
      "Iteration 436, loss = 0.13075119\n",
      "Iteration 347, loss = 0.32695026\n",
      "Iteration 351, loss = 0.23767630\n",
      "Iteration 442, loss = 0.14231902\n",
      "Iteration 437, loss = 0.13084155\n",
      "Iteration 208, loss = 0.39150703\n",
      "Iteration 348, loss = 0.32630149\n",
      "Iteration 352, loss = 0.23764990\n",
      "Iteration 443, loss = 0.14212786\n",
      "Iteration 438, loss = 0.13074082\n",
      "Iteration 444, loss = 0.14257431\n",
      "Iteration 349, loss = 0.32636415\n",
      "Iteration 439, loss = 0.13092777\n",
      "Iteration 353, loss = 0.23748065\n",
      "Iteration 445, loss = 0.14237251\n",
      "Iteration 209, loss = 0.39179390\n",
      "Iteration 440, loss = 0.13048698\n",
      "Iteration 350, loss = 0.32624697\n",
      "Iteration 354, loss = 0.23718033\n",
      "Iteration 446, loss = 0.14239859\n",
      "Iteration 441, loss = 0.13069442\n",
      "Iteration 351, loss = 0.32621619\n",
      "Iteration 355, loss = 0.23690993\n",
      "Iteration 447, loss = 0.14217638\n",
      "Iteration 442, loss = 0.13048670\n",
      "Iteration 210, loss = 0.39141620\n",
      "Iteration 448, loss = 0.14223267\n",
      "Iteration 352, loss = 0.32557175\n",
      "Iteration 356, loss = 0.23683186\n",
      "Iteration 443, loss = 0.13020779\n",
      "Iteration 449, loss = 0.14216518\n",
      "Iteration 444, loss = 0.13028628\n",
      "Iteration 353, loss = 0.32557929\n",
      "Iteration 357, loss = 0.23700685\n",
      "Iteration 450, loss = 0.14217016\n",
      "Iteration 211, loss = 0.39104643\n",
      "Iteration 445, loss = 0.13013797\n",
      "Iteration 358, loss = 0.23659978\n",
      "Iteration 354, loss = 0.32520754\n",
      "Iteration 451, loss = 0.14251257\n",
      "Iteration 446, loss = 0.13033537\n",
      "Iteration 452, loss = 0.14206164\n",
      "Iteration 359, loss = 0.23653166\n",
      "Iteration 355, loss = 0.32544352\n",
      "Iteration 447, loss = 0.13005838\n",
      "Iteration 212, loss = 0.39123815\n",
      "Iteration 453, loss = 0.14208375\n",
      "Iteration 360, loss = 0.23652831\n",
      "Iteration 448, loss = 0.13000377\n",
      "Iteration 356, loss = 0.32482797\n",
      "Iteration 454, loss = 0.14204632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 449, loss = 0.13010828\n",
      "Iteration 361, loss = 0.23656972\n",
      "Iteration 357, loss = 0.32483290\n",
      "Iteration 213, loss = 0.39088343\n",
      "Iteration 450, loss = 0.12995384\n",
      "Iteration 362, loss = 0.23662480\n",
      "Iteration 358, loss = 0.32461960\n",
      "Iteration 451, loss = 0.12995721\n",
      "Iteration 363, loss = 0.23618970\n",
      "Iteration 452, loss = 0.12990396\n",
      "Iteration 359, loss = 0.32430257\n",
      "Iteration 214, loss = 0.39115779\n",
      "Iteration 453, loss = 0.13011749\n",
      "Iteration 364, loss = 0.23600343\n",
      "Iteration 360, loss = 0.32432718\n",
      "Iteration 454, loss = 0.12989821\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 365, loss = 0.23600970\n",
      "Iteration 361, loss = 0.32426753\n",
      "Iteration 215, loss = 0.39072506\n",
      "Iteration 366, loss = 0.23601551\n",
      "Iteration 362, loss = 0.32393027\n",
      "Iteration 367, loss = 0.23577959\n",
      "Iteration 216, loss = 0.39025550\n",
      "Iteration 363, loss = 0.32404415\n",
      "Iteration 368, loss = 0.23558471\n",
      "Iteration 364, loss = 0.32380184\n",
      "Iteration 369, loss = 0.23567109\n",
      "Iteration 217, loss = 0.39013018\n",
      "Iteration 365, loss = 0.32345968\n",
      "Iteration 370, loss = 0.23551253\n",
      "Iteration 366, loss = 0.32318515\n",
      "Iteration 218, loss = 0.39004514\n",
      "Iteration 371, loss = 0.23559454\n",
      "Iteration 367, loss = 0.32300058\n",
      "Iteration 372, loss = 0.23529822\n",
      "Iteration 368, loss = 0.32301635\n",
      "Iteration 219, loss = 0.38982730\n",
      "Iteration 373, loss = 0.23544491\n",
      "Iteration 369, loss = 0.32246848\n",
      "Iteration 374, loss = 0.23518444\n",
      "Iteration 370, loss = 0.32261348\n",
      "Iteration 220, loss = 0.38979666\n",
      "Iteration 375, loss = 0.23523792\n",
      "Iteration 371, loss = 0.32261426\n",
      "Iteration 376, loss = 0.23493607\n",
      "Iteration 372, loss = 0.32205107\n",
      "Iteration 221, loss = 0.38959579\n",
      "Iteration 377, loss = 0.23477458\n",
      "Iteration 373, loss = 0.32173094\n",
      "Iteration 378, loss = 0.23488194\n",
      "Iteration 374, loss = 0.32199561\n",
      "Iteration 222, loss = 0.38970446\n",
      "Iteration 379, loss = 0.23512203\n",
      "Iteration 375, loss = 0.32133954\n",
      "Iteration 380, loss = 0.23465291\n",
      "Iteration 376, loss = 0.32132944\n",
      "Iteration 223, loss = 0.38940317\n",
      "Iteration 381, loss = 0.23461059\n",
      "Iteration 377, loss = 0.32142169\n",
      "Iteration 382, loss = 0.23483451\n",
      "Iteration 378, loss = 0.32094153\n",
      "Iteration 224, loss = 0.38894368\n",
      "Iteration 383, loss = 0.23462898\n",
      "Iteration 379, loss = 0.32101071\n",
      "Iteration 384, loss = 0.23438373\n",
      "Iteration 380, loss = 0.32053658\n",
      "Iteration 225, loss = 0.38894992\n",
      "Iteration 385, loss = 0.23439791\n",
      "Iteration 381, loss = 0.32036966\n",
      "Iteration 386, loss = 0.23390720\n",
      "Iteration 382, loss = 0.32009957\n",
      "Iteration 226, loss = 0.38882045\n",
      "Iteration 387, loss = 0.23375934\n",
      "Iteration 383, loss = 0.32012240\n",
      "Iteration 388, loss = 0.23422462\n",
      "Iteration 227, loss = 0.38873692\n",
      "Iteration 384, loss = 0.31997628\n",
      "Iteration 389, loss = 0.23392388\n",
      "Iteration 385, loss = 0.31979692\n",
      "Iteration 390, loss = 0.23373427\n",
      "Iteration 228, loss = 0.38849441\n",
      "Iteration 386, loss = 0.31981110\n",
      "Iteration 391, loss = 0.23354626\n",
      "Iteration 387, loss = 0.31930780\n",
      "Iteration 392, loss = 0.23347799\n",
      "Iteration 229, loss = 0.38846918\n",
      "Iteration 388, loss = 0.31885573\n",
      "Iteration 393, loss = 0.23327801\n",
      "Iteration 389, loss = 0.31923889\n",
      "Iteration 230, loss = 0.38838699\n",
      "Iteration 394, loss = 0.23341660\n",
      "Iteration 390, loss = 0.31847727\n",
      "Iteration 395, loss = 0.23382292\n",
      "Iteration 391, loss = 0.31903218\n",
      "Iteration 231, loss = 0.38849965\n",
      "Iteration 396, loss = 0.23292661\n",
      "Iteration 392, loss = 0.31830127\n",
      "Iteration 397, loss = 0.23278835\n",
      "Iteration 393, loss = 0.31848956\n",
      "Iteration 232, loss = 0.38807997\n",
      "Iteration 398, loss = 0.23306714\n",
      "Iteration 394, loss = 0.31790303\n",
      "Iteration 399, loss = 0.23281032\n",
      "Iteration 395, loss = 0.31771178\n",
      "Iteration 233, loss = 0.38786031\n",
      "Iteration 400, loss = 0.23273368\n",
      "Iteration 396, loss = 0.31736485\n",
      "Iteration 401, loss = 0.23254459\n",
      "Iteration 397, loss = 0.31734986\n",
      "Iteration 234, loss = 0.38786476\n",
      "Iteration 402, loss = 0.23276246\n",
      "Iteration 398, loss = 0.31737410\n",
      "Iteration 403, loss = 0.23273892\n",
      "Iteration 399, loss = 0.31713066\n",
      "Iteration 235, loss = 0.38758950\n",
      "Iteration 404, loss = 0.23249271\n",
      "Iteration 400, loss = 0.31706575\n",
      "Iteration 405, loss = 0.23269000\n",
      "Iteration 236, loss = 0.38765532\n",
      "Iteration 401, loss = 0.31651296\n",
      "Iteration 406, loss = 0.23224824\n",
      "Iteration 402, loss = 0.31682386\n",
      "Iteration 407, loss = 0.23233752\n",
      "Iteration 237, loss = 0.38713557\n",
      "Iteration 403, loss = 0.31596918\n",
      "Iteration 408, loss = 0.23200778\n",
      "Iteration 404, loss = 0.31624701\n",
      "Iteration 409, loss = 0.23200384\n",
      "Iteration 238, loss = 0.38751714\n",
      "Iteration 405, loss = 0.31576864\n",
      "Iteration 410, loss = 0.23224562\n",
      "Iteration 406, loss = 0.31566977\n",
      "Iteration 411, loss = 0.23190866\n",
      "Iteration 239, loss = 0.38709500\n",
      "Iteration 407, loss = 0.31545987\n",
      "Iteration 412, loss = 0.23176855\n",
      "Iteration 408, loss = 0.31536008\n",
      "Iteration 413, loss = 0.23182103\n",
      "Iteration 240, loss = 0.38701326\n",
      "Iteration 409, loss = 0.31546059\n",
      "Iteration 414, loss = 0.23184510\n",
      "Iteration 410, loss = 0.31514495\n",
      "Iteration 241, loss = 0.38690520\n",
      "Iteration 415, loss = 0.23182477\n",
      "Iteration 411, loss = 0.31462412\n",
      "Iteration 416, loss = 0.23156113\n",
      "Iteration 412, loss = 0.31462333\n",
      "Iteration 242, loss = 0.38649420\n",
      "Iteration 417, loss = 0.23104976\n",
      "Iteration 413, loss = 0.31415600\n",
      "Iteration 418, loss = 0.23153567\n",
      "Iteration 414, loss = 0.31399945\n",
      "Iteration 243, loss = 0.38665917\n",
      "Iteration 419, loss = 0.23158537\n",
      "Iteration 415, loss = 0.31387911\n",
      "Iteration 420, loss = 0.23094915\n",
      "Iteration 416, loss = 0.31402258\n",
      "Iteration 244, loss = 0.38671631\n",
      "Iteration 421, loss = 0.23092961\n",
      "Iteration 417, loss = 0.31331374\n",
      "Iteration 422, loss = 0.23129531\n",
      "Iteration 418, loss = 0.31305233\n",
      "Iteration 245, loss = 0.38656052\n",
      "Iteration 423, loss = 0.23105249\n",
      "Iteration 419, loss = 0.31310572\n",
      "Iteration 424, loss = 0.23044308\n",
      "Iteration 246, loss = 0.38631756\n",
      "Iteration 420, loss = 0.31252647\n",
      "Iteration 425, loss = 0.23054449\n",
      "Iteration 421, loss = 0.31286987\n",
      "Iteration 426, loss = 0.23085418\n",
      "Iteration 422, loss = 0.31280583\n",
      "Iteration 247, loss = 0.38609819\n",
      "Iteration 427, loss = 0.23082466\n",
      "Iteration 423, loss = 0.31239606\n",
      "Iteration 428, loss = 0.23043731\n",
      "Iteration 424, loss = 0.31221187\n",
      "Iteration 248, loss = 0.38578381\n",
      "Iteration 429, loss = 0.23077985\n",
      "Iteration 425, loss = 0.31212021\n",
      "Iteration 430, loss = 0.23012964\n",
      "Iteration 426, loss = 0.31168882\n",
      "Iteration 249, loss = 0.38575989\n",
      "Iteration 431, loss = 0.23057058\n",
      "Iteration 427, loss = 0.31161403\n",
      "Iteration 432, loss = 0.23017261\n",
      "Iteration 250, loss = 0.38597231\n",
      "Iteration 428, loss = 0.31130985\n",
      "Iteration 433, loss = 0.22999379\n",
      "Iteration 429, loss = 0.31106593\n",
      "Iteration 434, loss = 0.22991690\n",
      "Iteration 251, loss = 0.38598705\n",
      "Iteration 430, loss = 0.31075643\n",
      "Iteration 435, loss = 0.22993029\n",
      "Iteration 431, loss = 0.31061486\n",
      "Iteration 436, loss = 0.22995170\n",
      "Iteration 252, loss = 0.38562909\n",
      "Iteration 432, loss = 0.31005979\n",
      "Iteration 437, loss = 0.23013614\n",
      "Iteration 433, loss = 0.31068575\n",
      "Iteration 438, loss = 0.22978638\n",
      "Iteration 253, loss = 0.38522164\n",
      "Iteration 434, loss = 0.31045208\n",
      "Iteration 439, loss = 0.22966467\n",
      "Iteration 435, loss = 0.31009767\n",
      "Iteration 440, loss = 0.22971411\n",
      "Iteration 254, loss = 0.38531940\n",
      "Iteration 436, loss = 0.30963929\n",
      "Iteration 441, loss = 0.22931386\n",
      "Iteration 437, loss = 0.30948525\n",
      "Iteration 255, loss = 0.38525847\n",
      "Iteration 442, loss = 0.22941094\n",
      "Iteration 438, loss = 0.30927153\n",
      "Iteration 443, loss = 0.22960900\n",
      "Iteration 439, loss = 0.30903223\n",
      "Iteration 256, loss = 0.38486168\n",
      "Iteration 444, loss = 0.22925613\n",
      "Iteration 440, loss = 0.30888435\n",
      "Iteration 445, loss = 0.22922011\n",
      "Iteration 257, loss = 0.38484086\n",
      "Iteration 441, loss = 0.30886899\n",
      "Iteration 446, loss = 0.22906300\n",
      "Iteration 442, loss = 0.30869449\n",
      "Iteration 447, loss = 0.22919852\n",
      "Iteration 258, loss = 0.38497344\n",
      "Iteration 443, loss = 0.30821084\n",
      "Iteration 448, loss = 0.22906357\n",
      "Iteration 444, loss = 0.30794766\n",
      "Iteration 449, loss = 0.22895222\n",
      "Iteration 259, loss = 0.38453642\n",
      "Iteration 445, loss = 0.30786803\n",
      "Iteration 450, loss = 0.22900605\n",
      "Iteration 446, loss = 0.30762703\n",
      "Iteration 451, loss = 0.22890597\n",
      "Iteration 260, loss = 0.38469576\n",
      "Iteration 447, loss = 0.30754668\n",
      "Iteration 452, loss = 0.22878368\n",
      "Iteration 448, loss = 0.30733718\n",
      "Iteration 453, loss = 0.22896522\n",
      "Iteration 261, loss = 0.38458393\n",
      "Iteration 449, loss = 0.30758009\n",
      "Iteration 454, loss = 0.22853450\n",
      "Iteration 450, loss = 0.30697099\n",
      "Iteration 455, loss = 0.22846778\n",
      "Iteration 262, loss = 0.38438537\n",
      "Iteration 451, loss = 0.30688283\n",
      "Iteration 456, loss = 0.22821951\n",
      "Iteration 452, loss = 0.30662780\n",
      "Iteration 457, loss = 0.22885238\n",
      "Iteration 263, loss = 0.38448286\n",
      "Iteration 453, loss = 0.30639201\n",
      "Iteration 458, loss = 0.22848431\n",
      "Iteration 459, loss = 0.22880290\n",
      "Iteration 454, loss = 0.30601680\n",
      "Iteration 264, loss = 0.38397278\n",
      "Iteration 455, loss = 0.30585824\n",
      "Iteration 460, loss = 0.22802349\n",
      "Iteration 456, loss = 0.30584683\n",
      "Iteration 461, loss = 0.22818306\n",
      "Iteration 265, loss = 0.38393914\n",
      "Iteration 457, loss = 0.30529325\n",
      "Iteration 462, loss = 0.22785621\n",
      "Iteration 463, loss = 0.22811054\n",
      "Iteration 458, loss = 0.30544666\n",
      "Iteration 266, loss = 0.38405812\n",
      "Iteration 464, loss = 0.22810285\n",
      "Iteration 459, loss = 0.30504907\n",
      "Iteration 465, loss = 0.22783471\n",
      "Iteration 460, loss = 0.30489162\n",
      "Iteration 267, loss = 0.38394051\n",
      "Iteration 466, loss = 0.22751975\n",
      "Iteration 461, loss = 0.30480330\n",
      "Iteration 467, loss = 0.22764824\n",
      "Iteration 462, loss = 0.30451612\n",
      "Iteration 268, loss = 0.38372225\n",
      "Iteration 468, loss = 0.22776579\n",
      "Iteration 463, loss = 0.30435497\n",
      "Iteration 469, loss = 0.22743149\n",
      "Iteration 464, loss = 0.30420532\n",
      "Iteration 269, loss = 0.38337270\n",
      "Iteration 470, loss = 0.22756293\n",
      "Iteration 465, loss = 0.30467314\n",
      "Iteration 471, loss = 0.22730083\n",
      "Iteration 466, loss = 0.30355769\n",
      "Iteration 270, loss = 0.38330385\n",
      "Iteration 472, loss = 0.22772429\n",
      "Iteration 467, loss = 0.30412279\n",
      "Iteration 473, loss = 0.22731330\n",
      "Iteration 468, loss = 0.30338754\n",
      "Iteration 271, loss = 0.38347834\n",
      "Iteration 474, loss = 0.22714860\n",
      "Iteration 469, loss = 0.30321546\n",
      "Iteration 475, loss = 0.22744447\n",
      "Iteration 272, loss = 0.38308992\n",
      "Iteration 470, loss = 0.30353291\n",
      "Iteration 476, loss = 0.22688418\n",
      "Iteration 471, loss = 0.30293797\n",
      "Iteration 477, loss = 0.22713567\n",
      "Iteration 273, loss = 0.38293815\n",
      "Iteration 472, loss = 0.30248540\n",
      "Iteration 478, loss = 0.22668172\n",
      "Iteration 473, loss = 0.30252074\n",
      "Iteration 274, loss = 0.38281788\n",
      "Iteration 479, loss = 0.22724808\n",
      "Iteration 474, loss = 0.30240205\n",
      "Iteration 480, loss = 0.22698767\n",
      "Iteration 475, loss = 0.30200172\n",
      "Iteration 275, loss = 0.38300587\n",
      "Iteration 481, loss = 0.22653327\n",
      "Iteration 476, loss = 0.30159371\n",
      "Iteration 482, loss = 0.22654393\n",
      "Iteration 477, loss = 0.30209075\n",
      "Iteration 276, loss = 0.38282563\n",
      "Iteration 483, loss = 0.22679971\n",
      "Iteration 478, loss = 0.30185146\n",
      "Iteration 484, loss = 0.22637748\n",
      "Iteration 479, loss = 0.30135365\n",
      "Iteration 277, loss = 0.38253355\n",
      "Iteration 485, loss = 0.22671437\n",
      "Iteration 480, loss = 0.30189659\n",
      "Iteration 486, loss = 0.22628606\n",
      "Iteration 481, loss = 0.30081952\n",
      "Iteration 278, loss = 0.38279690\n",
      "Iteration 487, loss = 0.22656589\n",
      "Iteration 482, loss = 0.30092981\n",
      "Iteration 488, loss = 0.22599981\n",
      "Iteration 483, loss = 0.30104156\n",
      "Iteration 279, loss = 0.38257199\n",
      "Iteration 489, loss = 0.22610600\n",
      "Iteration 484, loss = 0.30088989\n",
      "Iteration 490, loss = 0.22609571\n",
      "Iteration 485, loss = 0.30009025\n",
      "Iteration 280, loss = 0.38231018\n",
      "Iteration 491, loss = 0.22612603\n",
      "Iteration 486, loss = 0.30065443\n",
      "Iteration 492, loss = 0.22618248\n",
      "Iteration 487, loss = 0.30004139\n",
      "Iteration 281, loss = 0.38234756\n",
      "Iteration 493, loss = 0.22574874\n",
      "Iteration 488, loss = 0.29985735\n",
      "Iteration 494, loss = 0.22592008\n",
      "Iteration 489, loss = 0.29982913\n",
      "Iteration 282, loss = 0.38235053\n",
      "Iteration 495, loss = 0.22585961\n",
      "Iteration 490, loss = 0.29929145\n",
      "Iteration 496, loss = 0.22597849\n",
      "Iteration 283, loss = 0.38207640\n",
      "Iteration 491, loss = 0.29920428\n",
      "Iteration 497, loss = 0.22561240\n",
      "Iteration 492, loss = 0.29926766\n",
      "Iteration 498, loss = 0.22549426\n",
      "Iteration 284, loss = 0.38174344\n",
      "Iteration 493, loss = 0.29881751\n",
      "Iteration 499, loss = 0.22572025\n",
      "Iteration 494, loss = 0.29903078\n",
      "Iteration 500, loss = 0.22575095\n",
      "Iteration 285, loss = 0.38168330\n",
      "Iteration 495, loss = 0.29848334\n",
      "Iteration 501, loss = 0.22525856\n",
      "Iteration 496, loss = 0.29832001\n",
      "Iteration 502, loss = 0.22530275\n",
      "Iteration 286, loss = 0.38135871\n",
      "Iteration 497, loss = 0.29810384\n",
      "Iteration 503, loss = 0.22544312\n",
      "Iteration 498, loss = 0.29827213\n",
      "Iteration 504, loss = 0.22521668\n",
      "Iteration 287, loss = 0.38136936\n",
      "Iteration 499, loss = 0.29813205\n",
      "Iteration 505, loss = 0.22552541\n",
      "Iteration 500, loss = 0.29744278\n",
      "Iteration 288, loss = 0.38134221\n",
      "Iteration 506, loss = 0.22525416\n",
      "Iteration 501, loss = 0.29750141\n",
      "Iteration 507, loss = 0.22519259\n",
      "Iteration 502, loss = 0.29735482\n",
      "Iteration 289, loss = 0.38130797\n",
      "Iteration 508, loss = 0.22511335\n",
      "Iteration 503, loss = 0.29697124\n",
      "Iteration 509, loss = 0.22489798\n",
      "Iteration 504, loss = 0.29736216\n",
      "Iteration 290, loss = 0.38174845\n",
      "Iteration 510, loss = 0.22508566\n",
      "Iteration 505, loss = 0.29683737\n",
      "Iteration 511, loss = 0.22466689\n",
      "Iteration 506, loss = 0.29676714\n",
      "Iteration 291, loss = 0.38097227\n",
      "Iteration 512, loss = 0.22496910\n",
      "Iteration 507, loss = 0.29690638\n",
      "Iteration 513, loss = 0.22472419\n",
      "Iteration 508, loss = 0.29616632\n",
      "Iteration 292, loss = 0.38112001\n",
      "Iteration 514, loss = 0.22446663\n",
      "Iteration 509, loss = 0.29584870\n",
      "Iteration 515, loss = 0.22458008\n",
      "Iteration 510, loss = 0.29607446\n",
      "Iteration 293, loss = 0.38130674\n",
      "Iteration 516, loss = 0.22429252\n",
      "Iteration 511, loss = 0.29576603\n",
      "Iteration 517, loss = 0.22477340\n",
      "Iteration 512, loss = 0.29561603\n",
      "Iteration 294, loss = 0.38066117\n",
      "Iteration 518, loss = 0.22435567\n",
      "Iteration 513, loss = 0.29577811\n",
      "Iteration 519, loss = 0.22438911\n",
      "Iteration 514, loss = 0.29529120\n",
      "Iteration 295, loss = 0.38088715\n",
      "Iteration 520, loss = 0.22443481\n",
      "Iteration 515, loss = 0.29518184\n",
      "Iteration 521, loss = 0.22406752\n",
      "Iteration 296, loss = 0.38068416\n",
      "Iteration 516, loss = 0.29499108\n",
      "Iteration 522, loss = 0.22433349\n",
      "Iteration 517, loss = 0.29444852\n",
      "Iteration 523, loss = 0.22463738\n",
      "Iteration 297, loss = 0.38047621\n",
      "Iteration 518, loss = 0.29533171\n",
      "Iteration 524, loss = 0.22386774\n",
      "Iteration 519, loss = 0.29480643\n",
      "Iteration 525, loss = 0.22426893\n",
      "Iteration 298, loss = 0.38045005\n",
      "Iteration 520, loss = 0.29402380\n",
      "Iteration 526, loss = 0.22393691\n",
      "Iteration 521, loss = 0.29432068\n",
      "Iteration 527, loss = 0.22377202\n",
      "Iteration 299, loss = 0.38050333\n",
      "Iteration 522, loss = 0.29416860\n",
      "Iteration 528, loss = 0.22399191\n",
      "Iteration 523, loss = 0.29407571\n",
      "Iteration 529, loss = 0.22395493\n",
      "Iteration 300, loss = 0.38023216\n",
      "Iteration 524, loss = 0.29363255\n",
      "Iteration 530, loss = 0.22375461\n",
      "Iteration 525, loss = 0.29352843\n",
      "Iteration 531, loss = 0.22366951\n",
      "Iteration 301, loss = 0.38022617\n",
      "Iteration 526, loss = 0.29359340\n",
      "Iteration 532, loss = 0.22362990\n",
      "Iteration 527, loss = 0.29316128\n",
      "Iteration 533, loss = 0.22354047\n",
      "Iteration 302, loss = 0.38053481\n",
      "Iteration 528, loss = 0.29244994\n",
      "Iteration 534, loss = 0.22318041\n",
      "Iteration 529, loss = 0.29317955\n",
      "Iteration 535, loss = 0.22333866\n",
      "Iteration 303, loss = 0.38010642\n",
      "Iteration 530, loss = 0.29284273\n",
      "Iteration 536, loss = 0.22368193\n",
      "Iteration 531, loss = 0.29234244\n",
      "Iteration 304, loss = 0.38030810\n",
      "Iteration 537, loss = 0.22332100\n",
      "Iteration 532, loss = 0.29259024\n",
      "Iteration 538, loss = 0.22324720\n",
      "Iteration 533, loss = 0.29219459\n",
      "Iteration 305, loss = 0.37977386\n",
      "Iteration 539, loss = 0.22325983\n",
      "Iteration 534, loss = 0.29190580\n",
      "Iteration 540, loss = 0.22308819\n",
      "Iteration 535, loss = 0.29188128\n",
      "Iteration 306, loss = 0.37985201\n",
      "Iteration 541, loss = 0.22299593\n",
      "Iteration 536, loss = 0.29184525\n",
      "Iteration 542, loss = 0.22278692\n",
      "Iteration 537, loss = 0.29167431\n",
      "Iteration 307, loss = 0.37958579\n",
      "Iteration 543, loss = 0.22280933\n",
      "Iteration 538, loss = 0.29192304\n",
      "Iteration 544, loss = 0.22296046\n",
      "Iteration 539, loss = 0.29080809\n",
      "Iteration 308, loss = 0.37950690\n",
      "Iteration 545, loss = 0.22292934\n",
      "Iteration 540, loss = 0.29118613\n",
      "Iteration 546, loss = 0.22339918\n",
      "Iteration 541, loss = 0.29112510\n",
      "Iteration 309, loss = 0.37992386\n",
      "Iteration 547, loss = 0.22233904\n",
      "Iteration 542, loss = 0.29078324\n",
      "Iteration 548, loss = 0.22281562\n",
      "Iteration 543, loss = 0.29100814\n",
      "Iteration 310, loss = 0.37913676\n",
      "Iteration 549, loss = 0.22235852\n",
      "Iteration 544, loss = 0.29034112\n",
      "Iteration 550, loss = 0.22262365\n",
      "Iteration 545, loss = 0.29044313\n",
      "Iteration 311, loss = 0.37936592\n",
      "Iteration 551, loss = 0.22269232\n",
      "Iteration 546, loss = 0.29015669\n",
      "Iteration 552, loss = 0.22220314\n",
      "Iteration 547, loss = 0.29028254\n",
      "Iteration 312, loss = 0.37939907\n",
      "Iteration 553, loss = 0.22240762\n",
      "Iteration 548, loss = 0.29003984\n",
      "Iteration 554, loss = 0.22222781\n",
      "Iteration 549, loss = 0.29001991\n",
      "Iteration 313, loss = 0.37897690\n",
      "Iteration 555, loss = 0.22169842\n",
      "Iteration 550, loss = 0.28965354\n",
      "Iteration 556, loss = 0.22224229\n",
      "Iteration 314, loss = 0.37876991\n",
      "Iteration 551, loss = 0.28952127\n",
      "Iteration 557, loss = 0.22228320\n",
      "Iteration 552, loss = 0.28916693\n",
      "Iteration 558, loss = 0.22213380\n",
      "Iteration 315, loss = 0.37886334\n",
      "Iteration 553, loss = 0.28947723\n",
      "Iteration 559, loss = 0.22240226\n",
      "Iteration 554, loss = 0.28907574\n",
      "Iteration 560, loss = 0.22181684\n",
      "Iteration 316, loss = 0.37858183\n",
      "Iteration 555, loss = 0.28879580\n",
      "Iteration 561, loss = 0.22243326\n",
      "Iteration 556, loss = 0.28873357\n",
      "Iteration 562, loss = 0.22180506\n",
      "Iteration 317, loss = 0.37886521\n",
      "Iteration 557, loss = 0.28886495\n",
      "Iteration 563, loss = 0.22179310\n",
      "Iteration 558, loss = 0.28821918\n",
      "Iteration 564, loss = 0.22162829\n",
      "Iteration 318, loss = 0.37852667\n",
      "Iteration 559, loss = 0.28805422\n",
      "Iteration 565, loss = 0.22175118\n",
      "Iteration 560, loss = 0.28807555\n",
      "Iteration 566, loss = 0.22136802\n",
      "Iteration 319, loss = 0.37817506\n",
      "Iteration 561, loss = 0.28783892\n",
      "Iteration 567, loss = 0.22212410\n",
      "Iteration 562, loss = 0.28761445\n",
      "Iteration 320, loss = 0.37824303\n",
      "Iteration 568, loss = 0.22146592\n",
      "Iteration 563, loss = 0.28745694\n",
      "Iteration 569, loss = 0.22134129\n",
      "Iteration 564, loss = 0.28723639\n",
      "Iteration 321, loss = 0.37848973\n",
      "Iteration 570, loss = 0.22181362\n",
      "Iteration 565, loss = 0.28733741\n",
      "Iteration 571, loss = 0.22143178\n",
      "Iteration 566, loss = 0.28734920\n",
      "Iteration 322, loss = 0.37829744\n",
      "Iteration 572, loss = 0.22158141\n",
      "Iteration 567, loss = 0.28694369\n",
      "Iteration 573, loss = 0.22179236\n",
      "Iteration 568, loss = 0.28693652\n",
      "Iteration 323, loss = 0.37819583\n",
      "Iteration 574, loss = 0.22147375\n",
      "Iteration 569, loss = 0.28701710\n",
      "Iteration 575, loss = 0.22103742\n",
      "Iteration 570, loss = 0.28671417\n",
      "Iteration 324, loss = 0.37793660\n",
      "Iteration 576, loss = 0.22079818\n",
      "Iteration 571, loss = 0.28681378\n",
      "Iteration 577, loss = 0.22131082\n",
      "Iteration 325, loss = 0.37833658\n",
      "Iteration 572, loss = 0.28666102\n",
      "Iteration 578, loss = 0.22131077\n",
      "Iteration 573, loss = 0.28598011\n",
      "Iteration 579, loss = 0.22186036\n",
      "Iteration 326, loss = 0.37796613\n",
      "Iteration 574, loss = 0.28590559\n",
      "Iteration 580, loss = 0.22055477\n",
      "Iteration 575, loss = 0.28609067\n",
      "Iteration 581, loss = 0.22072885\n",
      "Iteration 327, loss = 0.37786201\n",
      "Iteration 576, loss = 0.28567459\n",
      "Iteration 582, loss = 0.22069969\n",
      "Iteration 577, loss = 0.28553811\n",
      "Iteration 583, loss = 0.22047258\n",
      "Iteration 328, loss = 0.37742017\n",
      "Iteration 578, loss = 0.28516760\n",
      "Iteration 584, loss = 0.22137001\n",
      "Iteration 579, loss = 0.28525432\n",
      "Iteration 585, loss = 0.22054108\n",
      "Iteration 329, loss = 0.37747737\n",
      "Iteration 580, loss = 0.28513349\n",
      "Iteration 586, loss = 0.22040765\n",
      "Iteration 581, loss = 0.28500949\n",
      "Iteration 587, loss = 0.22039856\n",
      "Iteration 330, loss = 0.37724401\n",
      "Iteration 582, loss = 0.28487665\n",
      "Iteration 588, loss = 0.22071205\n",
      "Iteration 583, loss = 0.28491729\n",
      "Iteration 589, loss = 0.22028676\n",
      "Iteration 331, loss = 0.37730286\n",
      "Iteration 584, loss = 0.28481100\n",
      "Iteration 590, loss = 0.22058553\n",
      "Iteration 585, loss = 0.28432060\n",
      "Iteration 591, loss = 0.22025567\n",
      "Iteration 332, loss = 0.37752041\n",
      "Iteration 586, loss = 0.28450270\n",
      "Iteration 592, loss = 0.22042082\n",
      "Iteration 587, loss = 0.28395394\n",
      "Iteration 333, loss = 0.37740623\n",
      "Iteration 593, loss = 0.22016758\n",
      "Iteration 588, loss = 0.28383969\n",
      "Iteration 594, loss = 0.22043654\n",
      "Iteration 589, loss = 0.28375503\n",
      "Iteration 334, loss = 0.37726351\n",
      "Iteration 595, loss = 0.21989484\n",
      "Iteration 590, loss = 0.28375938\n",
      "Iteration 596, loss = 0.21994732\n",
      "Iteration 335, loss = 0.37711920\n",
      "Iteration 591, loss = 0.28376515\n",
      "Iteration 597, loss = 0.22015541\n",
      "Iteration 592, loss = 0.28361555\n",
      "Iteration 598, loss = 0.21995293\n",
      "Iteration 336, loss = 0.37730389\n",
      "Iteration 593, loss = 0.28375978\n",
      "Iteration 599, loss = 0.21982332\n",
      "Iteration 594, loss = 0.28309800\n",
      "Iteration 600, loss = 0.21996486\n",
      "Iteration 337, loss = 0.37668610\n",
      "Iteration 595, loss = 0.28332260\n",
      "Iteration 601, loss = 0.21996563\n",
      "Iteration 596, loss = 0.28249009\n",
      "Iteration 602, loss = 0.21968352\n",
      "Iteration 338, loss = 0.37650130\n",
      "Iteration 597, loss = 0.28294067\n",
      "Iteration 603, loss = 0.22011067\n",
      "Iteration 598, loss = 0.28297435\n",
      "Iteration 604, loss = 0.21956075\n",
      "Iteration 339, loss = 0.37671156\n",
      "Iteration 599, loss = 0.28243538\n",
      "Iteration 605, loss = 0.21956877\n",
      "Iteration 600, loss = 0.28254891\n",
      "Iteration 606, loss = 0.21917051\n",
      "Iteration 340, loss = 0.37680589\n",
      "Iteration 601, loss = 0.28203074\n",
      "Iteration 607, loss = 0.21953954\n",
      "Iteration 602, loss = 0.28207712\n",
      "Iteration 608, loss = 0.21951195\n",
      "Iteration 341, loss = 0.37673930\n",
      "Iteration 603, loss = 0.28205637\n",
      "Iteration 609, loss = 0.21962005\n",
      "Iteration 604, loss = 0.28203850\n",
      "Iteration 610, loss = 0.21950103\n",
      "Iteration 342, loss = 0.37619270\n",
      "Iteration 605, loss = 0.28164546\n",
      "Iteration 611, loss = 0.21961470\n",
      "Iteration 606, loss = 0.28184882\n",
      "Iteration 612, loss = 0.21946831\n",
      "Iteration 343, loss = 0.37648128\n",
      "Iteration 607, loss = 0.28103395\n",
      "Iteration 613, loss = 0.21926573\n",
      "Iteration 608, loss = 0.28128343\n",
      "Iteration 344, loss = 0.37622249\n",
      "Iteration 614, loss = 0.21941752\n",
      "Iteration 609, loss = 0.28117552\n",
      "Iteration 615, loss = 0.21919376\n",
      "Iteration 610, loss = 0.28105563\n",
      "Iteration 345, loss = 0.37595395\n",
      "Iteration 616, loss = 0.21937897\n",
      "Iteration 611, loss = 0.28102608\n",
      "Iteration 617, loss = 0.21917594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 346, loss = 0.37632556\n",
      "Iteration 612, loss = 0.28075547\n",
      "Iteration 613, loss = 0.28068060\n",
      "Iteration 347, loss = 0.37607612\n",
      "Iteration 614, loss = 0.28016720\n",
      "Iteration 615, loss = 0.28037256\n",
      "Iteration 348, loss = 0.37609315\n",
      "Iteration 616, loss = 0.28005226\n",
      "Iteration 617, loss = 0.28024763\n",
      "Iteration 349, loss = 0.37575065\n",
      "Iteration 618, loss = 0.28053704\n",
      "Iteration 619, loss = 0.27976767\n",
      "Iteration 350, loss = 0.37571590\n",
      "Iteration 620, loss = 0.27962240\n",
      "Iteration 621, loss = 0.27975698\n",
      "Iteration 351, loss = 0.37612047\n",
      "Iteration 622, loss = 0.27938694\n",
      "Iteration 623, loss = 0.27959330\n",
      "Iteration 352, loss = 0.37571715\n",
      "Iteration 624, loss = 0.27941374\n",
      "Iteration 625, loss = 0.27894381\n",
      "Iteration 353, loss = 0.37542900\n",
      "Iteration 626, loss = 0.27932285\n",
      "Iteration 627, loss = 0.27920303\n",
      "Iteration 354, loss = 0.37593285\n",
      "Iteration 628, loss = 0.27886201\n",
      "Iteration 629, loss = 0.27826375\n",
      "Iteration 355, loss = 0.37545681\n",
      "Iteration 630, loss = 0.27814794\n",
      "Iteration 631, loss = 0.27858624\n",
      "Iteration 356, loss = 0.37525513\n",
      "Iteration 632, loss = 0.27845435\n",
      "Iteration 633, loss = 0.27818567\n",
      "Iteration 357, loss = 0.37503166\n",
      "Iteration 634, loss = 0.27763127\n",
      "Iteration 635, loss = 0.27797382\n",
      "Iteration 358, loss = 0.37543898\n",
      "Iteration 636, loss = 0.27794205\n",
      "Iteration 637, loss = 0.27752523\n",
      "Iteration 359, loss = 0.37507953\n",
      "Iteration 638, loss = 0.27755243\n",
      "Iteration 639, loss = 0.27711119\n",
      "Iteration 360, loss = 0.37507550\n",
      "Iteration 640, loss = 0.27769161\n",
      "Iteration 641, loss = 0.27706569\n",
      "Iteration 361, loss = 0.37497835\n",
      "Iteration 642, loss = 0.27711913\n",
      "Iteration 362, loss = 0.37476272\n",
      "Iteration 643, loss = 0.27677163\n",
      "Iteration 644, loss = 0.27695943\n",
      "Iteration 363, loss = 0.37471969\n",
      "Iteration 645, loss = 0.27676381\n",
      "Iteration 646, loss = 0.27692109\n",
      "Iteration 364, loss = 0.37468607\n",
      "Iteration 647, loss = 0.27630970\n",
      "Iteration 648, loss = 0.27644819\n",
      "Iteration 365, loss = 0.37492332\n",
      "Iteration 649, loss = 0.27633096\n",
      "Iteration 650, loss = 0.27590446\n",
      "Iteration 366, loss = 0.37467768\n",
      "Iteration 651, loss = 0.27607649\n",
      "Iteration 652, loss = 0.27589216\n",
      "Iteration 367, loss = 0.37414166\n",
      "Iteration 653, loss = 0.27620575\n",
      "Iteration 654, loss = 0.27598785\n",
      "Iteration 368, loss = 0.37432475\n",
      "Iteration 655, loss = 0.27570841\n",
      "Iteration 656, loss = 0.27558182\n",
      "Iteration 369, loss = 0.37414982\n",
      "Iteration 657, loss = 0.27552304\n",
      "Iteration 658, loss = 0.27488223\n",
      "Iteration 370, loss = 0.37387451\n",
      "Iteration 659, loss = 0.27517427\n",
      "Iteration 660, loss = 0.27515147\n",
      "Iteration 371, loss = 0.37420178\n",
      "Iteration 661, loss = 0.27503538\n",
      "Iteration 662, loss = 0.27495522\n",
      "Iteration 372, loss = 0.37391910\n",
      "Iteration 663, loss = 0.27510697\n",
      "Iteration 664, loss = 0.27444634\n",
      "Iteration 373, loss = 0.37407301\n",
      "Iteration 665, loss = 0.27440104\n",
      "Iteration 374, loss = 0.37399219\n",
      "Iteration 666, loss = 0.27464449\n",
      "Iteration 667, loss = 0.27436927\n",
      "Iteration 375, loss = 0.37363202\n",
      "Iteration 668, loss = 0.27443570\n",
      "Iteration 669, loss = 0.27377389\n",
      "Iteration 376, loss = 0.37352374\n",
      "Iteration 670, loss = 0.27399177\n",
      "Iteration 671, loss = 0.27407551\n",
      "Iteration 377, loss = 0.37390500\n",
      "Iteration 672, loss = 0.27436930\n",
      "Iteration 673, loss = 0.27406221\n",
      "Iteration 378, loss = 0.37357263\n",
      "Iteration 674, loss = 0.27333510\n",
      "Iteration 675, loss = 0.27336733\n",
      "Iteration 379, loss = 0.37331295\n",
      "Iteration 676, loss = 0.27347069\n",
      "Iteration 677, loss = 0.27349559\n",
      "Iteration 380, loss = 0.37343381\n",
      "Iteration 678, loss = 0.27353529\n",
      "Iteration 679, loss = 0.27283469\n",
      "Iteration 381, loss = 0.37312404\n",
      "Iteration 680, loss = 0.27304924\n",
      "Iteration 681, loss = 0.27301301\n",
      "Iteration 382, loss = 0.37306570\n",
      "Iteration 682, loss = 0.27276696\n",
      "Iteration 683, loss = 0.27294596\n",
      "Iteration 383, loss = 0.37312339\n",
      "Iteration 684, loss = 0.27257875\n",
      "Iteration 685, loss = 0.27215571\n",
      "Iteration 384, loss = 0.37273232\n",
      "Iteration 686, loss = 0.27248301\n",
      "Iteration 687, loss = 0.27230900\n",
      "Iteration 385, loss = 0.37281096\n",
      "Iteration 688, loss = 0.27207493\n",
      "Iteration 386, loss = 0.37279684\n",
      "Iteration 689, loss = 0.27172974\n",
      "Iteration 690, loss = 0.27216795\n",
      "Iteration 387, loss = 0.37274161\n",
      "Iteration 691, loss = 0.27214542\n",
      "Iteration 692, loss = 0.27156533\n",
      "Iteration 388, loss = 0.37260081\n",
      "Iteration 693, loss = 0.27152039\n",
      "Iteration 694, loss = 0.27198813\n",
      "Iteration 389, loss = 0.37285202\n",
      "Iteration 695, loss = 0.27111402\n",
      "Iteration 696, loss = 0.27128054\n",
      "Iteration 390, loss = 0.37268246\n",
      "Iteration 697, loss = 0.27108860\n",
      "Iteration 698, loss = 0.27074099\n",
      "Iteration 391, loss = 0.37249577\n",
      "Iteration 699, loss = 0.27123154\n",
      "Iteration 700, loss = 0.27116076\n",
      "Iteration 392, loss = 0.37204512\n",
      "Iteration 701, loss = 0.27046912\n",
      "Iteration 702, loss = 0.27092753\n",
      "Iteration 393, loss = 0.37218655\n",
      "Iteration 703, loss = 0.27094985\n",
      "Iteration 704, loss = 0.27041856\n",
      "Iteration 394, loss = 0.37245738\n",
      "Iteration 705, loss = 0.27060792\n",
      "Iteration 706, loss = 0.27066877\n",
      "Iteration 395, loss = 0.37234742\n",
      "Iteration 707, loss = 0.27014013\n",
      "Iteration 708, loss = 0.27029626\n",
      "Iteration 396, loss = 0.37213858\n",
      "Iteration 709, loss = 0.27017246\n",
      "Iteration 710, loss = 0.26993589\n",
      "Iteration 397, loss = 0.37212431\n",
      "Iteration 711, loss = 0.27016211\n",
      "Iteration 398, loss = 0.37239028\n",
      "Iteration 712, loss = 0.26939819\n",
      "Iteration 713, loss = 0.26967120\n",
      "Iteration 399, loss = 0.37166958\n",
      "Iteration 714, loss = 0.26965744\n",
      "Iteration 715, loss = 0.26975800\n",
      "Iteration 400, loss = 0.37172708\n",
      "Iteration 716, loss = 0.26921861\n",
      "Iteration 717, loss = 0.26944211\n",
      "Iteration 401, loss = 0.37188097\n",
      "Iteration 718, loss = 0.26908961\n",
      "Iteration 719, loss = 0.26922354\n",
      "Iteration 402, loss = 0.37164609\n",
      "Iteration 720, loss = 0.26866039\n",
      "Iteration 721, loss = 0.26918939\n",
      "Iteration 403, loss = 0.37179226\n",
      "Iteration 722, loss = 0.26870797\n",
      "Iteration 723, loss = 0.26835478\n",
      "Iteration 404, loss = 0.37127234\n",
      "Iteration 724, loss = 0.26816916\n",
      "Iteration 725, loss = 0.26849171\n",
      "Iteration 405, loss = 0.37132584\n",
      "Iteration 726, loss = 0.26866778\n",
      "Iteration 727, loss = 0.26836375\n",
      "Iteration 406, loss = 0.37125245\n",
      "Iteration 728, loss = 0.26812160\n",
      "Iteration 729, loss = 0.26816028\n",
      "Iteration 407, loss = 0.37117070\n",
      "Iteration 730, loss = 0.26796451\n",
      "Iteration 408, loss = 0.37139136\n",
      "Iteration 731, loss = 0.26819409\n",
      "Iteration 732, loss = 0.26812621\n",
      "Iteration 409, loss = 0.37157727\n",
      "Iteration 733, loss = 0.26764080\n",
      "Iteration 734, loss = 0.26753343\n",
      "Iteration 410, loss = 0.37102158\n",
      "Iteration 735, loss = 0.26797750\n",
      "Iteration 736, loss = 0.26729444\n",
      "Iteration 411, loss = 0.37101780\n",
      "Iteration 737, loss = 0.26746792\n",
      "Iteration 738, loss = 0.26747935\n",
      "Iteration 412, loss = 0.37094257\n",
      "Iteration 739, loss = 0.26725689\n",
      "Iteration 740, loss = 0.26728515\n",
      "Iteration 413, loss = 0.37079009\n",
      "Iteration 741, loss = 0.26742892\n",
      "Iteration 742, loss = 0.26717499\n",
      "Iteration 414, loss = 0.37110829\n",
      "Iteration 743, loss = 0.26685820\n",
      "Iteration 744, loss = 0.26693804\n",
      "Iteration 415, loss = 0.37079097\n",
      "Iteration 745, loss = 0.26670731\n",
      "Iteration 746, loss = 0.26632113\n",
      "Iteration 416, loss = 0.37085706\n",
      "Iteration 747, loss = 0.26665963\n",
      "Iteration 748, loss = 0.26648900\n",
      "Iteration 417, loss = 0.37075496\n",
      "Iteration 749, loss = 0.26621219\n",
      "Iteration 750, loss = 0.26667853\n",
      "Iteration 418, loss = 0.37096601\n",
      "Iteration 751, loss = 0.26587726\n",
      "Iteration 752, loss = 0.26626219\n",
      "Iteration 419, loss = 0.37050158\n",
      "Iteration 753, loss = 0.26604130\n",
      "Iteration 754, loss = 0.26564103\n",
      "Iteration 420, loss = 0.37043752\n",
      "Iteration 755, loss = 0.26657566\n",
      "Iteration 421, loss = 0.37000791\n",
      "Iteration 756, loss = 0.26586387\n",
      "Iteration 757, loss = 0.26581910\n",
      "Iteration 422, loss = 0.37033388\n",
      "Iteration 758, loss = 0.26563857\n",
      "Iteration 759, loss = 0.26535863\n",
      "Iteration 423, loss = 0.37033871\n",
      "Iteration 760, loss = 0.26540626\n",
      "Iteration 761, loss = 0.26531084\n",
      "Iteration 424, loss = 0.36988161\n",
      "Iteration 762, loss = 0.26506344\n",
      "Iteration 763, loss = 0.26540913\n",
      "Iteration 425, loss = 0.37004406\n",
      "Iteration 764, loss = 0.26561227\n",
      "Iteration 765, loss = 0.26542825\n",
      "Iteration 426, loss = 0.37048098\n",
      "Iteration 766, loss = 0.26497045\n",
      "Iteration 767, loss = 0.26504506\n",
      "Iteration 427, loss = 0.36979866\n",
      "Iteration 768, loss = 0.26486067\n",
      "Iteration 769, loss = 0.26440571\n",
      "Iteration 428, loss = 0.36972996\n",
      "Iteration 770, loss = 0.26484311\n",
      "Iteration 771, loss = 0.26491395\n",
      "Iteration 429, loss = 0.36992800\n",
      "Iteration 772, loss = 0.26419762\n",
      "Iteration 773, loss = 0.26465902\n",
      "Iteration 430, loss = 0.36986627\n",
      "Iteration 774, loss = 0.26434313\n",
      "Iteration 775, loss = 0.26466388\n",
      "Iteration 431, loss = 0.36984310\n",
      "Iteration 776, loss = 0.26381033\n",
      "Iteration 777, loss = 0.26402608\n",
      "Iteration 432, loss = 0.36974773\n",
      "Iteration 778, loss = 0.26425350\n",
      "Iteration 779, loss = 0.26411142\n",
      "Iteration 433, loss = 0.36974541\n",
      "Iteration 780, loss = 0.26412503\n",
      "Iteration 434, loss = 0.36987560\n",
      "Iteration 781, loss = 0.26367081\n",
      "Iteration 782, loss = 0.26438763\n",
      "Iteration 435, loss = 0.37025174\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 783, loss = 0.26370624\n",
      "Iteration 784, loss = 0.26362455\n",
      "Iteration 785, loss = 0.26358187\n",
      "Iteration 786, loss = 0.26370834\n",
      "Iteration 787, loss = 0.26366985\n",
      "Iteration 788, loss = 0.26338601\n",
      "Iteration 789, loss = 0.26344697\n",
      "Iteration 790, loss = 0.26322496\n",
      "Iteration 791, loss = 0.26311318\n",
      "Iteration 792, loss = 0.26321653\n",
      "Iteration 793, loss = 0.26347664\n",
      "Iteration 794, loss = 0.26288833\n",
      "Iteration 795, loss = 0.26306890\n",
      "Iteration 796, loss = 0.26266786\n",
      "Iteration 797, loss = 0.26314931\n",
      "Iteration 798, loss = 0.26291979\n",
      "Iteration 799, loss = 0.26272186\n",
      "Iteration 800, loss = 0.26259982\n",
      "Iteration 801, loss = 0.26277025\n",
      "Iteration 802, loss = 0.26203512\n",
      "Iteration 803, loss = 0.26303699\n",
      "Iteration 804, loss = 0.26228453\n",
      "Iteration 805, loss = 0.26221193\n",
      "Iteration 806, loss = 0.26235859\n",
      "Iteration 807, loss = 0.26227845\n",
      "Iteration 808, loss = 0.26256447\n",
      "Iteration 809, loss = 0.26241634\n",
      "Iteration 810, loss = 0.26195285\n",
      "Iteration 811, loss = 0.26214158\n",
      "Iteration 812, loss = 0.26190865\n",
      "Iteration 813, loss = 0.26208620\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy on training set: 0.68\n"
     ]
    }
   ],
   "source": [
    "# create the MLP classifier\n",
    "mlp = MLPClassifier(verbose=True, max_iter=max_iterations)\n",
    "\n",
    "# create the one-vs-one classifier\n",
    "ovo_classifier = OneVsOneClassifier(mlp, n_jobs=-1)\n",
    "\n",
    "ovo_classifier.fit(X_train, t_train)\n",
    "\n",
    "# save the trained classifier to a file\n",
    "with open('./models/model1.pkl', 'wb') as f:\n",
    "    pickle.dump(ovo_classifier, f)\n",
    "\n",
    "print('Accuracy on training set: {:.2f}'.format(ovo_classifier.score(X_train, t_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38572799\n",
      "Iteration 1, loss = 0.40259838\n",
      "Iteration 1, loss = 0.40803386\n",
      "Iteration 1, loss = 0.41242150\n",
      "Iteration 1, loss = 0.29355731\n",
      "Iteration 1, loss = 0.38144565\n",
      "Iteration 1, loss = 0.38621790\n",
      "Iteration 2, loss = 0.33754782\n",
      "Iteration 2, loss = 0.37457033\n",
      "Iteration 2, loss = 0.39511025\n",
      "Iteration 2, loss = 0.39552642\n",
      "Iteration 2, loss = 0.21104350\n",
      "Iteration 2, loss = 0.31863640\n",
      "Iteration 2, loss = 0.34081506\n",
      "Iteration 3, loss = 0.32971033\n",
      "Iteration 3, loss = 0.35780033\n",
      "Iteration 3, loss = 0.39103769\n",
      "Iteration 3, loss = 0.38529397\n",
      "Iteration 3, loss = 0.16589342\n",
      "Iteration 3, loss = 0.29540093\n",
      "Iteration 3, loss = 0.33387402\n",
      "Iteration 4, loss = 0.32726795\n",
      "Iteration 4, loss = 0.38876379\n",
      "Iteration 4, loss = 0.37791312\n",
      "Iteration 4, loss = 0.34710176\n",
      "Iteration 4, loss = 0.28150914\n",
      "Iteration 4, loss = 0.13022239\n",
      "Iteration 4, loss = 0.33120153\n",
      "Iteration 5, loss = 0.32551717\n",
      "Iteration 5, loss = 0.38717409\n",
      "Iteration 5, loss = 0.37211736\n",
      "Iteration 5, loss = 0.33897692\n",
      "Iteration 5, loss = 0.27065611\n",
      "Iteration 5, loss = 0.10382896\n",
      "Iteration 5, loss = 0.32932476\n",
      "Iteration 6, loss = 0.32422358\n",
      "Iteration 6, loss = 0.38584777\n",
      "Iteration 6, loss = 0.36741208\n",
      "Iteration 6, loss = 0.33252348\n",
      "Iteration 6, loss = 0.26175354\n",
      "Iteration 6, loss = 0.08377623\n",
      "Iteration 6, loss = 0.32777627\n",
      "Iteration 7, loss = 0.38476040\n",
      "Iteration 7, loss = 0.32305892\n",
      "Iteration 7, loss = 0.36349946\n",
      "Iteration 7, loss = 0.32687109\n",
      "Iteration 7, loss = 0.25426126\n",
      "Iteration 7, loss = 0.06769372\n",
      "Iteration 7, loss = 0.32637323\n",
      "Iteration 8, loss = 0.38369531\n",
      "Iteration 8, loss = 0.32182681\n",
      "Iteration 8, loss = 0.32185599\n",
      "Iteration 8, loss = 0.36031385\n",
      "Iteration 8, loss = 0.24792524\n",
      "Iteration 8, loss = 0.05515195\n",
      "Iteration 8, loss = 0.32509211\n",
      "Iteration 9, loss = 0.38261126\n",
      "Iteration 9, loss = 0.32080825\n",
      "Iteration 9, loss = 0.35764074\n",
      "Iteration 9, loss = 0.31713959\n",
      "Iteration 9, loss = 0.24276677\n",
      "Iteration 9, loss = 0.04528646\n",
      "Iteration 9, loss = 0.32390565\n",
      "Iteration 10, loss = 0.38164246\n",
      "Iteration 10, loss = 0.31978836\n",
      "Iteration 10, loss = 0.35551647\n",
      "Iteration 10, loss = 0.31281707\n",
      "Iteration 10, loss = 0.23835101\n",
      "Iteration 10, loss = 0.03850430\n",
      "Iteration 10, loss = 0.32273868\n",
      "Iteration 11, loss = 0.38044578\n",
      "Iteration 11, loss = 0.31886528\n",
      "Iteration 11, loss = 0.35368241\n",
      "Iteration 11, loss = 0.30869411\n",
      "Iteration 11, loss = 0.23433931\n",
      "Iteration 11, loss = 0.03295935\n",
      "Iteration 11, loss = 0.32197142\n",
      "Iteration 12, loss = 0.37945349\n",
      "Iteration 12, loss = 0.31802746\n",
      "Iteration 12, loss = 0.35199585\n",
      "Iteration 12, loss = 0.23098601\n",
      "Iteration 12, loss = 0.30495420\n",
      "Iteration 12, loss = 0.02866981\n",
      "Iteration 12, loss = 0.32110360\n",
      "Iteration 13, loss = 0.37839875\n",
      "Iteration 13, loss = 0.35029781\n",
      "Iteration 13, loss = 0.31716727\n",
      "Iteration 13, loss = 0.22792615\n",
      "Iteration 13, loss = 0.32023957\n",
      "Iteration 13, loss = 0.02517422\n",
      "Iteration 14, loss = 0.37754240\n",
      "Iteration 14, loss = 0.31634952\n",
      "Iteration 14, loss = 0.34902860\n",
      "Iteration 13, loss = 0.30161231\n",
      "Iteration 14, loss = 0.31957710\n",
      "Iteration 14, loss = 0.02245343\n",
      "Iteration 15, loss = 0.37659533\n",
      "Iteration 15, loss = 0.34767146\n",
      "Iteration 15, loss = 0.31561709\n",
      "Iteration 14, loss = 0.22513151\n",
      "Iteration 15, loss = 0.31884285\n",
      "Iteration 15, loss = 0.02027232\n",
      "Iteration 16, loss = 0.37579076\n",
      "Iteration 14, loss = 0.29874938\n",
      "Iteration 16, loss = 0.34652327\n",
      "Iteration 16, loss = 0.31488076\n",
      "Iteration 16, loss = 0.31832985\n",
      "Iteration 16, loss = 0.01827610\n",
      "Iteration 17, loss = 0.37492445\n",
      "Iteration 17, loss = 0.34534209\n",
      "Iteration 17, loss = 0.31422482\n",
      "Iteration 15, loss = 0.22277571\n",
      "Iteration 15, loss = 0.29603232\n",
      "Iteration 17, loss = 0.31767182\n",
      "Iteration 18, loss = 0.37422584\n",
      "Iteration 17, loss = 0.01669489\n",
      "Iteration 18, loss = 0.34420726\n",
      "Iteration 18, loss = 0.31353882\n",
      "Iteration 18, loss = 0.31727389\n",
      "Iteration 18, loss = 0.01546597\n",
      "Iteration 19, loss = 0.37329216\n",
      "Iteration 19, loss = 0.34324635\n",
      "Iteration 19, loss = 0.31294327\n",
      "Iteration 16, loss = 0.29366759\n",
      "Iteration 19, loss = 0.01432423\n",
      "Iteration 19, loss = 0.31663415\n",
      "Iteration 16, loss = 0.22051585\n",
      "Iteration 20, loss = 0.34227573\n",
      "Iteration 20, loss = 0.31216709\n",
      "Iteration 20, loss = 0.37259204\n",
      "Iteration 20, loss = 0.01330988\n",
      "Iteration 20, loss = 0.31623171\n",
      "Iteration 21, loss = 0.34143553\n",
      "Iteration 21, loss = 0.31170929\n",
      "Iteration 17, loss = 0.29123521\n",
      "Iteration 21, loss = 0.01235197\n",
      "Iteration 22, loss = 0.34037594\n",
      "Iteration 21, loss = 0.31573403\n",
      "Iteration 21, loss = 0.37182765\n",
      "Iteration 22, loss = 0.31107577\n",
      "Iteration 17, loss = 0.21839252\n",
      "Iteration 22, loss = 0.01158628\n",
      "Iteration 23, loss = 0.33955562\n",
      "Iteration 22, loss = 0.31525682\n",
      "Iteration 18, loss = 0.28908798\n",
      "Iteration 23, loss = 0.31055945\n",
      "Iteration 22, loss = 0.37101840\n",
      "Iteration 23, loss = 0.31480084\n",
      "Iteration 23, loss = 0.01084563\n",
      "Iteration 24, loss = 0.33891183\n",
      "Iteration 24, loss = 0.30996952\n",
      "Iteration 18, loss = 0.21656993\n",
      "Iteration 19, loss = 0.28691179\n",
      "Iteration 25, loss = 0.33808249\n",
      "Iteration 24, loss = 0.31440505\n",
      "Iteration 24, loss = 0.01028796\n",
      "Iteration 25, loss = 0.30952205\n",
      "Iteration 23, loss = 0.37017799\n",
      "Iteration 26, loss = 0.33738900\n",
      "Iteration 25, loss = 0.00979775\n",
      "Iteration 25, loss = 0.31394255\n",
      "Iteration 26, loss = 0.30905778\n",
      "Iteration 20, loss = 0.28487827\n",
      "Iteration 27, loss = 0.33658315\n",
      "Iteration 26, loss = 0.00941721\n",
      "Iteration 26, loss = 0.31363154\n",
      "Iteration 19, loss = 0.21472177\n",
      "Iteration 27, loss = 0.30850668\n",
      "Iteration 24, loss = 0.36939151\n",
      "Iteration 28, loss = 0.33601562\n",
      "Iteration 27, loss = 0.00902155\n",
      "Iteration 27, loss = 0.31331441\n",
      "Iteration 21, loss = 0.28278868\n",
      "Iteration 28, loss = 0.30811475\n",
      "Iteration 29, loss = 0.33531813\n",
      "Iteration 28, loss = 0.00867139\n",
      "Iteration 28, loss = 0.31276446\n",
      "Iteration 29, loss = 0.30760595\n",
      "Iteration 25, loss = 0.36876213\n",
      "Iteration 20, loss = 0.21314843\n",
      "Iteration 30, loss = 0.33470582\n",
      "Iteration 22, loss = 0.28110934\n",
      "Iteration 29, loss = 0.00836637\n",
      "Iteration 29, loss = 0.31249288\n",
      "Iteration 30, loss = 0.30711856\n",
      "Iteration 31, loss = 0.33396274\n",
      "Iteration 30, loss = 0.00802339\n",
      "Iteration 30, loss = 0.31213781\n",
      "Iteration 31, loss = 0.30680374\n",
      "Iteration 23, loss = 0.27953436\n",
      "Iteration 32, loss = 0.33363583\n",
      "Iteration 26, loss = 0.36813586\n",
      "Iteration 31, loss = 0.00776966\n",
      "Iteration 31, loss = 0.31179365\n",
      "Iteration 21, loss = 0.21166145\n",
      "Iteration 32, loss = 0.30635896\n",
      "Iteration 33, loss = 0.33300098\n",
      "Iteration 32, loss = 0.00755835\n",
      "Iteration 32, loss = 0.31150502\n",
      "Iteration 33, loss = 0.30593957\n",
      "Iteration 24, loss = 0.27778600\n",
      "Iteration 34, loss = 0.33238298\n",
      "Iteration 33, loss = 0.00731703\n",
      "Iteration 33, loss = 0.31117723\n",
      "Iteration 34, loss = 0.30552474\n",
      "Iteration 27, loss = 0.36734756\n",
      "Iteration 22, loss = 0.21018052\n",
      "Iteration 35, loss = 0.33203078\n",
      "Iteration 34, loss = 0.00723256\n",
      "Iteration 25, loss = 0.27644706\n",
      "Iteration 34, loss = 0.31079121\n",
      "Iteration 35, loss = 0.30508457\n",
      "Iteration 36, loss = 0.33134973\n",
      "Iteration 35, loss = 0.00703013\n",
      "Iteration 35, loss = 0.31043422\n",
      "Iteration 36, loss = 0.30483082\n",
      "Iteration 37, loss = 0.33085058\n",
      "Iteration 26, loss = 0.27501159\n",
      "Iteration 28, loss = 0.36663595\n",
      "Iteration 36, loss = 0.00703832\n",
      "Iteration 23, loss = 0.20880789\n",
      "Iteration 36, loss = 0.31027644\n",
      "Iteration 37, loss = 0.30432144\n",
      "Iteration 38, loss = 0.33034407\n",
      "Iteration 37, loss = 0.00679759\n",
      "Iteration 37, loss = 0.30996507\n",
      "Iteration 38, loss = 0.30392359\n",
      "Iteration 27, loss = 0.27370258\n",
      "Iteration 39, loss = 0.33002807\n",
      "Iteration 38, loss = 0.00666860\n",
      "Iteration 38, loss = 0.30962029\n",
      "Iteration 39, loss = 0.30367538\n",
      "Iteration 29, loss = 0.36596250\n",
      "Iteration 24, loss = 0.20740905\n",
      "Iteration 40, loss = 0.32945291\n",
      "Iteration 39, loss = 0.00665838\n",
      "Iteration 39, loss = 0.30934965\n",
      "Iteration 28, loss = 0.27247814\n",
      "Iteration 40, loss = 0.30339156\n",
      "Iteration 41, loss = 0.32901937\n",
      "Iteration 40, loss = 0.00656295\n",
      "Iteration 40, loss = 0.30929113\n",
      "Iteration 41, loss = 0.30292430\n",
      "Iteration 42, loss = 0.32852176\n",
      "Iteration 30, loss = 0.36527599\n",
      "Iteration 25, loss = 0.20629714\n",
      "Iteration 29, loss = 0.27127953\n",
      "Iteration 41, loss = 0.00641663\n",
      "Iteration 41, loss = 0.30885954\n",
      "Iteration 42, loss = 0.30259479\n",
      "Iteration 43, loss = 0.32815803\n",
      "Iteration 42, loss = 0.00636743\n",
      "Iteration 42, loss = 0.30868301\n",
      "Iteration 43, loss = 0.30223494\n",
      "Iteration 44, loss = 0.32774099\n",
      "Iteration 30, loss = 0.27012517\n",
      "Iteration 43, loss = 0.00628518\n",
      "Iteration 43, loss = 0.30854143\n",
      "Iteration 31, loss = 0.36463791\n",
      "Iteration 44, loss = 0.30192213\n",
      "Iteration 26, loss = 0.20514001\n",
      "Iteration 45, loss = 0.32723072\n",
      "Iteration 44, loss = 0.00623812\n",
      "Iteration 44, loss = 0.30821522\n",
      "Iteration 45, loss = 0.30154064\n",
      "Iteration 31, loss = 0.26904080\n",
      "Iteration 46, loss = 0.32684311\n",
      "Iteration 45, loss = 0.00615660\n",
      "Iteration 45, loss = 0.30798272\n",
      "Iteration 46, loss = 0.30134178\n",
      "Iteration 47, loss = 0.32647777\n",
      "Iteration 32, loss = 0.36406665\n",
      "Iteration 27, loss = 0.20416983\n",
      "Iteration 46, loss = 0.00611551\n",
      "Iteration 46, loss = 0.30774228\n",
      "Iteration 32, loss = 0.26791985\n",
      "Iteration 47, loss = 0.30111580\n",
      "Iteration 48, loss = 0.32614085\n",
      "Iteration 47, loss = 0.00600650\n",
      "Iteration 47, loss = 0.30776540\n",
      "Iteration 48, loss = 0.30071757\n",
      "Iteration 49, loss = 0.32587251\n",
      "Iteration 33, loss = 0.26706180\n",
      "Iteration 48, loss = 0.30747286\n",
      "Iteration 48, loss = 0.00603335\n",
      "Iteration 33, loss = 0.36346910\n",
      "Iteration 28, loss = 0.20310347\n",
      "Iteration 49, loss = 0.30047535\n",
      "Iteration 50, loss = 0.32544384\n",
      "Iteration 49, loss = 0.00593653\n",
      "Iteration 49, loss = 0.30729395\n",
      "Iteration 50, loss = 0.30015313\n",
      "Iteration 51, loss = 0.32524494\n",
      "Iteration 34, loss = 0.26614630\n",
      "Iteration 50, loss = 0.00587753\n",
      "Iteration 50, loss = 0.30702096\n",
      "Iteration 51, loss = 0.29973783\n",
      "Iteration 52, loss = 0.32464123\n",
      "Iteration 29, loss = 0.20201895\n",
      "Iteration 34, loss = 0.36296269\n",
      "Iteration 51, loss = 0.00585392\n",
      "Iteration 51, loss = 0.30680834\n",
      "Iteration 53, loss = 0.32444015\n",
      "Iteration 35, loss = 0.26527443\n",
      "Iteration 52, loss = 0.29955249\n",
      "Iteration 52, loss = 0.00578883\n",
      "Iteration 52, loss = 0.30667176\n",
      "Iteration 54, loss = 0.32395696\n",
      "Iteration 53, loss = 0.29944156\n",
      "Iteration 36, loss = 0.26443313\n",
      "Iteration 53, loss = 0.00582136\n",
      "Iteration 30, loss = 0.20105901\n",
      "Iteration 35, loss = 0.36247508\n",
      "Iteration 53, loss = 0.30641648\n",
      "Iteration 55, loss = 0.32355540\n",
      "Iteration 54, loss = 0.29904508\n",
      "Iteration 54, loss = 0.30618061\n",
      "Iteration 56, loss = 0.32342973\n",
      "Iteration 54, loss = 0.00571334\n",
      "Iteration 55, loss = 0.29862362\n",
      "Iteration 37, loss = 0.26352900\n",
      "Iteration 55, loss = 0.30607150\n",
      "Iteration 57, loss = 0.32319742\n",
      "Iteration 56, loss = 0.29841267\n",
      "Iteration 31, loss = 0.20013218\n",
      "Iteration 36, loss = 0.36195493\n",
      "Iteration 55, loss = 0.00576560\n",
      "Iteration 56, loss = 0.30588573\n",
      "Iteration 58, loss = 0.32273476\n",
      "Iteration 57, loss = 0.29817598\n",
      "Iteration 38, loss = 0.26283750\n",
      "Iteration 57, loss = 0.30572533\n",
      "Iteration 59, loss = 0.32250994\n",
      "Iteration 58, loss = 0.29809575\n",
      "Iteration 56, loss = 0.00571373\n",
      "Iteration 32, loss = 0.19948461\n",
      "Iteration 37, loss = 0.36145867\n",
      "Iteration 58, loss = 0.30562958\n",
      "Iteration 39, loss = 0.26201551\n",
      "Iteration 60, loss = 0.32226435\n",
      "Iteration 59, loss = 0.29789238\n",
      "Iteration 59, loss = 0.30534123\n",
      "Iteration 61, loss = 0.32171078\n",
      "Iteration 57, loss = 0.00567821\n",
      "Iteration 60, loss = 0.29740869\n",
      "Iteration 40, loss = 0.26112215\n",
      "Iteration 62, loss = 0.32153835\n",
      "Iteration 60, loss = 0.30526923\n",
      "Iteration 33, loss = 0.19846561\n",
      "Iteration 61, loss = 0.29731395\n",
      "Iteration 38, loss = 0.36117670\n",
      "Iteration 58, loss = 0.00564290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 63, loss = 0.32149270\n",
      "Iteration 61, loss = 0.30496235\n",
      "Iteration 62, loss = 0.29709420\n",
      "Iteration 41, loss = 0.26058095\n",
      "Iteration 64, loss = 0.32109956\n",
      "Iteration 62, loss = 0.30486309\n",
      "Iteration 63, loss = 0.29698697\n",
      "Iteration 34, loss = 0.19775191\n",
      "Iteration 39, loss = 0.36070506\n",
      "Iteration 65, loss = 0.32070840\n",
      "Iteration 63, loss = 0.30459284\n",
      "Iteration 64, loss = 0.29656187\n",
      "Iteration 42, loss = 0.25990237\n",
      "Iteration 66, loss = 0.32038700\n",
      "Iteration 64, loss = 0.30447170\n",
      "Iteration 65, loss = 0.29644399\n",
      "Iteration 67, loss = 0.32014736\n",
      "Iteration 43, loss = 0.25927301\n",
      "Iteration 66, loss = 0.29632376\n",
      "Iteration 35, loss = 0.19687630\n",
      "Iteration 65, loss = 0.30432881\n",
      "Iteration 40, loss = 0.36010371\n",
      "Iteration 68, loss = 0.31989725\n",
      "Iteration 67, loss = 0.29606355\n",
      "Iteration 66, loss = 0.30418643\n",
      "Iteration 44, loss = 0.25864510\n",
      "Iteration 69, loss = 0.31993673\n",
      "Iteration 68, loss = 0.29568741\n",
      "Iteration 67, loss = 0.30406795\n",
      "Iteration 36, loss = 0.19610624\n",
      "Iteration 41, loss = 0.35968110\n",
      "Iteration 70, loss = 0.31947314\n",
      "Iteration 69, loss = 0.29560512\n",
      "Iteration 68, loss = 0.30396837\n",
      "Iteration 45, loss = 0.25793381\n",
      "Iteration 71, loss = 0.31934867\n",
      "Iteration 70, loss = 0.29539906\n",
      "Iteration 69, loss = 0.30362728\n",
      "Iteration 72, loss = 0.31891892\n",
      "Iteration 71, loss = 0.29517870\n",
      "Iteration 37, loss = 0.19533022\n",
      "Iteration 46, loss = 0.25754834\n",
      "Iteration 70, loss = 0.30360752\n",
      "Iteration 42, loss = 0.35937025\n",
      "Iteration 73, loss = 0.31861662\n",
      "Iteration 72, loss = 0.29515427\n",
      "Iteration 71, loss = 0.30354572\n",
      "Iteration 74, loss = 0.31852563\n",
      "Iteration 47, loss = 0.25690830\n",
      "Iteration 73, loss = 0.29506086\n",
      "Iteration 72, loss = 0.30332449\n",
      "Iteration 38, loss = 0.19465731\n",
      "Iteration 75, loss = 0.31818651\n",
      "Iteration 43, loss = 0.35885126\n",
      "Iteration 74, loss = 0.29470418\n",
      "Iteration 73, loss = 0.30322258\n",
      "Iteration 48, loss = 0.25648353\n",
      "Iteration 76, loss = 0.31791199\n",
      "Iteration 75, loss = 0.29453151\n",
      "Iteration 74, loss = 0.30311840\n",
      "Iteration 77, loss = 0.31771480\n",
      "Iteration 76, loss = 0.29433384\n",
      "Iteration 39, loss = 0.19399521\n",
      "Iteration 44, loss = 0.35850660\n",
      "Iteration 75, loss = 0.30277772\n",
      "Iteration 49, loss = 0.25591296\n",
      "Iteration 78, loss = 0.31767270\n",
      "Iteration 77, loss = 0.29433146\n",
      "Iteration 76, loss = 0.30265239\n",
      "Iteration 79, loss = 0.31724309\n",
      "Iteration 78, loss = 0.29397255\n",
      "Iteration 50, loss = 0.25548288\n",
      "Iteration 77, loss = 0.30264178\n",
      "Iteration 40, loss = 0.19329409\n",
      "Iteration 45, loss = 0.35833650\n",
      "Iteration 80, loss = 0.31713073\n",
      "Iteration 79, loss = 0.29383551\n",
      "Iteration 78, loss = 0.30261151\n",
      "Iteration 81, loss = 0.31696313\n",
      "Iteration 80, loss = 0.29382554\n",
      "Iteration 51, loss = 0.25489310\n",
      "Iteration 79, loss = 0.30250350\n",
      "Iteration 82, loss = 0.31643893\n",
      "Iteration 81, loss = 0.29357065\n",
      "Iteration 41, loss = 0.19285121\n",
      "Iteration 46, loss = 0.35800138\n",
      "Iteration 80, loss = 0.30218053\n",
      "Iteration 83, loss = 0.31651513\n",
      "Iteration 52, loss = 0.25436774\n",
      "Iteration 82, loss = 0.29319452\n",
      "Iteration 81, loss = 0.30222590\n",
      "Iteration 84, loss = 0.31629843\n",
      "Iteration 83, loss = 0.29326312\n",
      "Iteration 42, loss = 0.19227270\n",
      "Iteration 53, loss = 0.25394895\n",
      "Iteration 82, loss = 0.30189118\n",
      "Iteration 47, loss = 0.35759950\n",
      "Iteration 85, loss = 0.31602170\n",
      "Iteration 84, loss = 0.29290603\n",
      "Iteration 83, loss = 0.30171174\n",
      "Iteration 86, loss = 0.31597526\n",
      "Iteration 85, loss = 0.29296769\n",
      "Iteration 54, loss = 0.25346202\n",
      "Iteration 84, loss = 0.30168125\n",
      "Iteration 87, loss = 0.31555609\n",
      "Iteration 86, loss = 0.29272750\n",
      "Iteration 43, loss = 0.19159349\n",
      "Iteration 48, loss = 0.35729626\n",
      "Iteration 85, loss = 0.30149109\n",
      "Iteration 88, loss = 0.31554511\n",
      "Iteration 87, loss = 0.29260473\n",
      "Iteration 55, loss = 0.25300313\n",
      "Iteration 86, loss = 0.30138024\n",
      "Iteration 89, loss = 0.31510933\n",
      "Iteration 88, loss = 0.29262874\n",
      "Iteration 44, loss = 0.19111062\n",
      "Iteration 87, loss = 0.30135650\n",
      "Iteration 90, loss = 0.31522408\n",
      "Iteration 56, loss = 0.25254213\n",
      "Iteration 49, loss = 0.35678952\n",
      "Iteration 89, loss = 0.29241944\n",
      "Iteration 91, loss = 0.31485673\n",
      "Iteration 88, loss = 0.30115088\n",
      "Iteration 90, loss = 0.29219991\n",
      "Iteration 92, loss = 0.31469116\n",
      "Iteration 57, loss = 0.25234800\n",
      "Iteration 89, loss = 0.30112878\n",
      "Iteration 91, loss = 0.29217809\n",
      "Iteration 45, loss = 0.19052657\n",
      "Iteration 93, loss = 0.31455242\n",
      "Iteration 90, loss = 0.30093650\n",
      "Iteration 50, loss = 0.35677798\n",
      "Iteration 92, loss = 0.29181201\n",
      "Iteration 58, loss = 0.25171498\n",
      "Iteration 94, loss = 0.31427416\n",
      "Iteration 91, loss = 0.30090603\n",
      "Iteration 93, loss = 0.29194156\n",
      "Iteration 95, loss = 0.31442099\n",
      "Iteration 46, loss = 0.19007648\n",
      "Iteration 92, loss = 0.30057067\n",
      "Iteration 59, loss = 0.25144349\n",
      "Iteration 94, loss = 0.29162411\n",
      "Iteration 96, loss = 0.31387596\n",
      "Iteration 93, loss = 0.30066992\n",
      "Iteration 95, loss = 0.29145678\n",
      "Iteration 51, loss = 0.35649750\n",
      "Iteration 97, loss = 0.31373443\n",
      "Iteration 94, loss = 0.30060714\n",
      "Iteration 60, loss = 0.25114485\n",
      "Iteration 47, loss = 0.18946867\n",
      "Iteration 96, loss = 0.29156912\n",
      "Iteration 98, loss = 0.31368532\n",
      "Iteration 95, loss = 0.30043765\n",
      "Iteration 97, loss = 0.29153772\n",
      "Iteration 99, loss = 0.31346990\n",
      "Iteration 61, loss = 0.25087365\n",
      "Iteration 96, loss = 0.30035107\n",
      "Iteration 98, loss = 0.29119817\n",
      "Iteration 52, loss = 0.35616298\n",
      "Iteration 100, loss = 0.31315934\n",
      "Iteration 48, loss = 0.18882695\n",
      "Iteration 97, loss = 0.30020442\n",
      "Iteration 99, loss = 0.29105318\n",
      "Iteration 62, loss = 0.25019367\n",
      "Iteration 101, loss = 0.31316738\n",
      "Iteration 98, loss = 0.30024401\n",
      "Iteration 100, loss = 0.29085652\n",
      "Iteration 102, loss = 0.31288834\n",
      "Iteration 99, loss = 0.29998411\n",
      "Iteration 49, loss = 0.18846629\n",
      "Iteration 101, loss = 0.29092087\n",
      "Iteration 63, loss = 0.24988746\n",
      "Iteration 103, loss = 0.31276281\n",
      "Iteration 53, loss = 0.35602593\n",
      "Iteration 100, loss = 0.29997230\n",
      "Iteration 102, loss = 0.29063997\n",
      "Iteration 104, loss = 0.31272478\n",
      "Iteration 101, loss = 0.29981089\n",
      "Iteration 64, loss = 0.24955448\n",
      "Iteration 103, loss = 0.29081562\n",
      "Iteration 50, loss = 0.18798396\n",
      "Iteration 105, loss = 0.31236147\n",
      "Iteration 102, loss = 0.29970539\n",
      "Iteration 104, loss = 0.29034551\n",
      "Iteration 106, loss = 0.31241226\n",
      "Iteration 65, loss = 0.24923376\n",
      "Iteration 54, loss = 0.35553877\n",
      "Iteration 103, loss = 0.29970519\n",
      "Iteration 105, loss = 0.29030648\n",
      "Iteration 107, loss = 0.31204213\n",
      "Iteration 104, loss = 0.29951376\n",
      "Iteration 51, loss = 0.18744295\n",
      "Iteration 106, loss = 0.29041569\n",
      "Iteration 108, loss = 0.31212396\n",
      "Iteration 66, loss = 0.24875951\n",
      "Iteration 105, loss = 0.29939494\n",
      "Iteration 107, loss = 0.29032476\n",
      "Iteration 109, loss = 0.31197749\n",
      "Iteration 55, loss = 0.35534448\n",
      "Iteration 106, loss = 0.29937441\n",
      "Iteration 67, loss = 0.24865092\n",
      "Iteration 108, loss = 0.29005460\n",
      "Iteration 52, loss = 0.18693613\n",
      "Iteration 110, loss = 0.31163632\n",
      "Iteration 107, loss = 0.29925384\n",
      "Iteration 109, loss = 0.28998434\n",
      "Iteration 111, loss = 0.31164898\n",
      "Iteration 108, loss = 0.29931060\n",
      "Iteration 68, loss = 0.24831686\n",
      "Iteration 110, loss = 0.28997358\n",
      "Iteration 112, loss = 0.31134020\n",
      "Iteration 53, loss = 0.18661398\n",
      "Iteration 109, loss = 0.29902501\n",
      "Iteration 56, loss = 0.35497622\n",
      "Iteration 111, loss = 0.28954467\n",
      "Iteration 113, loss = 0.31113195\n",
      "Iteration 69, loss = 0.24804809\n",
      "Iteration 110, loss = 0.29903661\n",
      "Iteration 112, loss = 0.28951495\n",
      "Iteration 114, loss = 0.31121231\n",
      "Iteration 111, loss = 0.29894333\n",
      "Iteration 113, loss = 0.28960279\n",
      "Iteration 115, loss = 0.31104642\n",
      "Iteration 54, loss = 0.18621857\n",
      "Iteration 70, loss = 0.24774119\n",
      "Iteration 112, loss = 0.29894395\n",
      "Iteration 114, loss = 0.28957063\n",
      "Iteration 116, loss = 0.31073858\n",
      "Iteration 57, loss = 0.35507379\n",
      "Iteration 113, loss = 0.29883222\n",
      "Iteration 115, loss = 0.28943133\n",
      "Iteration 117, loss = 0.31085428\n",
      "Iteration 71, loss = 0.24748145\n",
      "Iteration 55, loss = 0.18571316\n",
      "Iteration 114, loss = 0.29873547\n",
      "Iteration 116, loss = 0.28927825\n",
      "Iteration 118, loss = 0.31055525\n",
      "Iteration 72, loss = 0.24707671\n",
      "Iteration 115, loss = 0.29858452\n",
      "Iteration 117, loss = 0.28909620\n",
      "Iteration 119, loss = 0.31040052\n",
      "Iteration 58, loss = 0.35463344\n",
      "Iteration 116, loss = 0.29855101\n",
      "Iteration 118, loss = 0.28909364\n",
      "Iteration 120, loss = 0.31039718\n",
      "Iteration 56, loss = 0.18521730\n",
      "Iteration 73, loss = 0.24679845\n",
      "Iteration 121, loss = 0.30997255\n",
      "Iteration 117, loss = 0.29850847\n",
      "Iteration 119, loss = 0.28907597\n",
      "Iteration 122, loss = 0.30992001\n",
      "Iteration 118, loss = 0.29855828\n",
      "Iteration 120, loss = 0.28897974\n",
      "Iteration 74, loss = 0.24656606\n",
      "Iteration 57, loss = 0.18488418\n",
      "Iteration 59, loss = 0.35461056\n",
      "Iteration 123, loss = 0.30976333\n",
      "Iteration 119, loss = 0.29828782\n",
      "Iteration 121, loss = 0.28888225\n",
      "Iteration 124, loss = 0.30969790\n",
      "Iteration 120, loss = 0.29845276\n",
      "Iteration 75, loss = 0.24610225\n",
      "Iteration 122, loss = 0.28888708\n",
      "Iteration 58, loss = 0.18449950\n",
      "Iteration 125, loss = 0.30938446\n",
      "Iteration 121, loss = 0.29826815\n",
      "Iteration 123, loss = 0.28871695\n",
      "Iteration 76, loss = 0.24599483\n",
      "Iteration 60, loss = 0.35425544\n",
      "Iteration 126, loss = 0.30969164\n",
      "Iteration 122, loss = 0.29821333\n",
      "Iteration 124, loss = 0.28866032\n",
      "Iteration 127, loss = 0.30930962\n",
      "Iteration 125, loss = 0.28856775\n",
      "Iteration 123, loss = 0.29802156\n",
      "Iteration 59, loss = 0.18418050\n",
      "Iteration 77, loss = 0.24571161\n",
      "Iteration 128, loss = 0.30915633\n",
      "Iteration 126, loss = 0.28858710\n",
      "Iteration 124, loss = 0.29811934\n",
      "Iteration 129, loss = 0.30916120\n",
      "Iteration 61, loss = 0.35399382\n",
      "Iteration 127, loss = 0.28826374\n",
      "Iteration 125, loss = 0.29789238\n",
      "Iteration 78, loss = 0.24560730\n",
      "Iteration 60, loss = 0.18367708\n",
      "Iteration 130, loss = 0.30897017\n",
      "Iteration 128, loss = 0.28825659\n",
      "Iteration 126, loss = 0.29782228\n",
      "Iteration 131, loss = 0.30891926\n",
      "Iteration 129, loss = 0.28822110\n",
      "Iteration 127, loss = 0.29786015\n",
      "Iteration 79, loss = 0.24524009\n",
      "Iteration 132, loss = 0.30860379\n",
      "Iteration 128, loss = 0.29756310\n",
      "Iteration 130, loss = 0.28831809\n",
      "Iteration 61, loss = 0.18327695\n",
      "Iteration 62, loss = 0.35381441\n",
      "Iteration 80, loss = 0.24500004\n",
      "Iteration 133, loss = 0.30880044\n",
      "Iteration 131, loss = 0.28804251\n",
      "Iteration 129, loss = 0.29768284\n",
      "Iteration 134, loss = 0.30850141\n",
      "Iteration 132, loss = 0.28801500\n",
      "Iteration 130, loss = 0.29748829\n",
      "Iteration 62, loss = 0.18307069\n",
      "Iteration 81, loss = 0.24468345\n",
      "Iteration 135, loss = 0.30840393\n",
      "Iteration 133, loss = 0.28818770\n",
      "Iteration 131, loss = 0.29742413\n",
      "Iteration 63, loss = 0.35357531\n",
      "Iteration 136, loss = 0.30833166\n",
      "Iteration 134, loss = 0.28801788\n",
      "Iteration 132, loss = 0.29755472\n",
      "Iteration 82, loss = 0.24441707\n",
      "Iteration 137, loss = 0.30808270\n",
      "Iteration 63, loss = 0.18253500\n",
      "Iteration 135, loss = 0.28794687\n",
      "Iteration 133, loss = 0.29741233\n",
      "Iteration 138, loss = 0.30811397\n",
      "Iteration 136, loss = 0.28783549\n",
      "Iteration 83, loss = 0.24422639\n",
      "Iteration 134, loss = 0.29735002\n",
      "Iteration 64, loss = 0.35335422\n",
      "Iteration 139, loss = 0.30806947\n",
      "Iteration 137, loss = 0.28778237\n",
      "Iteration 135, loss = 0.29717632\n",
      "Iteration 64, loss = 0.18233139\n",
      "Iteration 140, loss = 0.30785732\n",
      "Iteration 84, loss = 0.24407318\n",
      "Iteration 138, loss = 0.28752715\n",
      "Iteration 136, loss = 0.29713707\n",
      "Iteration 141, loss = 0.30757108\n",
      "Iteration 139, loss = 0.28754552\n",
      "Iteration 137, loss = 0.29716530\n",
      "Iteration 85, loss = 0.24395759\n",
      "Iteration 142, loss = 0.30761727\n",
      "Iteration 65, loss = 0.18189376\n",
      "Iteration 65, loss = 0.35334862\n",
      "Iteration 140, loss = 0.28759196\n",
      "Iteration 138, loss = 0.29716308\n",
      "Iteration 143, loss = 0.30748570\n",
      "Iteration 141, loss = 0.28760250\n",
      "Iteration 139, loss = 0.29697307\n",
      "Iteration 86, loss = 0.24379159\n",
      "Iteration 144, loss = 0.30753446\n",
      "Iteration 142, loss = 0.28740438\n",
      "Iteration 66, loss = 0.18154284\n",
      "Iteration 140, loss = 0.29680945\n",
      "Iteration 145, loss = 0.30732719\n",
      "Iteration 143, loss = 0.28740037\n",
      "Iteration 66, loss = 0.35308776\n",
      "Iteration 87, loss = 0.24353343\n",
      "Iteration 141, loss = 0.29675580\n",
      "Iteration 146, loss = 0.30728636\n",
      "Iteration 144, loss = 0.28726054\n",
      "Iteration 142, loss = 0.29693487\n",
      "Iteration 147, loss = 0.30705647\n",
      "Iteration 67, loss = 0.18111361\n",
      "Iteration 88, loss = 0.24332611\n",
      "Iteration 145, loss = 0.28729935\n",
      "Iteration 143, loss = 0.29670296\n",
      "Iteration 148, loss = 0.30711675\n",
      "Iteration 146, loss = 0.28722709\n",
      "Iteration 144, loss = 0.29674499\n",
      "Iteration 67, loss = 0.35296005\n",
      "Iteration 89, loss = 0.24302957\n",
      "Iteration 149, loss = 0.30694095\n",
      "Iteration 147, loss = 0.28712807\n",
      "Iteration 68, loss = 0.18087248\n",
      "Iteration 145, loss = 0.29645926\n",
      "Iteration 150, loss = 0.30703889\n",
      "Iteration 148, loss = 0.28710810\n",
      "Iteration 90, loss = 0.24290534\n",
      "Iteration 146, loss = 0.29659475\n",
      "Iteration 151, loss = 0.30686683\n",
      "Iteration 149, loss = 0.28689611\n",
      "Iteration 147, loss = 0.29657600\n",
      "Iteration 68, loss = 0.35280367\n",
      "Iteration 152, loss = 0.30650759\n",
      "Iteration 69, loss = 0.18067435\n",
      "Iteration 150, loss = 0.28709471\n",
      "Iteration 91, loss = 0.24265842\n",
      "Iteration 148, loss = 0.29636992\n",
      "Iteration 153, loss = 0.30652222\n",
      "Iteration 151, loss = 0.28677464\n",
      "Iteration 149, loss = 0.29629938\n",
      "Iteration 154, loss = 0.30652535\n",
      "Iteration 92, loss = 0.24246232\n",
      "Iteration 152, loss = 0.28669303\n",
      "Iteration 70, loss = 0.18018420\n",
      "Iteration 150, loss = 0.29634633\n",
      "Iteration 155, loss = 0.30641229\n",
      "Iteration 69, loss = 0.35254259\n",
      "Iteration 153, loss = 0.28702989\n",
      "Iteration 151, loss = 0.29638418\n",
      "Iteration 93, loss = 0.24237441\n",
      "Iteration 156, loss = 0.30656876\n",
      "Iteration 154, loss = 0.28665922\n",
      "Iteration 157, loss = 0.30640711\n",
      "Iteration 152, loss = 0.29622281\n",
      "Iteration 71, loss = 0.17994731\n",
      "Iteration 155, loss = 0.28685317\n",
      "Iteration 94, loss = 0.24222237\n",
      "Iteration 158, loss = 0.30612735\n",
      "Iteration 153, loss = 0.29625013\n",
      "Iteration 156, loss = 0.28663680\n",
      "Iteration 70, loss = 0.35261391\n",
      "Iteration 159, loss = 0.30595178\n",
      "Iteration 154, loss = 0.29618125\n",
      "Iteration 157, loss = 0.28659513\n",
      "Iteration 95, loss = 0.24205316\n",
      "Iteration 72, loss = 0.17944043\n",
      "Iteration 160, loss = 0.30576926\n",
      "Iteration 155, loss = 0.29603526\n",
      "Iteration 158, loss = 0.28651744\n",
      "Iteration 161, loss = 0.30588622\n",
      "Iteration 156, loss = 0.29621144\n",
      "Iteration 96, loss = 0.24176964\n",
      "Iteration 159, loss = 0.28638804\n",
      "Iteration 71, loss = 0.35218729\n",
      "Iteration 162, loss = 0.30572141\n",
      "Iteration 73, loss = 0.17920352\n",
      "Iteration 157, loss = 0.29593213\n",
      "Iteration 160, loss = 0.28631451\n",
      "Iteration 163, loss = 0.30581220\n",
      "Iteration 97, loss = 0.24154868\n",
      "Iteration 158, loss = 0.29617640\n",
      "Iteration 161, loss = 0.28647471\n",
      "Iteration 164, loss = 0.30562792\n",
      "Iteration 159, loss = 0.29596584\n",
      "Iteration 162, loss = 0.28622280\n",
      "Iteration 74, loss = 0.17887940\n",
      "Iteration 98, loss = 0.24128933\n",
      "Iteration 72, loss = 0.35217080\n",
      "Iteration 165, loss = 0.30544941\n",
      "Iteration 160, loss = 0.29582780\n",
      "Iteration 163, loss = 0.28613310\n",
      "Iteration 166, loss = 0.30540878\n",
      "Iteration 164, loss = 0.28602702\n",
      "Iteration 161, loss = 0.29577036\n",
      "Iteration 99, loss = 0.24144815\n",
      "Iteration 167, loss = 0.30530027\n",
      "Iteration 75, loss = 0.17845850\n",
      "Iteration 162, loss = 0.29587867\n",
      "Iteration 165, loss = 0.28612697\n",
      "Iteration 168, loss = 0.30550680\n",
      "Iteration 73, loss = 0.35218981\n",
      "Iteration 163, loss = 0.29589596\n",
      "Iteration 100, loss = 0.24106614\n",
      "Iteration 166, loss = 0.28618699\n",
      "Iteration 169, loss = 0.30529123\n",
      "Iteration 164, loss = 0.29561375\n",
      "Iteration 167, loss = 0.28613127\n",
      "Iteration 76, loss = 0.17823293\n",
      "Iteration 170, loss = 0.30496499\n",
      "Iteration 101, loss = 0.24093664\n",
      "Iteration 165, loss = 0.29581934\n",
      "Iteration 168, loss = 0.28601536\n",
      "Iteration 171, loss = 0.30509123\n",
      "Iteration 166, loss = 0.29561010\n",
      "Iteration 169, loss = 0.28596806\n",
      "Iteration 74, loss = 0.35181684\n",
      "Iteration 102, loss = 0.24076018\n",
      "Iteration 77, loss = 0.17791083\n",
      "Iteration 172, loss = 0.30513331\n",
      "Iteration 170, loss = 0.28608536\n",
      "Iteration 167, loss = 0.29530565\n",
      "Iteration 173, loss = 0.30488530\n",
      "Iteration 171, loss = 0.28579688\n",
      "Iteration 168, loss = 0.29562359\n",
      "Iteration 103, loss = 0.24053283\n",
      "Iteration 174, loss = 0.30506189\n",
      "Iteration 172, loss = 0.28587723\n",
      "Iteration 78, loss = 0.17764785\n",
      "Iteration 169, loss = 0.29530128\n",
      "Iteration 75, loss = 0.35163549\n",
      "Iteration 175, loss = 0.30482258\n",
      "Iteration 104, loss = 0.24052657\n",
      "Iteration 173, loss = 0.28573961\n",
      "Iteration 170, loss = 0.29536541\n",
      "Iteration 176, loss = 0.30457885\n",
      "Iteration 174, loss = 0.28583824\n",
      "Iteration 171, loss = 0.29542769\n",
      "Iteration 177, loss = 0.30462148\n",
      "Iteration 79, loss = 0.17724543\n",
      "Iteration 105, loss = 0.24026236\n",
      "Iteration 175, loss = 0.28577519\n",
      "Iteration 172, loss = 0.29523992\n",
      "Iteration 178, loss = 0.30459682\n",
      "Iteration 76, loss = 0.35150009\n",
      "Iteration 176, loss = 0.28571136\n",
      "Iteration 173, loss = 0.29528688\n",
      "Iteration 106, loss = 0.24015438\n",
      "Iteration 179, loss = 0.30444035\n",
      "Iteration 177, loss = 0.28571025\n",
      "Iteration 174, loss = 0.29532719\n",
      "Iteration 80, loss = 0.17709388\n",
      "Iteration 180, loss = 0.30436796\n",
      "Iteration 178, loss = 0.28552832\n",
      "Iteration 107, loss = 0.24018537\n",
      "Iteration 175, loss = 0.29525915\n",
      "Iteration 181, loss = 0.30469338\n",
      "Iteration 77, loss = 0.35154833\n",
      "Iteration 179, loss = 0.28560475\n",
      "Iteration 176, loss = 0.29493078\n",
      "Iteration 182, loss = 0.30420972\n",
      "Iteration 81, loss = 0.17661868\n",
      "Iteration 108, loss = 0.23992015\n",
      "Iteration 180, loss = 0.28522958\n",
      "Iteration 177, loss = 0.29509312\n",
      "Iteration 183, loss = 0.30433081\n",
      "Iteration 181, loss = 0.28540534\n",
      "Iteration 178, loss = 0.29520597\n",
      "Iteration 184, loss = 0.30409529\n",
      "Iteration 109, loss = 0.23957266\n",
      "Iteration 182, loss = 0.28530043\n",
      "Iteration 82, loss = 0.17634428\n",
      "Iteration 78, loss = 0.35132814\n",
      "Iteration 185, loss = 0.30391195\n",
      "Iteration 179, loss = 0.29502348\n",
      "Iteration 183, loss = 0.28537853\n",
      "Iteration 186, loss = 0.30428929\n",
      "Iteration 110, loss = 0.23965547\n",
      "Iteration 180, loss = 0.29501981\n",
      "Iteration 184, loss = 0.28552852\n",
      "Iteration 187, loss = 0.30412139\n",
      "Iteration 181, loss = 0.29495000\n",
      "Iteration 83, loss = 0.17605976\n",
      "Iteration 185, loss = 0.28527593\n",
      "Iteration 111, loss = 0.23938250\n",
      "Iteration 79, loss = 0.35095792\n",
      "Iteration 188, loss = 0.30386213\n",
      "Iteration 182, loss = 0.29483034\n",
      "Iteration 186, loss = 0.28520181\n",
      "Iteration 189, loss = 0.30419019\n",
      "Iteration 183, loss = 0.29480860\n",
      "Iteration 112, loss = 0.23926752\n",
      "Iteration 84, loss = 0.17583319\n",
      "Iteration 187, loss = 0.28503112\n",
      "Iteration 190, loss = 0.30386024\n",
      "Iteration 184, loss = 0.29494976\n",
      "Iteration 188, loss = 0.28500935\n",
      "Iteration 191, loss = 0.30363347\n",
      "Iteration 113, loss = 0.23918695\n",
      "Iteration 185, loss = 0.29471334\n",
      "Iteration 80, loss = 0.35098522\n",
      "Iteration 189, loss = 0.28527925\n",
      "Iteration 192, loss = 0.30369152\n",
      "Iteration 85, loss = 0.17542304\n",
      "Iteration 186, loss = 0.29472201\n",
      "Iteration 190, loss = 0.28491463\n",
      "Iteration 114, loss = 0.23905211\n",
      "Iteration 193, loss = 0.30381429\n",
      "Iteration 187, loss = 0.29476488\n",
      "Iteration 191, loss = 0.28510348\n",
      "Iteration 194, loss = 0.30329068\n",
      "Iteration 188, loss = 0.29456510\n",
      "Iteration 81, loss = 0.35080002\n",
      "Iteration 86, loss = 0.17526603\n",
      "Iteration 192, loss = 0.28511490\n",
      "Iteration 115, loss = 0.23903606\n",
      "Iteration 195, loss = 0.30355763\n",
      "Iteration 189, loss = 0.29466274\n",
      "Iteration 193, loss = 0.28488951\n",
      "Iteration 196, loss = 0.30323151\n",
      "Iteration 190, loss = 0.29462365\n",
      "Iteration 116, loss = 0.23861730\n",
      "Iteration 194, loss = 0.28491845\n",
      "Iteration 87, loss = 0.17501774\n",
      "Iteration 197, loss = 0.30340363\n",
      "Iteration 191, loss = 0.29437078\n",
      "Iteration 195, loss = 0.28476697\n",
      "Iteration 82, loss = 0.35093389\n",
      "Iteration 198, loss = 0.30313989\n",
      "Iteration 192, loss = 0.29457662\n",
      "Iteration 117, loss = 0.23864383\n",
      "Iteration 196, loss = 0.28493570\n",
      "Iteration 199, loss = 0.30353175\n",
      "Iteration 193, loss = 0.29449458\n",
      "Iteration 88, loss = 0.17474681\n",
      "Iteration 197, loss = 0.28464221\n",
      "Iteration 200, loss = 0.30326231\n",
      "Iteration 118, loss = 0.23843447\n",
      "Iteration 194, loss = 0.29453271\n",
      "Iteration 198, loss = 0.28514136\n",
      "Iteration 83, loss = 0.35069856\n",
      "Iteration 201, loss = 0.30319250\n",
      "Iteration 195, loss = 0.29439480\n",
      "Iteration 199, loss = 0.28463442\n",
      "Iteration 119, loss = 0.23834388\n",
      "Iteration 202, loss = 0.30327982\n",
      "Iteration 89, loss = 0.17462014\n",
      "Iteration 196, loss = 0.29429444\n",
      "Iteration 200, loss = 0.28451081\n",
      "Iteration 203, loss = 0.30307039\n",
      "Iteration 197, loss = 0.29432162\n",
      "Iteration 120, loss = 0.23831564\n",
      "Iteration 201, loss = 0.28467235\n",
      "Iteration 204, loss = 0.30299804\n",
      "Iteration 84, loss = 0.35050183\n",
      "Iteration 198, loss = 0.29435814\n",
      "Iteration 90, loss = 0.17403827\n",
      "Iteration 202, loss = 0.28452199\n",
      "Iteration 205, loss = 0.30296984\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 199, loss = 0.29424380\n",
      "Iteration 121, loss = 0.23797795\n",
      "Iteration 203, loss = 0.28474056\n",
      "Iteration 200, loss = 0.29412841\n",
      "Iteration 204, loss = 0.28468640\n",
      "Iteration 91, loss = 0.17397742\n",
      "Iteration 122, loss = 0.23782169\n",
      "Iteration 201, loss = 0.29436624\n",
      "Iteration 85, loss = 0.35028553\n",
      "Iteration 205, loss = 0.28457018\n",
      "Iteration 202, loss = 0.29417467\n",
      "Iteration 206, loss = 0.28438867\n",
      "Iteration 123, loss = 0.23799957\n",
      "Iteration 203, loss = 0.29417603\n",
      "Iteration 92, loss = 0.17368536\n",
      "Iteration 207, loss = 0.28456971\n",
      "Iteration 204, loss = 0.29398116\n",
      "Iteration 86, loss = 0.35028415\n",
      "Iteration 124, loss = 0.23781230\n",
      "Iteration 208, loss = 0.28442992\n",
      "Iteration 205, loss = 0.29389162\n",
      "Iteration 209, loss = 0.28452460\n",
      "Iteration 93, loss = 0.17346161\n",
      "Iteration 206, loss = 0.29403136\n",
      "Iteration 125, loss = 0.23765619\n",
      "Iteration 210, loss = 0.28454164\n",
      "Iteration 207, loss = 0.29383504\n",
      "Iteration 87, loss = 0.35028858\n",
      "Iteration 211, loss = 0.28428167\n",
      "Iteration 126, loss = 0.23741671\n",
      "Iteration 208, loss = 0.29389771\n",
      "Iteration 94, loss = 0.17304580\n",
      "Iteration 212, loss = 0.28410990\n",
      "Iteration 209, loss = 0.29381091\n",
      "Iteration 127, loss = 0.23742546\n",
      "Iteration 213, loss = 0.28416471\n",
      "Iteration 210, loss = 0.29382016\n",
      "Iteration 95, loss = 0.17282993\n",
      "Iteration 88, loss = 0.34995472\n",
      "Iteration 214, loss = 0.28404712\n",
      "Iteration 211, loss = 0.29375962\n",
      "Iteration 128, loss = 0.23748747\n",
      "Iteration 215, loss = 0.28435591\n",
      "Iteration 212, loss = 0.29362988\n",
      "Iteration 216, loss = 0.28414639\n",
      "Iteration 96, loss = 0.17272134\n",
      "Iteration 213, loss = 0.29390104\n",
      "Iteration 129, loss = 0.23728140\n",
      "Iteration 217, loss = 0.28417607\n",
      "Iteration 89, loss = 0.34979646\n",
      "Iteration 214, loss = 0.29360671\n",
      "Iteration 218, loss = 0.28398440\n",
      "Iteration 130, loss = 0.23701749\n",
      "Iteration 215, loss = 0.29362567\n",
      "Iteration 97, loss = 0.17240905\n",
      "Iteration 219, loss = 0.28386381\n",
      "Iteration 216, loss = 0.29357032\n",
      "Iteration 220, loss = 0.28410866\n",
      "Iteration 131, loss = 0.23693346\n",
      "Iteration 90, loss = 0.34976005\n",
      "Iteration 217, loss = 0.29355925\n",
      "Iteration 221, loss = 0.28401173\n",
      "Iteration 98, loss = 0.17212196\n",
      "Iteration 218, loss = 0.29363894\n",
      "Iteration 132, loss = 0.23673927\n",
      "Iteration 222, loss = 0.28386128\n",
      "Iteration 219, loss = 0.29353730\n",
      "Iteration 223, loss = 0.28411001\n",
      "Iteration 133, loss = 0.23689033\n",
      "Iteration 99, loss = 0.17184958\n",
      "Iteration 220, loss = 0.29365369\n",
      "Iteration 91, loss = 0.34986196\n",
      "Iteration 224, loss = 0.28409002\n",
      "Iteration 221, loss = 0.29351923\n",
      "Iteration 225, loss = 0.28374926\n",
      "Iteration 134, loss = 0.23669538\n",
      "Iteration 222, loss = 0.29336299\n",
      "Iteration 226, loss = 0.28384375\n",
      "Iteration 100, loss = 0.17163907\n",
      "Iteration 223, loss = 0.29338810\n",
      "Iteration 92, loss = 0.34973689\n",
      "Iteration 227, loss = 0.28387394\n",
      "Iteration 135, loss = 0.23648319\n",
      "Iteration 224, loss = 0.29349323\n",
      "Iteration 228, loss = 0.28380603\n",
      "Iteration 101, loss = 0.17141094\n",
      "Iteration 225, loss = 0.29351490\n",
      "Iteration 136, loss = 0.23651900\n",
      "Iteration 229, loss = 0.28379815\n",
      "Iteration 226, loss = 0.29321306\n",
      "Iteration 230, loss = 0.28370414\n",
      "Iteration 93, loss = 0.34963001\n",
      "Iteration 137, loss = 0.23621849\n",
      "Iteration 227, loss = 0.29326400\n",
      "Iteration 102, loss = 0.17109127\n",
      "Iteration 231, loss = 0.28354219\n",
      "Iteration 228, loss = 0.29333087\n",
      "Iteration 232, loss = 0.28366826\n",
      "Iteration 138, loss = 0.23611699\n",
      "Iteration 229, loss = 0.29321534\n",
      "Iteration 233, loss = 0.28370966\n",
      "Iteration 94, loss = 0.34928873\n",
      "Iteration 103, loss = 0.17090929\n",
      "Iteration 230, loss = 0.29317001\n",
      "Iteration 234, loss = 0.28362574\n",
      "Iteration 139, loss = 0.23612321\n",
      "Iteration 231, loss = 0.29315292\n",
      "Iteration 235, loss = 0.28345742\n",
      "Iteration 232, loss = 0.29307134\n",
      "Iteration 140, loss = 0.23598358\n",
      "Iteration 236, loss = 0.28337880\n",
      "Iteration 104, loss = 0.17075010\n",
      "Iteration 95, loss = 0.34934927\n",
      "Iteration 233, loss = 0.29309601\n",
      "Iteration 237, loss = 0.28356275\n",
      "Iteration 141, loss = 0.23579461\n",
      "Iteration 234, loss = 0.29300370\n",
      "Iteration 238, loss = 0.28337675\n",
      "Iteration 105, loss = 0.17052558\n",
      "Iteration 235, loss = 0.29293405\n",
      "Iteration 239, loss = 0.28344438\n",
      "Iteration 142, loss = 0.23566036\n",
      "Iteration 96, loss = 0.34926879\n",
      "Iteration 236, loss = 0.29301398\n",
      "Iteration 240, loss = 0.28324256\n",
      "Iteration 237, loss = 0.29294134\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 106, loss = 0.17018408\n",
      "Iteration 241, loss = 0.28347645\n",
      "Iteration 143, loss = 0.23559908\n",
      "Iteration 242, loss = 0.28336587\n",
      "Iteration 144, loss = 0.23566263\n",
      "Iteration 97, loss = 0.34923486\n",
      "Iteration 243, loss = 0.28319851\n",
      "Iteration 107, loss = 0.16995561\n",
      "Iteration 244, loss = 0.28325421\n",
      "Iteration 145, loss = 0.23546415\n",
      "Iteration 245, loss = 0.28336765\n",
      "Iteration 108, loss = 0.16972933\n",
      "Iteration 246, loss = 0.28305667\n",
      "Iteration 98, loss = 0.34906762\n",
      "Iteration 146, loss = 0.23544326\n",
      "Iteration 247, loss = 0.28303845\n",
      "Iteration 248, loss = 0.28314488\n",
      "Iteration 147, loss = 0.23526114\n",
      "Iteration 109, loss = 0.16969539\n",
      "Iteration 249, loss = 0.28307186\n",
      "Iteration 99, loss = 0.34918192\n",
      "Iteration 148, loss = 0.23520406\n",
      "Iteration 250, loss = 0.28301716\n",
      "Iteration 110, loss = 0.16923072\n",
      "Iteration 251, loss = 0.28289704\n",
      "Iteration 149, loss = 0.23509953\n",
      "Iteration 252, loss = 0.28313121\n",
      "Iteration 100, loss = 0.34888013\n",
      "Iteration 253, loss = 0.28288686\n",
      "Iteration 150, loss = 0.23493969\n",
      "Iteration 111, loss = 0.16909043\n",
      "Iteration 254, loss = 0.28286253\n",
      "Iteration 255, loss = 0.28302754\n",
      "Iteration 151, loss = 0.23488050\n",
      "Iteration 112, loss = 0.16892930\n",
      "Iteration 101, loss = 0.34883586\n",
      "Iteration 256, loss = 0.28279745\n",
      "Iteration 152, loss = 0.23480705\n",
      "Iteration 257, loss = 0.28267478\n",
      "Iteration 258, loss = 0.28272838\n",
      "Iteration 113, loss = 0.16885083\n",
      "Iteration 153, loss = 0.23470506\n",
      "Iteration 259, loss = 0.28290364\n",
      "Iteration 102, loss = 0.34869240\n",
      "Iteration 260, loss = 0.28291015\n",
      "Iteration 154, loss = 0.23454370\n",
      "Iteration 114, loss = 0.16845650\n",
      "Iteration 261, loss = 0.28274503\n",
      "Iteration 262, loss = 0.28263253\n",
      "Iteration 155, loss = 0.23434737\n",
      "Iteration 103, loss = 0.34867976\n",
      "Iteration 263, loss = 0.28282770\n",
      "Iteration 115, loss = 0.16830140\n",
      "Iteration 156, loss = 0.23427477\n",
      "Iteration 264, loss = 0.28278284\n",
      "Iteration 265, loss = 0.28263102\n",
      "Iteration 157, loss = 0.23431708\n",
      "Iteration 116, loss = 0.16793329\n",
      "Iteration 104, loss = 0.34868528\n",
      "Iteration 266, loss = 0.28265903\n",
      "Iteration 267, loss = 0.28246736\n",
      "Iteration 158, loss = 0.23436981\n",
      "Iteration 268, loss = 0.28265601\n",
      "Iteration 117, loss = 0.16781838\n",
      "Iteration 105, loss = 0.34846428\n",
      "Iteration 269, loss = 0.28274169\n",
      "Iteration 159, loss = 0.23411523\n",
      "Iteration 270, loss = 0.28251856\n",
      "Iteration 118, loss = 0.16760884\n",
      "Iteration 160, loss = 0.23423904\n",
      "Iteration 271, loss = 0.28255122\n",
      "Iteration 272, loss = 0.28274459\n",
      "Iteration 106, loss = 0.34835225\n",
      "Iteration 161, loss = 0.23395270\n",
      "Iteration 273, loss = 0.28245511\n",
      "Iteration 119, loss = 0.16748438\n",
      "Iteration 274, loss = 0.28267044\n",
      "Iteration 162, loss = 0.23392789\n",
      "Iteration 275, loss = 0.28232798\n",
      "Iteration 107, loss = 0.34837244\n",
      "Iteration 120, loss = 0.16731823\n",
      "Iteration 276, loss = 0.28241733\n",
      "Iteration 163, loss = 0.23377082\n",
      "Iteration 277, loss = 0.28205319\n",
      "Iteration 164, loss = 0.23376258\n",
      "Iteration 278, loss = 0.28226070\n",
      "Iteration 121, loss = 0.16714737\n",
      "Iteration 108, loss = 0.34835229\n",
      "Iteration 279, loss = 0.28231407\n",
      "Iteration 165, loss = 0.23348685\n",
      "Iteration 280, loss = 0.28231793\n",
      "Iteration 122, loss = 0.16691910\n",
      "Iteration 281, loss = 0.28223488\n",
      "Iteration 166, loss = 0.23352346\n",
      "Iteration 109, loss = 0.34819105\n",
      "Iteration 282, loss = 0.28237359\n",
      "Iteration 283, loss = 0.28208897\n",
      "Iteration 167, loss = 0.23337467\n",
      "Iteration 123, loss = 0.16659716\n",
      "Iteration 284, loss = 0.28206045\n",
      "Iteration 168, loss = 0.23347311\n",
      "Iteration 285, loss = 0.28214996\n",
      "Iteration 110, loss = 0.34812960\n",
      "Iteration 124, loss = 0.16649743\n",
      "Iteration 286, loss = 0.28212709\n",
      "Iteration 169, loss = 0.23320751\n",
      "Iteration 287, loss = 0.28210938\n",
      "Iteration 288, loss = 0.28205007\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 125, loss = 0.16629286\n",
      "Iteration 170, loss = 0.23330470\n",
      "Iteration 111, loss = 0.34792294\n",
      "Iteration 171, loss = 0.23325287\n",
      "Iteration 126, loss = 0.16613958\n",
      "Iteration 112, loss = 0.34786755\n",
      "Iteration 172, loss = 0.23320504\n",
      "Iteration 127, loss = 0.16580926\n",
      "Iteration 173, loss = 0.23301584\n",
      "Iteration 113, loss = 0.34796747\n",
      "Iteration 174, loss = 0.23309501\n",
      "Iteration 128, loss = 0.16578984\n",
      "Iteration 175, loss = 0.23287997\n",
      "Iteration 129, loss = 0.16557457\n",
      "Iteration 114, loss = 0.34786819\n",
      "Iteration 176, loss = 0.23296656\n",
      "Iteration 177, loss = 0.23271296\n",
      "Iteration 130, loss = 0.16541874\n",
      "Iteration 115, loss = 0.34768308\n",
      "Iteration 178, loss = 0.23260627\n",
      "Iteration 131, loss = 0.16509614\n",
      "Iteration 179, loss = 0.23264427\n",
      "Iteration 116, loss = 0.34749859\n",
      "Iteration 132, loss = 0.16506397\n",
      "Iteration 180, loss = 0.23263142\n",
      "Iteration 181, loss = 0.23253563\n",
      "Iteration 117, loss = 0.34745442\n",
      "Iteration 133, loss = 0.16489196\n",
      "Iteration 182, loss = 0.23264688\n",
      "Iteration 134, loss = 0.16462258\n",
      "Iteration 183, loss = 0.23235483\n",
      "Iteration 118, loss = 0.34739970\n",
      "Iteration 184, loss = 0.23235191\n",
      "Iteration 135, loss = 0.16435942\n",
      "Iteration 119, loss = 0.34740531\n",
      "Iteration 185, loss = 0.23232670\n",
      "Iteration 136, loss = 0.16427589\n",
      "Iteration 186, loss = 0.23226354\n",
      "Iteration 120, loss = 0.34740844\n",
      "Iteration 187, loss = 0.23209693\n",
      "Iteration 137, loss = 0.16411105\n",
      "Iteration 188, loss = 0.23207525\n",
      "Iteration 138, loss = 0.16395094\n",
      "Iteration 121, loss = 0.34738044\n",
      "Iteration 189, loss = 0.23206994\n",
      "Iteration 139, loss = 0.16365172\n",
      "Iteration 190, loss = 0.23194420\n",
      "Iteration 122, loss = 0.34717644\n",
      "Iteration 191, loss = 0.23191608\n",
      "Iteration 140, loss = 0.16365362\n",
      "Iteration 192, loss = 0.23186420\n",
      "Iteration 123, loss = 0.34722362\n",
      "Iteration 141, loss = 0.16353052\n",
      "Iteration 193, loss = 0.23174598\n",
      "Iteration 194, loss = 0.23188380\n",
      "Iteration 142, loss = 0.16337469\n",
      "Iteration 124, loss = 0.34707001\n",
      "Iteration 195, loss = 0.23179116\n",
      "Iteration 143, loss = 0.16304993\n",
      "Iteration 196, loss = 0.23181011\n",
      "Iteration 125, loss = 0.34705032\n",
      "Iteration 197, loss = 0.23159087\n",
      "Iteration 144, loss = 0.16298534\n",
      "Iteration 198, loss = 0.23142044\n",
      "Iteration 126, loss = 0.34685708\n",
      "Iteration 145, loss = 0.16275281\n",
      "Iteration 199, loss = 0.23144028\n",
      "Iteration 146, loss = 0.16258063\n",
      "Iteration 200, loss = 0.23126085\n",
      "Iteration 127, loss = 0.34708680\n",
      "Iteration 201, loss = 0.23132650\n",
      "Iteration 147, loss = 0.16268845\n",
      "Iteration 128, loss = 0.34669829\n",
      "Iteration 202, loss = 0.23144640\n",
      "Iteration 148, loss = 0.16221790\n",
      "Iteration 203, loss = 0.23144077\n",
      "Iteration 129, loss = 0.34690396\n",
      "Iteration 204, loss = 0.23124070\n",
      "Iteration 149, loss = 0.16220824\n",
      "Iteration 205, loss = 0.23114291\n",
      "Iteration 150, loss = 0.16207183\n",
      "Iteration 130, loss = 0.34675522\n",
      "Iteration 206, loss = 0.23115846\n",
      "Iteration 207, loss = 0.23121476\n",
      "Iteration 151, loss = 0.16188366\n",
      "Iteration 131, loss = 0.34641041\n",
      "Iteration 208, loss = 0.23116735\n",
      "Iteration 152, loss = 0.16175245\n",
      "Iteration 209, loss = 0.23098109\n",
      "Iteration 132, loss = 0.34665756\n",
      "Iteration 153, loss = 0.16149680\n",
      "Iteration 210, loss = 0.23085046\n",
      "Iteration 211, loss = 0.23085384\n",
      "Iteration 133, loss = 0.34638510\n",
      "Iteration 154, loss = 0.16139377\n",
      "Iteration 212, loss = 0.23074862\n",
      "Iteration 155, loss = 0.16121754\n",
      "Iteration 213, loss = 0.23066483\n",
      "Iteration 134, loss = 0.34638093\n",
      "Iteration 214, loss = 0.23081849\n",
      "Iteration 156, loss = 0.16114991\n",
      "Iteration 215, loss = 0.23087593\n",
      "Iteration 135, loss = 0.34654372\n",
      "Iteration 157, loss = 0.16084553\n",
      "Iteration 216, loss = 0.23059354\n",
      "Iteration 136, loss = 0.34631940\n",
      "Iteration 217, loss = 0.23048927\n",
      "Iteration 158, loss = 0.16071633\n",
      "Iteration 218, loss = 0.23042981\n",
      "Iteration 159, loss = 0.16061709\n",
      "Iteration 137, loss = 0.34609682\n",
      "Iteration 219, loss = 0.23068553\n",
      "Iteration 160, loss = 0.16052487\n",
      "Iteration 220, loss = 0.23057621\n",
      "Iteration 138, loss = 0.34621275\n",
      "Iteration 221, loss = 0.23046153\n",
      "Iteration 161, loss = 0.16033488\n",
      "Iteration 222, loss = 0.23055851\n",
      "Iteration 139, loss = 0.34596087\n",
      "Iteration 162, loss = 0.16021106\n",
      "Iteration 223, loss = 0.23046091\n",
      "Iteration 224, loss = 0.23019415\n",
      "Iteration 163, loss = 0.16015701\n",
      "Iteration 140, loss = 0.34612945\n",
      "Iteration 225, loss = 0.23020741\n",
      "Iteration 164, loss = 0.15994640\n",
      "Iteration 226, loss = 0.23030142\n",
      "Iteration 141, loss = 0.34582453\n",
      "Iteration 165, loss = 0.15977192\n",
      "Iteration 227, loss = 0.23018731\n",
      "Iteration 228, loss = 0.23010647\n",
      "Iteration 142, loss = 0.34582758\n",
      "Iteration 166, loss = 0.15962794\n",
      "Iteration 229, loss = 0.23010426\n",
      "Iteration 167, loss = 0.15957160\n",
      "Iteration 143, loss = 0.34607558\n",
      "Iteration 230, loss = 0.23011251\n",
      "Iteration 231, loss = 0.22997209\n",
      "Iteration 168, loss = 0.15942048\n",
      "Iteration 144, loss = 0.34566484\n",
      "Iteration 232, loss = 0.23002784\n",
      "Iteration 169, loss = 0.15924843\n",
      "Iteration 233, loss = 0.22980471\n",
      "Iteration 145, loss = 0.34562350\n",
      "Iteration 234, loss = 0.23005426\n",
      "Iteration 170, loss = 0.15911458\n",
      "Iteration 235, loss = 0.23001348\n",
      "Iteration 171, loss = 0.15911761\n",
      "Iteration 146, loss = 0.34584072\n",
      "Iteration 236, loss = 0.22977338\n",
      "Iteration 172, loss = 0.15892810\n",
      "Iteration 237, loss = 0.22993463\n",
      "Iteration 147, loss = 0.34584283\n",
      "Iteration 238, loss = 0.22981140\n",
      "Iteration 173, loss = 0.15886231\n",
      "Iteration 239, loss = 0.22966002\n",
      "Iteration 148, loss = 0.34538007\n",
      "Iteration 174, loss = 0.15872175\n",
      "Iteration 240, loss = 0.22966493\n",
      "Iteration 241, loss = 0.22969756\n",
      "Iteration 149, loss = 0.34546907\n",
      "Iteration 175, loss = 0.15862013\n",
      "Iteration 242, loss = 0.22966593\n",
      "Iteration 176, loss = 0.15837670\n",
      "Iteration 243, loss = 0.22956815\n",
      "Iteration 150, loss = 0.34560037\n",
      "Iteration 244, loss = 0.22966903\n",
      "Iteration 177, loss = 0.15834816\n",
      "Iteration 245, loss = 0.22950744\n",
      "Iteration 151, loss = 0.34530800\n",
      "Iteration 178, loss = 0.15817537\n",
      "Iteration 246, loss = 0.22948139\n",
      "Iteration 152, loss = 0.34547612\n",
      "Iteration 179, loss = 0.15819412\n",
      "Iteration 247, loss = 0.22942128\n",
      "Iteration 248, loss = 0.22939635\n",
      "Iteration 180, loss = 0.15817736\n",
      "Iteration 153, loss = 0.34528828\n",
      "Iteration 249, loss = 0.22936101\n",
      "Iteration 181, loss = 0.15805344\n",
      "Iteration 250, loss = 0.22956125\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 154, loss = 0.34525806\n",
      "Iteration 182, loss = 0.15774183\n",
      "Iteration 155, loss = 0.34512534\n",
      "Iteration 183, loss = 0.15780507\n",
      "Iteration 184, loss = 0.15762738\n",
      "Iteration 156, loss = 0.34524343\n",
      "Iteration 185, loss = 0.15746015\n",
      "Iteration 157, loss = 0.34514839\n",
      "Iteration 186, loss = 0.15747549\n",
      "Iteration 158, loss = 0.34519617\n",
      "Iteration 187, loss = 0.15734707\n",
      "Iteration 188, loss = 0.15729981\n",
      "Iteration 159, loss = 0.34504157\n",
      "Iteration 189, loss = 0.15713702\n",
      "Iteration 160, loss = 0.34491461\n",
      "Iteration 190, loss = 0.15691833\n",
      "Iteration 161, loss = 0.34486915\n",
      "Iteration 191, loss = 0.15692240\n",
      "Iteration 192, loss = 0.15681606\n",
      "Iteration 162, loss = 0.34504795\n",
      "Iteration 193, loss = 0.15677797\n",
      "Iteration 163, loss = 0.34475156\n",
      "Iteration 194, loss = 0.15674226\n",
      "Iteration 164, loss = 0.34487539\n",
      "Iteration 195, loss = 0.15640044\n",
      "Iteration 165, loss = 0.34480198\n",
      "Iteration 196, loss = 0.15652703\n",
      "Iteration 197, loss = 0.15623427\n",
      "Iteration 166, loss = 0.34480644\n",
      "Iteration 198, loss = 0.15622633\n",
      "Iteration 167, loss = 0.34456776\n",
      "Iteration 199, loss = 0.15623974\n",
      "Iteration 168, loss = 0.34483206\n",
      "Iteration 200, loss = 0.15617012\n",
      "Iteration 201, loss = 0.15593409\n",
      "Iteration 169, loss = 0.34442225\n",
      "Iteration 202, loss = 0.15587363\n",
      "Iteration 170, loss = 0.34476202\n",
      "Iteration 203, loss = 0.15569011\n",
      "Iteration 171, loss = 0.34468893\n",
      "Iteration 204, loss = 0.15565813\n",
      "Iteration 205, loss = 0.15570624\n",
      "Iteration 172, loss = 0.34436245\n",
      "Iteration 206, loss = 0.15568921\n",
      "Iteration 173, loss = 0.34435964\n",
      "Iteration 207, loss = 0.15555486\n",
      "Iteration 174, loss = 0.34441066\n",
      "Iteration 208, loss = 0.15533408\n",
      "Iteration 209, loss = 0.15536609\n",
      "Iteration 175, loss = 0.34450457\n",
      "Iteration 210, loss = 0.15514530\n",
      "Iteration 176, loss = 0.34434129\n",
      "Iteration 211, loss = 0.15544563\n",
      "Iteration 177, loss = 0.34417358\n",
      "Iteration 212, loss = 0.15513088\n",
      "Iteration 213, loss = 0.15507456\n",
      "Iteration 178, loss = 0.34423735\n",
      "Iteration 214, loss = 0.15489444\n",
      "Iteration 179, loss = 0.34425520\n",
      "Iteration 215, loss = 0.15496610\n",
      "Iteration 180, loss = 0.34423070\n",
      "Iteration 216, loss = 0.15490126\n",
      "Iteration 181, loss = 0.34442204\n",
      "Iteration 217, loss = 0.15470074\n",
      "Iteration 218, loss = 0.15456132\n",
      "Iteration 182, loss = 0.34397771\n",
      "Iteration 219, loss = 0.15468347\n",
      "Iteration 183, loss = 0.34404468\n",
      "Iteration 220, loss = 0.15444026\n",
      "Iteration 184, loss = 0.34387324\n",
      "Iteration 221, loss = 0.15444836\n",
      "Iteration 222, loss = 0.15434708\n",
      "Iteration 185, loss = 0.34413277\n",
      "Iteration 223, loss = 0.15435786\n",
      "Iteration 186, loss = 0.34402469\n",
      "Iteration 224, loss = 0.15432275\n",
      "Iteration 187, loss = 0.34379457\n",
      "Iteration 225, loss = 0.15410259\n",
      "Iteration 226, loss = 0.15421371\n",
      "Iteration 188, loss = 0.34374517\n",
      "Iteration 227, loss = 0.15396871\n",
      "Iteration 189, loss = 0.34379711\n",
      "Iteration 228, loss = 0.15390893\n",
      "Iteration 190, loss = 0.34371769\n",
      "Iteration 229, loss = 0.15398053\n",
      "Iteration 230, loss = 0.15376548\n",
      "Iteration 191, loss = 0.34366123\n",
      "Iteration 231, loss = 0.15389837\n",
      "Iteration 192, loss = 0.34392099\n",
      "Iteration 232, loss = 0.15371290\n",
      "Iteration 193, loss = 0.34365451\n",
      "Iteration 233, loss = 0.15385269\n",
      "Iteration 234, loss = 0.15337555\n",
      "Iteration 194, loss = 0.34358538\n",
      "Iteration 235, loss = 0.15364565\n",
      "Iteration 195, loss = 0.34371013\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 236, loss = 0.15334151\n",
      "Iteration 237, loss = 0.15341770\n",
      "Iteration 238, loss = 0.15336637\n",
      "Iteration 239, loss = 0.15332193\n",
      "Iteration 240, loss = 0.15318655\n",
      "Iteration 241, loss = 0.15308602\n",
      "Iteration 242, loss = 0.15296945\n",
      "Iteration 243, loss = 0.15308701\n",
      "Iteration 244, loss = 0.15294626\n",
      "Iteration 245, loss = 0.15292610\n",
      "Iteration 246, loss = 0.15285292\n",
      "Iteration 247, loss = 0.15270351\n",
      "Iteration 248, loss = 0.15270053\n",
      "Iteration 249, loss = 0.15259732\n",
      "Iteration 250, loss = 0.15249651\n",
      "Iteration 251, loss = 0.15246944\n",
      "Iteration 252, loss = 0.15226277\n",
      "Iteration 253, loss = 0.15235682\n",
      "Iteration 254, loss = 0.15207814\n",
      "Iteration 255, loss = 0.15225885\n",
      "Iteration 256, loss = 0.15208088\n",
      "Iteration 257, loss = 0.15205324\n",
      "Iteration 258, loss = 0.15216385\n",
      "Iteration 259, loss = 0.15190457\n",
      "Iteration 260, loss = 0.15191173\n",
      "Iteration 261, loss = 0.15192535\n",
      "Iteration 262, loss = 0.15181580\n",
      "Iteration 263, loss = 0.15163556\n",
      "Iteration 264, loss = 0.15172309\n",
      "Iteration 265, loss = 0.15150694\n",
      "Iteration 266, loss = 0.15163024\n",
      "Iteration 267, loss = 0.15144471\n",
      "Iteration 268, loss = 0.15137849\n",
      "Iteration 269, loss = 0.15145177\n",
      "Iteration 270, loss = 0.15141218\n",
      "Iteration 271, loss = 0.15126941\n",
      "Iteration 272, loss = 0.15119536\n",
      "Iteration 273, loss = 0.15102372\n",
      "Iteration 274, loss = 0.15115066\n",
      "Iteration 275, loss = 0.15090193\n",
      "Iteration 276, loss = 0.15104626\n",
      "Iteration 277, loss = 0.15102856\n",
      "Iteration 278, loss = 0.15091914\n",
      "Iteration 279, loss = 0.15072201\n",
      "Iteration 280, loss = 0.15091153\n",
      "Iteration 281, loss = 0.15084740\n",
      "Iteration 282, loss = 0.15058989\n",
      "Iteration 283, loss = 0.15060737\n",
      "Iteration 284, loss = 0.15061534\n",
      "Iteration 285, loss = 0.15050486\n",
      "Iteration 286, loss = 0.15054788\n",
      "Iteration 287, loss = 0.15034207\n",
      "Iteration 288, loss = 0.15031738\n",
      "Iteration 289, loss = 0.15027584\n",
      "Iteration 290, loss = 0.15031450\n",
      "Iteration 291, loss = 0.15027385\n",
      "Iteration 292, loss = 0.15024224\n",
      "Iteration 293, loss = 0.14996030\n",
      "Iteration 294, loss = 0.15001471\n",
      "Iteration 295, loss = 0.15005573\n",
      "Iteration 296, loss = 0.14996378\n",
      "Iteration 297, loss = 0.14983974\n",
      "Iteration 298, loss = 0.14997133\n",
      "Iteration 299, loss = 0.14972423\n",
      "Iteration 300, loss = 0.14973517\n",
      "Iteration 301, loss = 0.14985162\n",
      "Iteration 302, loss = 0.14948524\n",
      "Iteration 303, loss = 0.14964846\n",
      "Iteration 304, loss = 0.14945678\n",
      "Iteration 305, loss = 0.14946217\n",
      "Iteration 306, loss = 0.14932694\n",
      "Iteration 307, loss = 0.14939197\n",
      "Iteration 308, loss = 0.14938586\n",
      "Iteration 309, loss = 0.14939179\n",
      "Iteration 310, loss = 0.14930601\n",
      "Iteration 311, loss = 0.14925610\n",
      "Iteration 312, loss = 0.14908491\n",
      "Iteration 313, loss = 0.14915644\n",
      "Iteration 314, loss = 0.14899943\n",
      "Iteration 315, loss = 0.14880582\n",
      "Iteration 316, loss = 0.14893916\n",
      "Iteration 317, loss = 0.14905253\n",
      "Iteration 318, loss = 0.14878891\n",
      "Iteration 319, loss = 0.14896287\n",
      "Iteration 320, loss = 0.14880438\n",
      "Iteration 321, loss = 0.14870043\n",
      "Iteration 322, loss = 0.14868369\n",
      "Iteration 323, loss = 0.14883142\n",
      "Iteration 324, loss = 0.14857287\n",
      "Iteration 325, loss = 0.14862119\n",
      "Iteration 326, loss = 0.14851600\n",
      "Iteration 327, loss = 0.14845558\n",
      "Iteration 328, loss = 0.14846618\n",
      "Iteration 329, loss = 0.14849177\n",
      "Iteration 330, loss = 0.14828641\n",
      "Iteration 331, loss = 0.14831694\n",
      "Iteration 332, loss = 0.14817325\n",
      "Iteration 333, loss = 0.14819444\n",
      "Iteration 334, loss = 0.14815724\n",
      "Iteration 335, loss = 0.14808388\n",
      "Iteration 336, loss = 0.14821891\n",
      "Iteration 337, loss = 0.14804021\n",
      "Iteration 338, loss = 0.14799983\n",
      "Iteration 339, loss = 0.14805429\n",
      "Iteration 340, loss = 0.14795297\n",
      "Iteration 341, loss = 0.14802399\n",
      "Iteration 342, loss = 0.14778261\n",
      "Iteration 343, loss = 0.14770807\n",
      "Iteration 344, loss = 0.14778996\n",
      "Iteration 345, loss = 0.14784742\n",
      "Iteration 346, loss = 0.14776935\n",
      "Iteration 347, loss = 0.14773500\n",
      "Iteration 348, loss = 0.14755401\n",
      "Iteration 349, loss = 0.14763545\n",
      "Iteration 350, loss = 0.14750931\n",
      "Iteration 351, loss = 0.14761767\n",
      "Iteration 352, loss = 0.14743902\n",
      "Iteration 353, loss = 0.14742901\n",
      "Iteration 354, loss = 0.14737450\n",
      "Iteration 355, loss = 0.14740400\n",
      "Iteration 356, loss = 0.14735257\n",
      "Iteration 357, loss = 0.14735958\n",
      "Iteration 358, loss = 0.14723266\n",
      "Iteration 359, loss = 0.14730755\n",
      "Iteration 360, loss = 0.14730724\n",
      "Iteration 361, loss = 0.14723309\n",
      "Iteration 362, loss = 0.14732600\n",
      "Iteration 363, loss = 0.14710396\n",
      "Iteration 364, loss = 0.14703611\n",
      "Iteration 365, loss = 0.14720340\n",
      "Iteration 366, loss = 0.14708450\n",
      "Iteration 367, loss = 0.14705644\n",
      "Iteration 368, loss = 0.14696220\n",
      "Iteration 369, loss = 0.14696150\n",
      "Iteration 370, loss = 0.14693950\n",
      "Iteration 371, loss = 0.14698583\n",
      "Iteration 372, loss = 0.14677143\n",
      "Iteration 373, loss = 0.14682934\n",
      "Iteration 374, loss = 0.14675301\n",
      "Iteration 375, loss = 0.14685909\n",
      "Iteration 376, loss = 0.14666563\n",
      "Iteration 377, loss = 0.14657208\n",
      "Iteration 378, loss = 0.14674362\n",
      "Iteration 379, loss = 0.14662761\n",
      "Iteration 380, loss = 0.14667766\n",
      "Iteration 381, loss = 0.14647423\n",
      "Iteration 382, loss = 0.14651107\n",
      "Iteration 383, loss = 0.14660281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy on training set: 0.64\n"
     ]
    }
   ],
   "source": [
    "# create the MLP classifier\n",
    "mlp = MLPClassifier(verbose=True, max_iter=max_iterations)\n",
    "\n",
    "# create the one-vs-one classifier\n",
    "ovr_classifier = OneVsRestClassifier(mlp, n_jobs=-1)\n",
    "\n",
    "ovr_classifier.fit(X_train, t_train)\n",
    "\n",
    "# save the trained classifier to a file\n",
    "with open('./models/model2.pkl', 'wb') as f:\n",
    "    pickle.dump(ovr_classifier, f)\n",
    "\n",
    "print('Accuracy on training set: {:.2f}'.format(ovr_classifier.score(X_train, t_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68132476\n",
      "Iteration 1, loss = 0.54908082\n",
      "Iteration 1, loss = 0.69243963\n",
      "Iteration 1, loss = 0.38394481\n",
      "Iteration 1, loss = 0.24008682\n",
      "Iteration 1, loss = 0.63484432\n",
      "Iteration 1, loss = 0.47001413\n",
      "Iteration 1, loss = 0.63811346\n",
      "Iteration 1, loss = 0.68274556\n",
      "Iteration 1, loss = 0.50985964\n",
      "Iteration 1, loss = 0.29125529Iteration 1, loss = 0.65209511\n",
      "\n",
      "Iteration 1, loss = 0.62284525\n",
      "Iteration 1, loss = 0.68011090\n",
      "Iteration 1, loss = 0.43478134\n",
      "Iteration 1, loss = 0.34245229\n",
      "Iteration 1, loss = 0.56507046\n",
      "Iteration 1, loss = 0.36511141\n",
      "Iteration 1, loss = 0.26313518\n",
      "Iteration 1, loss = 0.43372344\n",
      "Iteration 1, loss = 0.63955149\n",
      "Iteration 2, loss = 0.57261748\n",
      "Iteration 2, loss = 0.18645295\n",
      "Iteration 2, loss = 0.29548547\n",
      "Iteration 2, loss = 0.68190881\n",
      "Iteration 2, loss = 0.58222412\n",
      "Iteration 2, loss = 0.05269876\n",
      "Iteration 2, loss = 0.51453952\n",
      "Iteration 2, loss = 0.34292139\n",
      "Iteration 2, loss = 0.31564564\n",
      "Iteration 2, loss = 0.64523025\n",
      "Iteration 2, loss = 0.06679696\n",
      "Iteration 2, loss = 0.60507632\n",
      "Iteration 2, loss = 0.48027960\n",
      "Iteration 2, loss = 0.53016625\n",
      "Iteration 2, loss = 0.20191899\n",
      "Iteration 2, loss = 0.09731307\n",
      "Iteration 2, loss = 0.36748952\n",
      "Iteration 2, loss = 0.06716492\n",
      "Iteration 2, loss = 0.57701898\n",
      "Iteration 2, loss = 0.05515340\n",
      "Iteration 3, loss = 0.55248292\n",
      "Iteration 2, loss = 0.09580947\n",
      "Iteration 3, loss = 0.16867047\n",
      "Iteration 3, loss = 0.67686082\n",
      "Iteration 3, loss = 0.26562607\n",
      "Iteration 3, loss = 0.57454356\n",
      "Iteration 3, loss = 0.02476433\n",
      "Iteration 3, loss = 0.48793824\n",
      "Iteration 3, loss = 0.32110554\n",
      "Iteration 3, loss = 0.28812960\n",
      "Iteration 3, loss = 0.03373346\n",
      "Iteration 3, loss = 0.61991333\n",
      "Iteration 3, loss = 0.42004642\n",
      "Iteration 3, loss = 0.18066440\n",
      "Iteration 3, loss = 0.58182172\n",
      "Iteration 3, loss = 0.04677105\n",
      "Iteration 3, loss = 0.48793915\n",
      "Iteration 3, loss = 0.31568842\n",
      "Iteration 3, loss = 0.02833664\n",
      "Iteration 3, loss = 0.56562548\n",
      "Iteration 4, loss = 0.54504336\n",
      "Iteration 3, loss = 0.02827240\n",
      "Iteration 3, loss = 0.03945766\n",
      "Iteration 4, loss = 0.15704340\n",
      "Iteration 4, loss = 0.67204673\n",
      "Iteration 4, loss = 0.56966708\n",
      "Iteration 4, loss = 0.24865663\n",
      "Iteration 4, loss = 0.01540256\n",
      "Iteration 4, loss = 0.30589411\n",
      "Iteration 4, loss = 0.47116251\n",
      "Iteration 4, loss = 0.27286119\n",
      "Iteration 4, loss = 0.17020237\n",
      "Iteration 4, loss = 0.02204259\n",
      "Iteration 4, loss = 0.59083279\n",
      "Iteration 4, loss = 0.57314855\n",
      "Iteration 4, loss = 0.38186277\n",
      "Iteration 4, loss = 0.03044874\n",
      "Iteration 4, loss = 0.45835332\n",
      "Iteration 4, loss = 0.28100547\n",
      "Iteration 4, loss = 0.01692083\n",
      "Iteration 4, loss = 0.55718056\n",
      "Iteration 5, loss = 0.53873112\n",
      "Iteration 4, loss = 0.02463280\n",
      "Iteration 4, loss = 0.01756511\n",
      "Iteration 5, loss = 0.66307035\n",
      "Iteration 5, loss = 0.14690149\n",
      "Iteration 5, loss = 0.56502928\n",
      "Iteration 5, loss = 0.23687050\n",
      "Iteration 5, loss = 0.01093489\n",
      "Iteration 5, loss = 0.25909851\n",
      "Iteration 5, loss = 0.45738941\n",
      "Iteration 5, loss = 0.29351646\n",
      "Iteration 5, loss = 0.56680661\n",
      "Iteration 5, loss = 0.16142581\n",
      "Iteration 5, loss = 0.01549734\n",
      "Iteration 5, loss = 0.56738442\n",
      "Iteration 5, loss = 0.35744118\n",
      "Iteration 5, loss = 0.01038198\n",
      "Iteration 5, loss = 0.25710572\n",
      "Iteration 5, loss = 0.02291238\n",
      "Iteration 5, loss = 0.43409870\n",
      "Iteration 5, loss = 0.54985059\n",
      "Iteration 6, loss = 0.53442798\n",
      "Iteration 5, loss = 0.01680959\n",
      "Iteration 5, loss = 0.01276777\n",
      "Iteration 6, loss = 0.65037738\n",
      "Iteration 6, loss = 0.13904512\n",
      "Iteration 6, loss = 0.56018891\n",
      "Iteration 6, loss = 0.22619974\n",
      "Iteration 6, loss = 0.25169551\n",
      "Iteration 6, loss = 0.28384441\n",
      "Iteration 6, loss = 0.00872722\n",
      "Iteration 6, loss = 0.15276730\n",
      "Iteration 6, loss = 0.44712296\n",
      "Iteration 6, loss = 0.55206331\n",
      "Iteration 6, loss = 0.01262586\n",
      "Iteration 6, loss = 0.23964120\n",
      "Iteration 6, loss = 0.34044477\n",
      "Iteration 6, loss = 0.56341819\n",
      "Iteration 6, loss = 0.00707400\n",
      "Iteration 6, loss = 0.01820839\n",
      "Iteration 6, loss = 0.54406239\n",
      "Iteration 6, loss = 0.41649113\n",
      "Iteration 7, loss = 0.52677776\n",
      "Iteration 6, loss = 0.01195524\n",
      "Iteration 6, loss = 0.01402846\n",
      "Iteration 7, loss = 0.63872155\n",
      "Iteration 7, loss = 0.13111455\n",
      "Iteration 7, loss = 0.55486049\n",
      "Iteration 7, loss = 0.23968127\n",
      "Iteration 7, loss = 0.21719599\n",
      "Iteration 7, loss = 0.14382890\n",
      "Iteration 7, loss = 0.27574613\n",
      "Iteration 7, loss = 0.00721804\n",
      "Iteration 7, loss = 0.43720171\n",
      "Iteration 7, loss = 0.54067159\n",
      "Iteration 7, loss = 0.01198519\n",
      "Iteration 7, loss = 0.22651156\n",
      "Iteration 7, loss = 0.56052028\n",
      "Iteration 7, loss = 0.00574127\n",
      "Iteration 7, loss = 0.01323481\n",
      "Iteration 7, loss = 0.32583935\n",
      "Iteration 8, loss = 0.51996957\n",
      "Iteration 7, loss = 0.53865008\n",
      "Iteration 7, loss = 0.40263241\n",
      "Iteration 7, loss = 0.00715549\n",
      "Iteration 7, loss = 0.01001156\n",
      "Iteration 8, loss = 0.62893438\n",
      "Iteration 8, loss = 0.54829058\n",
      "Iteration 8, loss = 0.12342188\n",
      "Iteration 8, loss = 0.23194964\n",
      "Iteration 8, loss = 0.13756117\n",
      "Iteration 8, loss = 0.20602150\n",
      "Iteration 8, loss = 0.26645832\n",
      "Iteration 8, loss = 0.42988788\n",
      "Iteration 8, loss = 0.00613124\n",
      "Iteration 8, loss = 0.00828631\n",
      "Iteration 8, loss = 0.21725464\n",
      "Iteration 8, loss = 0.00571675\n",
      "Iteration 8, loss = 0.55734817\n",
      "Iteration 8, loss = 0.53226761\n",
      "Iteration 9, loss = 0.51325776\n",
      "Iteration 8, loss = 0.01079589\n",
      "Iteration 8, loss = 0.31678316Iteration 8, loss = 0.53437537\n",
      "\n",
      "Iteration 8, loss = 0.39039846\n",
      "Iteration 8, loss = 0.00586684\n",
      "Iteration 8, loss = 0.01014238\n",
      "Iteration 9, loss = 0.62103674\n",
      "Iteration 9, loss = 0.54178978\n",
      "Iteration 9, loss = 0.11596996\n",
      "Iteration 9, loss = 0.22529023\n",
      "Iteration 9, loss = 0.13189714\n",
      "Iteration 9, loss = 0.19719105\n",
      "Iteration 9, loss = 0.25855310\n",
      "Iteration 9, loss = 0.42183649\n",
      "Iteration 9, loss = 0.20685690\n",
      "Iteration 9, loss = 0.00511603\n",
      "Iteration 9, loss = 0.00478772\n",
      "Iteration 9, loss = 0.52431839\n",
      "Iteration 9, loss = 0.00664795\n",
      "Iteration 10, loss = 0.50907833\n",
      "Iteration 9, loss = 0.55415572\n",
      "Iteration 9, loss = 0.52750214\n",
      "Iteration 9, loss = 0.00961018\n",
      "Iteration 9, loss = 0.30614059\n",
      "Iteration 9, loss = 0.37899993\n",
      "Iteration 9, loss = 0.00534992\n",
      "Iteration 9, loss = 0.00639077\n",
      "Iteration 10, loss = 0.61466744\n",
      "Iteration 10, loss = 0.53350305\n",
      "Iteration 10, loss = 0.11088317\n",
      "Iteration 10, loss = 0.21784197\n",
      "Iteration 10, loss = 0.12663347\n",
      "Iteration 10, loss = 0.19079603\n",
      "Iteration 10, loss = 0.25217417\n",
      "Iteration 10, loss = 0.19841285\n",
      "Iteration 10, loss = 0.41606311\n",
      "Iteration 10, loss = 0.00847224\n",
      "Iteration 10, loss = 0.51849732\n",
      "Iteration 10, loss = 0.00428839\n",
      "Iteration 10, loss = 0.00433623\n",
      "Iteration 11, loss = 0.50547401\n",
      "Iteration 10, loss = 0.52249598\n",
      "Iteration 10, loss = 0.55166409\n",
      "Iteration 10, loss = 0.29838749\n",
      "Iteration 10, loss = 0.36942240\n",
      "Iteration 10, loss = 0.00794804\n",
      "Iteration 10, loss = 0.00502076\n",
      "Iteration 10, loss = 0.01101221\n",
      "Iteration 11, loss = 0.60754414\n",
      "Iteration 11, loss = 0.52531655\n",
      "Iteration 11, loss = 0.12140108\n",
      "Iteration 11, loss = 0.10403232\n",
      "Iteration 11, loss = 0.21297361\n",
      "Iteration 11, loss = 0.18453639\n",
      "Iteration 11, loss = 0.24597715\n",
      "Iteration 11, loss = 0.41124183\n",
      "Iteration 11, loss = 0.00577171\n",
      "Iteration 11, loss = 0.19237147\n",
      "Iteration 12, loss = 0.50110291\n",
      "Iteration 11, loss = 0.51184285\n",
      "Iteration 11, loss = 0.00217241\n",
      "Iteration 11, loss = 0.00585873\n",
      "Iteration 11, loss = 0.28940947\n",
      "Iteration 11, loss = 0.51866425\n",
      "Iteration 11, loss = 0.54797990\n",
      "Iteration 11, loss = 0.00652833\n",
      "Iteration 11, loss = 0.36086413\n",
      "Iteration 11, loss = 0.00352845\n",
      "Iteration 11, loss = 0.00620198\n",
      "Iteration 12, loss = 0.51806364\n",
      "Iteration 12, loss = 0.60120253\n",
      "Iteration 12, loss = 0.11841057\n",
      "Iteration 12, loss = 0.09887204\n",
      "Iteration 12, loss = 0.20700135\n",
      "Iteration 12, loss = 0.17781696\n",
      "Iteration 13, loss = 0.49788541\n",
      "Iteration 12, loss = 0.24083897\n",
      "Iteration 12, loss = 0.00512727\n",
      "Iteration 12, loss = 0.00553241\n",
      "Iteration 12, loss = 0.40736638\n",
      "Iteration 12, loss = 0.00222662\n",
      "Iteration 12, loss = 0.18733383\n",
      "Iteration 12, loss = 0.50733913\n",
      "Iteration 12, loss = 0.00622460\n",
      "Iteration 12, loss = 0.28293411\n",
      "Iteration 12, loss = 0.51359847\n",
      "Iteration 12, loss = 0.54375741\n",
      "Iteration 12, loss = 0.35341638\n",
      "Iteration 12, loss = 0.00458861\n",
      "Iteration 12, loss = 0.00559042\n",
      "Iteration 13, loss = 0.51093400\n",
      "Iteration 13, loss = 0.59433956\n",
      "Iteration 13, loss = 0.11372993\n",
      "Iteration 13, loss = 0.20130180\n",
      "Iteration 13, loss = 0.09462776\n",
      "Iteration 14, loss = 0.49450043\n",
      "Iteration 13, loss = 0.17296151\n",
      "Iteration 13, loss = 0.00138235\n",
      "Iteration 13, loss = 0.00525983\n",
      "Iteration 13, loss = 0.40266385\n",
      "Iteration 13, loss = 0.23547566\n",
      "Iteration 13, loss = 0.18075436\n",
      "Iteration 13, loss = 0.50277523\n",
      "Iteration 13, loss = 0.00385812\n",
      "Iteration 13, loss = 0.00552536\n",
      "Iteration 13, loss = 0.27958835\n",
      "Iteration 13, loss = 0.50938286\n",
      "Iteration 13, loss = 0.34634991\n",
      "Iteration 13, loss = 0.00279746\n",
      "Iteration 13, loss = 0.54069735\n",
      "Iteration 13, loss = 0.00530693\n",
      "Iteration 14, loss = 0.11007468\n",
      "Iteration 14, loss = 0.50579815\n",
      "Iteration 14, loss = 0.58864282\n",
      "Iteration 15, loss = 0.49137431\n",
      "Iteration 14, loss = 0.09015508\n",
      "Iteration 14, loss = 0.19785229\n",
      "Iteration 14, loss = 0.00273015\n",
      "Iteration 14, loss = 0.16807356\n",
      "Iteration 14, loss = 0.00415761\n",
      "Iteration 14, loss = 0.23142464\n",
      "Iteration 14, loss = 0.17419123\n",
      "Iteration 14, loss = 0.49894825\n",
      "Iteration 14, loss = 0.39808863\n",
      "Iteration 14, loss = 0.00288830\n",
      "Iteration 14, loss = 0.00449709\n",
      "Iteration 14, loss = 0.33850977\n",
      "Iteration 14, loss = 0.50556387\n",
      "Iteration 14, loss = 0.00282149\n",
      "Iteration 14, loss = 0.27073751\n",
      "Iteration 14, loss = 0.53690046\n",
      "Iteration 14, loss = 0.00573428\n",
      "Iteration 15, loss = 0.10577710\n",
      "Iteration 15, loss = 0.49899163\n",
      "Iteration 16, loss = 0.48830719\n",
      "Iteration 15, loss = 0.08482580\n",
      "Iteration 15, loss = 0.58342439\n",
      "Iteration 15, loss = 0.00317547\n",
      "Iteration 15, loss = 0.19253297\n",
      "Iteration 15, loss = 0.16484609\n",
      "Iteration 15, loss = 0.00590908\n",
      "Iteration 15, loss = 0.22704156\n",
      "Iteration 15, loss = 0.17022144\n",
      "Iteration 15, loss = 0.49381385\n",
      "Iteration 15, loss = 0.39483432\n",
      "Iteration 15, loss = 0.00372933\n",
      "Iteration 15, loss = 0.00392361\n",
      "Iteration 15, loss = 0.33408637\n",
      "Iteration 15, loss = 0.50092309\n",
      "Iteration 15, loss = 0.26388642\n",
      "Iteration 15, loss = 0.00361501\n",
      "Iteration 15, loss = 0.53472679\n",
      "Iteration 15, loss = 0.00506149\n",
      "Iteration 16, loss = 0.10190152\n",
      "Iteration 17, loss = 0.48543503\n",
      "Iteration 16, loss = 0.49641107\n",
      "Iteration 16, loss = 0.08137183\n",
      "Iteration 16, loss = 0.18925458\n",
      "Iteration 16, loss = 0.57801678\n",
      "Iteration 16, loss = 0.00195741\n",
      "Iteration 16, loss = 0.00342286\n",
      "Iteration 16, loss = 0.16087504\n",
      "Iteration 16, loss = 0.22302233\n",
      "Iteration 16, loss = 0.16662092\n",
      "Iteration 16, loss = 0.49121124\n",
      "Iteration 16, loss = 0.39002713\n",
      "Iteration 16, loss = 0.00259343\n",
      "Iteration 16, loss = 0.00378498\n",
      "Iteration 16, loss = 0.49801411\n",
      "Iteration 16, loss = 0.32794798\n",
      "Iteration 16, loss = 0.00392899\n",
      "Iteration 16, loss = 0.25810543\n",
      "Iteration 16, loss = 0.53182686\n",
      "Iteration 16, loss = 0.00326668\n",
      "Iteration 17, loss = 0.49126810\n",
      "Iteration 17, loss = 0.09963669\n",
      "Iteration 18, loss = 0.48313156\n",
      "Iteration 17, loss = 0.00192505\n",
      "Iteration 17, loss = 0.18464927\n",
      "Iteration 17, loss = 0.57293981\n",
      "Iteration 17, loss = 0.07995756\n",
      "Iteration 17, loss = 0.21765370\n",
      "Iteration 17, loss = 0.00341138\n",
      "Iteration 17, loss = 0.16266810\n",
      "Iteration 17, loss = 0.48576825\n",
      "Iteration 17, loss = 0.15590013\n",
      "Iteration 17, loss = 0.00429431\n",
      "Iteration 17, loss = 0.38675495\n",
      "Iteration 17, loss = 0.00362933\n",
      "Iteration 17, loss = 0.49444770\n",
      "Iteration 17, loss = 0.32081358\n",
      "Iteration 17, loss = 0.25424434\n",
      "Iteration 17, loss = 0.00098636\n",
      "Iteration 17, loss = 0.52811485\n",
      "Iteration 17, loss = 0.00840596\n",
      "Iteration 19, loss = 0.48048789\n",
      "Iteration 18, loss = 0.09500237\n",
      "Iteration 18, loss = 0.48717351\n",
      "Iteration 18, loss = 0.18153950\n",
      "Iteration 18, loss = 0.00401024\n",
      "Iteration 18, loss = 0.56756537\n",
      "Iteration 18, loss = 0.07580949\n",
      "Iteration 18, loss = 0.21468136\n",
      "Iteration 18, loss = 0.00373612\n",
      "Iteration 18, loss = 0.15868428\n",
      "Iteration 18, loss = 0.48281253\n",
      "Iteration 18, loss = 0.15354849\n",
      "Iteration 18, loss = 0.00093346\n",
      "Iteration 18, loss = 0.00276983\n",
      "Iteration 18, loss = 0.38353642\n",
      "Iteration 18, loss = 0.49046409\n",
      "Iteration 18, loss = 0.25073551\n",
      "Iteration 18, loss = 0.31698051\n",
      "Iteration 18, loss = 0.00371596\n",
      "Iteration 18, loss = 0.52534865\n",
      "Iteration 18, loss = 0.00391219\n",
      "Iteration 20, loss = 0.47938034\n",
      "Iteration 19, loss = 0.09157055\n",
      "Iteration 19, loss = 0.48502662\n",
      "Iteration 19, loss = 0.17773354\n",
      "Iteration 19, loss = 0.00100192\n",
      "Iteration 19, loss = 0.07283640\n",
      "Iteration 19, loss = 0.56329511Iteration 19, loss = 0.00315638\n",
      "\n",
      "Iteration 19, loss = 0.21090616\n",
      "Iteration 19, loss = 0.15450534\n",
      "Iteration 19, loss = 0.47877152\n",
      "Iteration 19, loss = 0.00427433\n",
      "Iteration 19, loss = 0.15264812\n",
      "Iteration 19, loss = 0.00159400\n",
      "Iteration 19, loss = 0.38035425\n",
      "Iteration 19, loss = 0.48788463\n",
      "Iteration 19, loss = 0.31179389\n",
      "Iteration 19, loss = 0.24593667\n",
      "Iteration 19, loss = 0.00201252\n",
      "Iteration 19, loss = 0.52327828\n",
      "Iteration 19, loss = 0.00320126\n",
      "Iteration 21, loss = 0.47683871\n",
      "Iteration 20, loss = 0.08926453\n",
      "Iteration 20, loss = 0.48335047\n",
      "Iteration 20, loss = 0.00331901\n",
      "Iteration 20, loss = 0.00225656\n",
      "Iteration 20, loss = 0.17478702\n",
      "Iteration 20, loss = 0.20765770\n",
      "Iteration 20, loss = 0.07150432\n",
      "Iteration 20, loss = 0.55993909\n",
      "Iteration 20, loss = 0.15241948\n",
      "Iteration 20, loss = 0.00437388\n",
      "Iteration 20, loss = 0.00096284\n",
      "Iteration 20, loss = 0.47526015\n",
      "Iteration 20, loss = 0.14758608\n",
      "Iteration 20, loss = 0.48531232\n",
      "Iteration 20, loss = 0.24302188\n",
      "Iteration 20, loss = 0.00184027\n",
      "Iteration 20, loss = 0.37756824\n",
      "Iteration 20, loss = 0.30604140\n",
      "Iteration 20, loss = 0.52016162\n",
      "Iteration 20, loss = 0.00188640\n",
      "Iteration 22, loss = 0.47337900\n",
      "Iteration 21, loss = 0.08543654\n",
      "Iteration 21, loss = 0.47867722\n",
      "Iteration 21, loss = 0.00256140\n",
      "Iteration 21, loss = 0.00332386\n",
      "Iteration 21, loss = 0.00372352\n",
      "Iteration 21, loss = 0.00188693\n",
      "Iteration 21, loss = 0.00202940\n",
      "Iteration 21, loss = 0.15087988\n",
      "Iteration 21, loss = 0.23870488\n",
      "Iteration 21, loss = 0.47144279\n",
      "Iteration 21, loss = 0.55570428\n",
      "Iteration 21, loss = 0.20535910\n",
      "Iteration 21, loss = 0.06809474\n",
      "Iteration 21, loss = 0.48279535\n",
      "Iteration 21, loss = 0.17203769\n",
      "Iteration 21, loss = 0.37369293\n",
      "Iteration 21, loss = 0.14422759\n",
      "Iteration 23, loss = 0.47246102\n",
      "Iteration 21, loss = 0.51883879\n",
      "Iteration 21, loss = 0.30130528\n",
      "Iteration 22, loss = 0.08356879\n",
      "Iteration 22, loss = 0.47685781\n",
      "Iteration 22, loss = 0.00340854\n",
      "Iteration 21, loss = 0.00318491\n",
      "Iteration 22, loss = 0.00190311\n",
      "Iteration 22, loss = 0.00236749\n",
      "Iteration 22, loss = 0.23767438\n",
      "Iteration 24, loss = 0.47082848\n",
      "Iteration 22, loss = 0.46891239\n",
      "Iteration 22, loss = 0.00338128\n",
      "Iteration 22, loss = 0.14627160\n",
      "Iteration 22, loss = 0.55199330\n",
      "Iteration 22, loss = 0.00075732\n",
      "Iteration 22, loss = 0.20258528\n",
      "Iteration 23, loss = 0.47370090\n",
      "Iteration 23, loss = 0.08134138\n",
      "Iteration 23, loss = 0.00229736\n",
      "Iteration 23, loss = 0.00097579\n",
      "Iteration 22, loss = 0.06574656\n",
      "Iteration 22, loss = 0.48001522\n",
      "Iteration 22, loss = 0.37137043\n",
      "Iteration 22, loss = 0.51650801\n",
      "Iteration 22, loss = 0.14150139\n",
      "Iteration 25, loss = 0.46896190\n",
      "Iteration 22, loss = 0.17001941\n",
      "Iteration 23, loss = 0.00133163\n",
      "Iteration 22, loss = 0.29693344\n",
      "Iteration 23, loss = 0.23286616\n",
      "Iteration 24, loss = 0.47219608\n",
      "Iteration 24, loss = 0.00263803\n",
      "Iteration 23, loss = 0.00322781\n",
      "Iteration 23, loss = 0.46583979\n",
      "Iteration 23, loss = 0.14432341\n",
      "Iteration 24, loss = 0.00250730\n",
      "Iteration 24, loss = 0.07945632\n",
      "Iteration 23, loss = 0.54788367\n",
      "Iteration 22, loss = 0.00399158\n",
      "Iteration 23, loss = 0.00151515\n",
      "Iteration 26, loss = 0.46776117\n",
      "Iteration 23, loss = 0.19988770\n",
      "Iteration 25, loss = 0.00066615\n",
      "Iteration 24, loss = 0.00384841\n",
      "Iteration 23, loss = 0.51489760\n",
      "Iteration 25, loss = 0.46952139\n",
      "Iteration 23, loss = 0.06442167\n",
      "Iteration 23, loss = 0.47773701\n",
      "Iteration 25, loss = 0.00204993\n",
      "Iteration 23, loss = 0.36829486\n",
      "Iteration 24, loss = 0.22975233\n",
      "Iteration 25, loss = 0.07594443\n",
      "Iteration 24, loss = 0.00215610\n",
      "Iteration 24, loss = 0.46288708\n",
      "Iteration 24, loss = 0.14121668\n",
      "Iteration 23, loss = 0.13934220\n",
      "Iteration 27, loss = 0.46565728\n",
      "Iteration 24, loss = 0.54516393\n",
      "Iteration 26, loss = 0.00026402\n",
      "Iteration 23, loss = 0.16515137\n",
      "Iteration 23, loss = 0.29262686\n",
      "Iteration 26, loss = 0.46787252\n",
      "Iteration 26, loss = 0.00302026\n",
      "Iteration 24, loss = 0.00157386\n",
      "Iteration 25, loss = 0.00200914\n",
      "Iteration 26, loss = 0.07593775\n",
      "Iteration 25, loss = 0.22614425\n",
      "Iteration 28, loss = 0.46326238\n",
      "Iteration 24, loss = 0.19685779\n",
      "Iteration 23, loss = 0.00350424\n",
      "Iteration 27, loss = 0.00023397\n",
      "Iteration 24, loss = 0.51271672\n",
      "Iteration 25, loss = 0.13953675\n",
      "Iteration 25, loss = 0.46080764\n",
      "Iteration 25, loss = 0.00079501\n",
      "Iteration 27, loss = 0.46466810\n",
      "Iteration 24, loss = 0.47530967\n",
      "Iteration 24, loss = 0.06173759\n",
      "Iteration 24, loss = 0.36555651\n",
      "Iteration 27, loss = 0.00259615\n",
      "Iteration 25, loss = 0.54161154\n",
      "Iteration 29, loss = 0.46197402\n",
      "Iteration 28, loss = 0.00022766\n",
      "Iteration 24, loss = 0.13716575\n",
      "Iteration 27, loss = 0.07276981\n",
      "Iteration 26, loss = 0.00152710\n",
      "Iteration 25, loss = 0.00161937\n",
      "Iteration 26, loss = 0.22363811\n",
      "Iteration 28, loss = 0.46222155\n",
      "Iteration 24, loss = 0.16333575\n",
      "Iteration 26, loss = 0.13668122\n",
      "Iteration 26, loss = 0.45660437\n",
      "Iteration 28, loss = 0.00162733\n",
      "Iteration 24, loss = 0.28847877\n",
      "Iteration 26, loss = 0.00043840\n",
      "Iteration 25, loss = 0.19398538\n",
      "Iteration 25, loss = 0.51079183\n",
      "Iteration 29, loss = 0.00022372\n",
      "Iteration 30, loss = 0.46054861\n",
      "Iteration 26, loss = 0.54124615\n",
      "Iteration 28, loss = 0.07046704\n",
      "Iteration 29, loss = 0.46038907\n",
      "Iteration 25, loss = 0.47319642\n",
      "Iteration 25, loss = 0.06072533\n",
      "Iteration 27, loss = 0.00229766\n",
      "Iteration 25, loss = 0.36200531\n",
      "Iteration 24, loss = 0.00245019\n",
      "Iteration 29, loss = 0.00216719\n",
      "Iteration 27, loss = 0.22100067\n",
      "Iteration 30, loss = 0.00022049\n",
      "Iteration 26, loss = 0.00218962\n",
      "Iteration 31, loss = 0.45957225\n",
      "Iteration 27, loss = 0.13374736\n",
      "Iteration 27, loss = 0.45383473\n",
      "Iteration 25, loss = 0.13418301\n",
      "Iteration 27, loss = 0.00403706\n",
      "Iteration 29, loss = 0.06867122\n",
      "Iteration 30, loss = 0.45848120\n",
      "Iteration 26, loss = 0.50886505\n",
      "Iteration 26, loss = 0.19080443\n",
      "Iteration 31, loss = 0.00021675\n",
      "Iteration 30, loss = 0.00162857\n",
      "Iteration 27, loss = 0.53671045\n",
      "Iteration 25, loss = 0.15932457\n",
      "Iteration 32, loss = 0.45844436\n",
      "Iteration 28, loss = 0.00188535\n",
      "Iteration 25, loss = 0.28171587\n",
      "Iteration 28, loss = 0.21767888\n",
      "Iteration 28, loss = 0.13317737\n",
      "Iteration 31, loss = 0.45596460\n",
      "Iteration 28, loss = 0.45179550\n",
      "Iteration 30, loss = 0.06864355\n",
      "Iteration 26, loss = 0.47125807\n",
      "Iteration 26, loss = 0.05899544\n",
      "Iteration 27, loss = 0.00095630\n",
      "Iteration 26, loss = 0.35923806\n",
      "Iteration 32, loss = 0.00021344\n",
      "Iteration 28, loss = 0.00131142\n",
      "Iteration 33, loss = 0.45702748\n",
      "Iteration 31, loss = 0.00190877\n",
      "Iteration 25, loss = 0.00250426\n",
      "Iteration 29, loss = 0.00231383\n",
      "Iteration 28, loss = 0.53530834\n",
      "Iteration 26, loss = 0.13359417\n",
      "Iteration 27, loss = 0.50736662\n",
      "Iteration 32, loss = 0.45681981\n",
      "Iteration 27, loss = 0.18931454\n",
      "Iteration 33, loss = 0.00020755\n",
      "Iteration 31, loss = 0.06635879\n",
      "Iteration 29, loss = 0.21602099\n",
      "Iteration 29, loss = 0.13016222\n",
      "Iteration 34, loss = 0.45453982\n",
      "Iteration 29, loss = 0.44943751\n",
      "Iteration 32, loss = 0.00227880\n",
      "Iteration 26, loss = 0.15798599\n",
      "Iteration 28, loss = 0.00252010\n",
      "Iteration 29, loss = 0.00185980\n",
      "Iteration 26, loss = 0.27954151\n",
      "Iteration 33, loss = 0.45254958\n",
      "Iteration 34, loss = 0.00020246\n",
      "Iteration 27, loss = 0.05672874\n",
      "Iteration 27, loss = 0.46891590\n",
      "Iteration 27, loss = 0.35781906\n",
      "Iteration 35, loss = 0.45405707\n",
      "Iteration 30, loss = 0.00026015\n",
      "Iteration 32, loss = 0.06433366\n",
      "Iteration 29, loss = 0.53117289\n",
      "Iteration 33, loss = 0.00054259\n",
      "Iteration 28, loss = 0.50563903\n",
      "Iteration 30, loss = 0.21335771\n",
      "Iteration 30, loss = 0.12671906\n",
      "Iteration 30, loss = 0.44668163\n",
      "Iteration 35, loss = 0.00540064\n",
      "Iteration 28, loss = 0.18638067\n",
      "Iteration 34, loss = 0.44927669\n",
      "Iteration 27, loss = 0.13072290\n",
      "Iteration 26, loss = 0.00350246\n",
      "Iteration 36, loss = 0.45294060\n",
      "Iteration 30, loss = 0.00052036\n",
      "Iteration 29, loss = 0.00133997\n",
      "Iteration 33, loss = 0.06332034\n",
      "Iteration 34, loss = 0.00040796\n",
      "Iteration 31, loss = 0.00022886\n",
      "Iteration 36, loss = 0.00118294\n",
      "Iteration 27, loss = 0.15577223\n",
      "Iteration 35, loss = 0.44695670\n",
      "Iteration 28, loss = 0.05447271\n",
      "Iteration 30, loss = 0.52867972\n",
      "Iteration 28, loss = 0.46882688\n",
      "Iteration 31, loss = 0.12474812\n",
      "Iteration 31, loss = 0.21157751\n",
      "Iteration 31, loss = 0.44535352\n",
      "Iteration 37, loss = 0.45246460\n",
      "Iteration 27, loss = 0.27606366\n",
      "Iteration 28, loss = 0.35561349\n",
      "Iteration 29, loss = 0.50406816\n",
      "Iteration 34, loss = 0.06011108\n",
      "Iteration 37, loss = 0.00023678\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 29, loss = 0.18416568\n",
      "Iteration 31, loss = 0.00345341\n",
      "Iteration 35, loss = 0.00413654\n",
      "Iteration 36, loss = 0.44700098\n",
      "Iteration 30, loss = 0.00031609\n",
      "Iteration 32, loss = 0.00022031\n",
      "Iteration 38, loss = 0.44980086\n",
      "Iteration 28, loss = 0.12993516\n",
      "Iteration 32, loss = 0.44355156\n",
      "Iteration 31, loss = 0.52810311\n",
      "Iteration 32, loss = 0.12504574\n",
      "Iteration 27, loss = 0.00036786\n",
      "Iteration 35, loss = 0.06178203\n",
      "Iteration 32, loss = 0.20930192\n",
      "Iteration 37, loss = 0.44539738\n",
      "Iteration 36, loss = 0.00102719\n",
      "Iteration 29, loss = 0.05446456\n",
      "Iteration 28, loss = 0.15415257\n",
      "Iteration 39, loss = 0.44864776\n",
      "Iteration 29, loss = 0.46676162\n",
      "Iteration 30, loss = 0.50295640\n",
      "Iteration 32, loss = 0.00177430\n",
      "Iteration 33, loss = 0.00021506\n",
      "Iteration 30, loss = 0.18107426\n",
      "Iteration 29, loss = 0.35311241\n",
      "Iteration 31, loss = 0.00023430\n",
      "Iteration 28, loss = 0.27277723\n",
      "Iteration 36, loss = 0.05812286\n",
      "Iteration 38, loss = 0.44587946\n",
      "Iteration 33, loss = 0.44171594\n",
      "Iteration 37, loss = 0.00270457\n",
      "Iteration 40, loss = 0.44821989\n",
      "Iteration 33, loss = 0.12135467\n",
      "Iteration 32, loss = 0.52429704\n",
      "Iteration 33, loss = 0.20709682\n",
      "Iteration 29, loss = 0.12762945\n",
      "Iteration 34, loss = 0.00021130\n",
      "Iteration 33, loss = 0.00029249\n",
      "Iteration 30, loss = 0.05273987\n",
      "Iteration 31, loss = 0.50116719\n",
      "Iteration 37, loss = 0.05693308\n",
      "Iteration 39, loss = 0.44148918\n",
      "Iteration 41, loss = 0.44614305\n",
      "Iteration 28, loss = 0.00046400\n",
      "Iteration 30, loss = 0.46314631\n",
      "Iteration 38, loss = 0.00073679\n",
      "Iteration 32, loss = 0.00022739\n",
      "Iteration 29, loss = 0.15095864\n",
      "Iteration 31, loss = 0.18126709\n",
      "Iteration 34, loss = 0.43907947\n",
      "Iteration 30, loss = 0.35153573\n",
      "Iteration 34, loss = 0.11997150\n",
      "Iteration 33, loss = 0.52360364\n",
      "Iteration 34, loss = 0.20551753\n",
      "Iteration 29, loss = 0.26675516\n",
      "Iteration 40, loss = 0.43935808\n",
      "Iteration 38, loss = 0.05566324\n",
      "Iteration 42, loss = 0.44630270\n",
      "Iteration 35, loss = 0.00020775\n",
      "Iteration 39, loss = 0.00138628\n",
      "Iteration 34, loss = 0.00031570\n",
      "Iteration 30, loss = 0.12536176\n",
      "Iteration 35, loss = 0.43852768\n",
      "Iteration 32, loss = 0.49967219\n",
      "Iteration 31, loss = 0.05201321\n",
      "Iteration 33, loss = 0.00022410\n",
      "Iteration 32, loss = 0.17715500\n",
      "Iteration 34, loss = 0.52180379\n",
      "Iteration 31, loss = 0.46248808\n",
      "Iteration 35, loss = 0.11911967\n",
      "Iteration 41, loss = 0.43715583\n",
      "Iteration 43, loss = 0.44622632\n",
      "Iteration 39, loss = 0.05506212\n",
      "Iteration 40, loss = 0.00111799\n",
      "Iteration 35, loss = 0.20367753\n",
      "Iteration 30, loss = 0.14891459\n",
      "Iteration 29, loss = 0.00427502\n",
      "Iteration 36, loss = 0.00020387\n",
      "Iteration 31, loss = 0.34849462\n",
      "Iteration 35, loss = 0.00438602\n",
      "Iteration 36, loss = 0.43529105\n",
      "Iteration 30, loss = 0.26455714\n",
      "Iteration 42, loss = 0.43768278\n",
      "Iteration 44, loss = 0.44399892\n",
      "Iteration 33, loss = 0.49823379\n",
      "Iteration 41, loss = 0.00231389\n",
      "Iteration 34, loss = 0.00022225\n",
      "Iteration 40, loss = 0.05329905\n",
      "Iteration 32, loss = 0.05012696\n",
      "Iteration 35, loss = 0.51847655\n",
      "Iteration 36, loss = 0.11789587\n",
      "Iteration 31, loss = 0.12640142\n",
      "Iteration 33, loss = 0.17612900\n",
      "Iteration 37, loss = 0.00019954\n",
      "Iteration 32, loss = 0.46116079\n",
      "Iteration 36, loss = 0.20037384\n",
      "Iteration 43, loss = 0.43602431\n",
      "Iteration 37, loss = 0.43463408\n",
      "Iteration 45, loss = 0.44282124\n",
      "Iteration 42, loss = 0.00166860\n",
      "Iteration 36, loss = 0.00036294\n",
      "Iteration 31, loss = 0.14654162\n",
      "Iteration 30, loss = 0.00103602\n",
      "Iteration 41, loss = 0.05160219\n",
      "Iteration 32, loss = 0.34594162\n",
      "Iteration 36, loss = 0.51658471\n",
      "Iteration 35, loss = 0.00021866\n",
      "Iteration 34, loss = 0.49640027\n",
      "Iteration 38, loss = 0.00650123\n",
      "Iteration 37, loss = 0.11445206\n",
      "Iteration 33, loss = 0.04855305\n",
      "Iteration 44, loss = 0.43404296\n",
      "Iteration 34, loss = 0.17497665\n",
      "Iteration 31, loss = 0.26134300\n",
      "Iteration 43, loss = 0.00142508\n",
      "Iteration 46, loss = 0.44212965\n",
      "Iteration 32, loss = 0.12086588\n",
      "Iteration 38, loss = 0.43244391\n",
      "Iteration 33, loss = 0.45875490\n",
      "Iteration 37, loss = 0.20122990\n",
      "Iteration 42, loss = 0.05120336\n",
      "Iteration 37, loss = 0.00108278\n",
      "Iteration 45, loss = 0.43168084\n",
      "Iteration 39, loss = 0.00062014\n",
      "Iteration 37, loss = 0.51348435\n",
      "Iteration 44, loss = 0.00147011\n",
      "Iteration 36, loss = 0.00021441\n",
      "Iteration 32, loss = 0.14553826\n",
      "Iteration 35, loss = 0.49514961\n",
      "Iteration 38, loss = 0.11256094\n",
      "Iteration 47, loss = 0.44052729\n",
      "Iteration 33, loss = 0.34353605\n",
      "Iteration 31, loss = 0.00575300\n",
      "Iteration 34, loss = 0.04866560\n",
      "Iteration 35, loss = 0.17322552\n",
      "Iteration 39, loss = 0.43183892\n",
      "Iteration 46, loss = 0.43119898\n",
      "Iteration 45, loss = 0.00029456\n",
      "Iteration 43, loss = 0.05066626\n",
      "Iteration 32, loss = 0.25738340\n",
      "Iteration 40, loss = 0.00026194\n",
      "Iteration 38, loss = 0.19794721\n",
      "Iteration 34, loss = 0.45740685\n",
      "Iteration 38, loss = 0.51295605\n",
      "Iteration 33, loss = 0.12130914\n",
      "Iteration 38, loss = 0.00273386\n",
      "Iteration 48, loss = 0.44112884\n",
      "Iteration 37, loss = 0.00621514\n",
      "Iteration 39, loss = 0.11141781\n",
      "Iteration 36, loss = 0.49328900\n",
      "Iteration 47, loss = 0.43087666\n",
      "Iteration 46, loss = 0.00026723\n",
      "Iteration 35, loss = 0.04783361\n",
      "Iteration 33, loss = 0.14230061\n",
      "Iteration 36, loss = 0.17178070\n",
      "Iteration 40, loss = 0.43038166\n",
      "Iteration 34, loss = 0.34380464\n",
      "Iteration 41, loss = 0.00022243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.00068692\n",
      "Iteration 44, loss = 0.04902129\n",
      "Iteration 49, loss = 0.43875121\n",
      "Iteration 39, loss = 0.51089503\n",
      "Iteration 48, loss = 0.42892047\n",
      "Iteration 47, loss = 0.00026300\n",
      "Iteration 35, loss = 0.45581438\n",
      "Iteration 39, loss = 0.19572419\n",
      "Iteration 38, loss = 0.00040752\n",
      "Iteration 39, loss = 0.00096438\n",
      "Iteration 40, loss = 0.10861774\n",
      "Iteration 34, loss = 0.11858692\n",
      "Iteration 33, loss = 0.25512808\n",
      "Iteration 37, loss = 0.49310998\n",
      "Iteration 36, loss = 0.04732293\n",
      "Iteration 37, loss = 0.16857520\n",
      "Iteration 49, loss = 0.42678576\n",
      "Iteration 48, loss = 0.00025998\n",
      "Iteration 50, loss = 0.43735106\n",
      "Iteration 41, loss = 0.42766199\n",
      "Iteration 45, loss = 0.04748990\n",
      "Iteration 40, loss = 0.50939419\n",
      "Iteration 34, loss = 0.14132888\n",
      "Iteration 35, loss = 0.34133378\n",
      "Iteration 33, loss = 0.00290694\n",
      "Iteration 39, loss = 0.00025149\n",
      "Iteration 36, loss = 0.45412441\n",
      "Iteration 41, loss = 0.10827507\n",
      "Iteration 49, loss = 0.00025724\n",
      "Iteration 50, loss = 0.42445013\n",
      "Iteration 40, loss = 0.00031979\n",
      "Iteration 40, loss = 0.19456420\n",
      "Iteration 38, loss = 0.49073961\n",
      "Iteration 51, loss = 0.43651307\n",
      "Iteration 37, loss = 0.04475818\n",
      "Iteration 35, loss = 0.11673081\n",
      "Iteration 38, loss = 0.16693980\n",
      "Iteration 41, loss = 0.50797530\n",
      "Iteration 34, loss = 0.25214516\n",
      "Iteration 50, loss = 0.00025423\n",
      "Iteration 42, loss = 0.42651170\n",
      "Iteration 51, loss = 0.42384643\n",
      "Iteration 46, loss = 0.04757028\n",
      "Iteration 40, loss = 0.00023502\n",
      "Iteration 35, loss = 0.13798814\n",
      "Iteration 42, loss = 0.10842792\n",
      "Iteration 52, loss = 0.43602203\n",
      "Iteration 36, loss = 0.33901024\n",
      "Iteration 37, loss = 0.45406608\n",
      "Iteration 34, loss = 0.00194534\n",
      "Iteration 51, loss = 0.00025050\n",
      "Iteration 39, loss = 0.48900101\n",
      "Iteration 41, loss = 0.00025662\n",
      "Iteration 52, loss = 0.42428158\n",
      "Iteration 41, loss = 0.19330149\n",
      "Iteration 38, loss = 0.04493487\n",
      "Iteration 42, loss = 0.50371104\n",
      "Iteration 39, loss = 0.16661844\n",
      "Iteration 36, loss = 0.11690051\n",
      "Iteration 52, loss = 0.00024601\n",
      "Iteration 43, loss = 0.42594214\n",
      "Iteration 41, loss = 0.00022760\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 53, loss = 0.43562363\n",
      "Iteration 53, loss = 0.42193771\n",
      "Iteration 47, loss = 0.04679792\n",
      "Iteration 43, loss = 0.10607681\n",
      "Iteration 35, loss = 0.24753020\n",
      "Iteration 38, loss = 0.45133045\n",
      "Iteration 36, loss = 0.13577857\n",
      "Iteration 40, loss = 0.48831054\n",
      "Iteration 43, loss = 0.50520849\n",
      "Iteration 39, loss = 0.04325937\n",
      "Iteration 53, loss = 0.00024069\n",
      "Iteration 37, loss = 0.33634241\n",
      "Iteration 35, loss = 0.00105919\n",
      "Iteration 42, loss = 0.00025200\n",
      "Iteration 54, loss = 0.42006164\n",
      "Iteration 40, loss = 0.16443421\n",
      "Iteration 42, loss = 0.18949154\n",
      "Iteration 54, loss = 0.43413485\n",
      "Iteration 54, loss = 0.00545942\n",
      "Iteration 44, loss = 0.10425833\n",
      "Iteration 44, loss = 0.42458943\n",
      "Iteration 37, loss = 0.11564931\n",
      "Iteration 55, loss = 0.41845208\n",
      "Iteration 48, loss = 0.04563200\n",
      "Iteration 44, loss = 0.50270819\n",
      "Iteration 41, loss = 0.48716982\n",
      "Iteration 39, loss = 0.45060324\n",
      "Iteration 40, loss = 0.04226336\n",
      "Iteration 55, loss = 0.43273203\n",
      "Iteration 55, loss = 0.00067800\n",
      "Iteration 36, loss = 0.24707800\n",
      "Iteration 37, loss = 0.13460886\n",
      "Iteration 41, loss = 0.16243418\n",
      "Iteration 56, loss = 0.41852511\n",
      "Iteration 43, loss = 0.00024738\n",
      "Iteration 36, loss = 0.00294673\n",
      "Iteration 43, loss = 0.18842504\n",
      "Iteration 38, loss = 0.33630106\n",
      "Iteration 45, loss = 0.10189305\n",
      "Iteration 45, loss = 0.50090398\n",
      "Iteration 56, loss = 0.00030983\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 45, loss = 0.42331870\n",
      "Iteration 57, loss = 0.41615301\n",
      "Iteration 49, loss = 0.04370126\n",
      "Iteration 41, loss = 0.04098665\n",
      "Iteration 42, loss = 0.48656773\n",
      "Iteration 56, loss = 0.43228463\n",
      "Iteration 38, loss = 0.11471661\n",
      "Iteration 40, loss = 0.44986266\n",
      "Iteration 42, loss = 0.16218706\n",
      "Iteration 58, loss = 0.41567314\n",
      "Iteration 46, loss = 0.10033355\n",
      "Iteration 38, loss = 0.13365512\n",
      "Iteration 44, loss = 0.00024311\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 46, loss = 0.49962539\n",
      "Iteration 44, loss = 0.18754037\n",
      "Iteration 37, loss = 0.00137549\n",
      "Iteration 37, loss = 0.24268392\n",
      "Iteration 57, loss = 0.43135618\n",
      "Iteration 39, loss = 0.33388484\n",
      "Iteration 42, loss = 0.03967681\n",
      "Iteration 43, loss = 0.48569402\n",
      "Iteration 46, loss = 0.42198613\n",
      "Iteration 59, loss = 0.41481759\n",
      "Iteration 41, loss = 0.44868494\n",
      "Iteration 50, loss = 0.04399306\n",
      "Iteration 43, loss = 0.16118494\n",
      "Iteration 39, loss = 0.11353412\n",
      "Iteration 47, loss = 0.49696136\n",
      "Iteration 47, loss = 0.10043946\n",
      "Iteration 58, loss = 0.43071018\n",
      "Iteration 60, loss = 0.41426747\n",
      "Iteration 45, loss = 0.18674905\n",
      "Iteration 43, loss = 0.04031827\n",
      "Iteration 38, loss = 0.00249623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 39, loss = 0.13241383\n",
      "Iteration 44, loss = 0.48393626\n",
      "Iteration 61, loss = 0.41253278\n",
      "Iteration 42, loss = 0.44641797\n",
      "Iteration 38, loss = 0.24115341\n",
      "Iteration 47, loss = 0.42127471\n",
      "Iteration 51, loss = 0.04351782\n",
      "Iteration 48, loss = 0.49520033\n",
      "Iteration 40, loss = 0.33150422\n",
      "Iteration 59, loss = 0.42985807\n",
      "Iteration 44, loss = 0.15878699\n",
      "Iteration 48, loss = 0.09904227\n",
      "Iteration 62, loss = 0.41099568\n",
      "Iteration 44, loss = 0.03929452\n",
      "Iteration 40, loss = 0.11246305\n",
      "Iteration 45, loss = 0.48360188\n",
      "Iteration 46, loss = 0.18375849\n",
      "Iteration 49, loss = 0.49525665\n",
      "Iteration 43, loss = 0.44688802\n",
      "Iteration 40, loss = 0.13073826\n",
      "Iteration 60, loss = 0.42869877\n",
      "Iteration 63, loss = 0.41100787\n",
      "Iteration 48, loss = 0.41962220\n",
      "Iteration 49, loss = 0.09730561\n",
      "Iteration 52, loss = 0.04331697\n",
      "Iteration 45, loss = 0.15639968\n",
      "Iteration 45, loss = 0.03832607\n",
      "Iteration 39, loss = 0.23870579\n",
      "Iteration 64, loss = 0.40769296\n",
      "Iteration 41, loss = 0.33054630\n",
      "Iteration 46, loss = 0.48108631\n",
      "Iteration 50, loss = 0.49319491\n",
      "Iteration 61, loss = 0.42877614\n",
      "Iteration 47, loss = 0.18261094\n",
      "Iteration 41, loss = 0.11006203\n",
      "Iteration 44, loss = 0.44702544\n",
      "Iteration 65, loss = 0.40867744\n",
      "Iteration 50, loss = 0.09649886\n",
      "Iteration 46, loss = 0.15778404\n",
      "Iteration 41, loss = 0.13031809\n",
      "Iteration 46, loss = 0.03933762\n",
      "Iteration 53, loss = 0.04154346\n",
      "Iteration 49, loss = 0.41764629\n",
      "Iteration 47, loss = 0.48135087\n",
      "Iteration 51, loss = 0.49032358\n",
      "Iteration 62, loss = 0.42731082\n",
      "Iteration 66, loss = 0.40611574\n",
      "Iteration 40, loss = 0.23601581\n",
      "Iteration 42, loss = 0.32877234\n",
      "Iteration 48, loss = 0.18077738\n",
      "Iteration 51, loss = 0.09593112\n",
      "Iteration 45, loss = 0.44349231\n",
      "Iteration 67, loss = 0.40795028\n",
      "Iteration 47, loss = 0.03629928\n",
      "Iteration 47, loss = 0.15428434\n",
      "Iteration 42, loss = 0.10843434\n",
      "Iteration 63, loss = 0.42655381\n",
      "Iteration 52, loss = 0.49039838\n",
      "Iteration 48, loss = 0.47995645\n",
      "Iteration 54, loss = 0.04045346\n",
      "Iteration 50, loss = 0.41732150\n",
      "Iteration 42, loss = 0.12835901\n",
      "Iteration 68, loss = 0.40404603\n",
      "Iteration 52, loss = 0.09550696\n",
      "Iteration 48, loss = 0.03565450\n",
      "Iteration 49, loss = 0.18018339\n",
      "Iteration 64, loss = 0.42496102\n",
      "Iteration 69, loss = 0.40586110\n",
      "Iteration 46, loss = 0.44315637\n",
      "Iteration 53, loss = 0.48988953\n",
      "Iteration 48, loss = 0.15277891\n",
      "Iteration 49, loss = 0.47897358\n",
      "Iteration 43, loss = 0.32674428\n",
      "Iteration 41, loss = 0.23315762\n",
      "Iteration 55, loss = 0.04015915\n",
      "Iteration 51, loss = 0.41547702\n",
      "Iteration 43, loss = 0.11020489\n",
      "Iteration 70, loss = 0.40340587\n",
      "Iteration 43, loss = 0.12619949\n",
      "Iteration 49, loss = 0.03567127\n",
      "Iteration 53, loss = 0.09329358\n",
      "Iteration 65, loss = 0.42557958\n",
      "Iteration 54, loss = 0.48634394\n",
      "Iteration 50, loss = 0.47799029\n",
      "Iteration 71, loss = 0.40192046\n",
      "Iteration 49, loss = 0.15028330\n",
      "Iteration 50, loss = 0.18031053\n",
      "Iteration 47, loss = 0.44067552\n",
      "Iteration 56, loss = 0.03998316\n",
      "Iteration 72, loss = 0.40044138\n",
      "Iteration 66, loss = 0.42360116\n",
      "Iteration 52, loss = 0.41515645\n",
      "Iteration 50, loss = 0.03493971\n",
      "Iteration 55, loss = 0.48555358\n",
      "Iteration 54, loss = 0.09237610\n",
      "Iteration 44, loss = 0.32635326\n",
      "Iteration 44, loss = 0.10668308\n",
      "Iteration 51, loss = 0.47741020\n",
      "Iteration 44, loss = 0.12400744\n",
      "Iteration 42, loss = 0.23192100\n",
      "Iteration 50, loss = 0.15116429\n",
      "Iteration 73, loss = 0.40198234\n",
      "Iteration 51, loss = 0.17974155\n",
      "Iteration 48, loss = 0.44070315\n",
      "Iteration 67, loss = 0.42367204\n",
      "Iteration 56, loss = 0.48395750\n",
      "Iteration 51, loss = 0.03314320\n",
      "Iteration 74, loss = 0.40002175\n",
      "Iteration 57, loss = 0.03872266\n",
      "Iteration 55, loss = 0.09226139\n",
      "Iteration 53, loss = 0.41414115\n",
      "Iteration 52, loss = 0.47764402\n",
      "Iteration 51, loss = 0.15121906\n",
      "Iteration 75, loss = 0.39974499\n",
      "Iteration 68, loss = 0.42320040\n",
      "Iteration 45, loss = 0.12363166\n",
      "Iteration 57, loss = 0.48301826\n",
      "Iteration 45, loss = 0.10627169\n",
      "Iteration 45, loss = 0.32351957\n",
      "Iteration 52, loss = 0.17619778\n",
      "Iteration 52, loss = 0.03376426\n",
      "Iteration 49, loss = 0.43915220\n",
      "Iteration 56, loss = 0.09014364\n",
      "Iteration 43, loss = 0.22797668\n",
      "Iteration 76, loss = 0.39740558\n",
      "Iteration 53, loss = 0.47437544\n",
      "Iteration 58, loss = 0.03767424\n",
      "Iteration 54, loss = 0.41363000\n",
      "Iteration 69, loss = 0.42172574\n",
      "Iteration 58, loss = 0.48291832\n",
      "Iteration 52, loss = 0.14839448\n",
      "Iteration 77, loss = 0.39619547\n",
      "Iteration 53, loss = 0.03246004\n",
      "Iteration 53, loss = 0.17966725\n",
      "Iteration 46, loss = 0.12249848\n",
      "Iteration 50, loss = 0.43788119\n",
      "Iteration 57, loss = 0.08898363\n",
      "Iteration 54, loss = 0.47399278\n",
      "Iteration 46, loss = 0.10463724\n",
      "Iteration 78, loss = 0.39769488\n",
      "Iteration 46, loss = 0.32256727\n",
      "Iteration 70, loss = 0.42130300\n",
      "Iteration 59, loss = 0.03816214\n",
      "Iteration 59, loss = 0.48098509\n",
      "Iteration 55, loss = 0.41252136\n",
      "Iteration 54, loss = 0.03178505\n",
      "Iteration 53, loss = 0.14857444\n",
      "Iteration 44, loss = 0.22842847\n",
      "Iteration 79, loss = 0.39527884\n",
      "Iteration 58, loss = 0.08912619\n",
      "Iteration 55, loss = 0.47315510\n",
      "Iteration 54, loss = 0.17402619\n",
      "Iteration 71, loss = 0.42100999\n",
      "Iteration 51, loss = 0.43771068\n",
      "Iteration 60, loss = 0.47952411\n",
      "Iteration 47, loss = 0.12190194\n",
      "Iteration 80, loss = 0.39595051\n",
      "Iteration 60, loss = 0.03717553\n",
      "Iteration 55, loss = 0.03171604\n",
      "Iteration 47, loss = 0.10385309\n",
      "Iteration 54, loss = 0.14642844\n",
      "Iteration 56, loss = 0.41227420\n",
      "Iteration 47, loss = 0.32169232\n",
      "Iteration 81, loss = 0.39465487\n",
      "Iteration 72, loss = 0.41953665\n",
      "Iteration 56, loss = 0.47210943\n",
      "Iteration 59, loss = 0.08887903\n",
      "Iteration 61, loss = 0.47878990\n",
      "Iteration 55, loss = 0.17368157\n",
      "Iteration 56, loss = 0.02980740\n",
      "Iteration 82, loss = 0.39373254\n",
      "Iteration 45, loss = 0.22254773\n",
      "Iteration 52, loss = 0.43713722\n",
      "Iteration 48, loss = 0.12045807\n",
      "Iteration 61, loss = 0.03707690\n",
      "Iteration 55, loss = 0.14513720\n",
      "Iteration 73, loss = 0.41910894\n",
      "Iteration 57, loss = 0.47153951\n",
      "Iteration 83, loss = 0.39458808\n",
      "Iteration 57, loss = 0.41208939\n",
      "Iteration 62, loss = 0.47575707\n",
      "Iteration 60, loss = 0.08781969\n",
      "Iteration 48, loss = 0.10286655\n",
      "Iteration 57, loss = 0.03031446\n",
      "Iteration 48, loss = 0.32066592\n",
      "Iteration 56, loss = 0.17095618\n",
      "Iteration 84, loss = 0.39149412\n",
      "Iteration 74, loss = 0.41883907\n",
      "Iteration 62, loss = 0.03669321\n",
      "Iteration 63, loss = 0.47550362\n",
      "Iteration 56, loss = 0.14393417\n",
      "Iteration 58, loss = 0.47134056\n",
      "Iteration 53, loss = 0.43658172\n",
      "Iteration 49, loss = 0.12147984\n",
      "Iteration 85, loss = 0.39077869\n",
      "Iteration 61, loss = 0.08863712\n",
      "Iteration 58, loss = 0.03021853\n",
      "Iteration 58, loss = 0.41003885\n",
      "Iteration 46, loss = 0.22112526\n",
      "Iteration 75, loss = 0.41848109\n",
      "Iteration 49, loss = 0.10227352\n",
      "Iteration 86, loss = 0.38963352\n",
      "Iteration 57, loss = 0.17036524\n",
      "Iteration 64, loss = 0.47624661\n",
      "Iteration 59, loss = 0.46965863\n",
      "Iteration 63, loss = 0.03549457\n",
      "Iteration 57, loss = 0.14363550\n",
      "Iteration 49, loss = 0.31933917\n",
      "Iteration 59, loss = 0.02896946\n",
      "Iteration 87, loss = 0.39077203\n",
      "Iteration 62, loss = 0.08482192\n",
      "Iteration 54, loss = 0.43392801\n",
      "Iteration 50, loss = 0.11998694\n",
      "Iteration 76, loss = 0.41851931\n",
      "Iteration 59, loss = 0.40818390\n",
      "Iteration 65, loss = 0.47370434\n",
      "Iteration 88, loss = 0.38885569\n",
      "Iteration 60, loss = 0.46862888\n",
      "Iteration 58, loss = 0.16999931\n",
      "Iteration 60, loss = 0.02979986\n",
      "Iteration 58, loss = 0.14170196\n",
      "Iteration 50, loss = 0.10542299\n",
      "Iteration 64, loss = 0.03680092\n",
      "Iteration 47, loss = 0.21921901\n",
      "Iteration 77, loss = 0.41612914\n",
      "Iteration 89, loss = 0.38906634\n",
      "Iteration 63, loss = 0.08443101\n",
      "Iteration 66, loss = 0.47194598\n",
      "Iteration 60, loss = 0.40788419\n",
      "Iteration 55, loss = 0.43392376\n",
      "Iteration 50, loss = 0.31748329\n",
      "Iteration 51, loss = 0.11594081\n",
      "Iteration 61, loss = 0.46748329\n",
      "Iteration 90, loss = 0.38809611\n",
      "Iteration 61, loss = 0.02932830\n",
      "Iteration 59, loss = 0.17008865\n",
      "Iteration 59, loss = 0.14203758\n",
      "Iteration 78, loss = 0.41643682\n",
      "Iteration 67, loss = 0.47245890\n",
      "Iteration 65, loss = 0.03343333\n",
      "Iteration 91, loss = 0.38893254\n",
      "Iteration 64, loss = 0.08389725\n",
      "Iteration 51, loss = 0.09922834\n",
      "Iteration 62, loss = 0.46978314\n",
      "Iteration 62, loss = 0.02766024\n",
      "Iteration 61, loss = 0.40635104\n",
      "Iteration 92, loss = 0.38745933\n",
      "Iteration 79, loss = 0.41540034\n",
      "Iteration 48, loss = 0.21837521\n",
      "Iteration 56, loss = 0.43122360\n",
      "Iteration 52, loss = 0.11609271\n",
      "Iteration 68, loss = 0.47067349\n",
      "Iteration 60, loss = 0.13942964\n",
      "Iteration 60, loss = 0.16774645\n",
      "Iteration 51, loss = 0.31627801\n",
      "Iteration 66, loss = 0.03419385\n",
      "Iteration 93, loss = 0.38526352\n",
      "Iteration 65, loss = 0.08243996\n",
      "Iteration 63, loss = 0.46626464\n",
      "Iteration 63, loss = 0.02666750\n",
      "Iteration 80, loss = 0.41558386\n",
      "Iteration 52, loss = 0.09921874\n",
      "Iteration 69, loss = 0.46987759\n",
      "Iteration 94, loss = 0.38497034\n",
      "Iteration 62, loss = 0.40765385\n",
      "Iteration 61, loss = 0.13887964\n",
      "Iteration 57, loss = 0.43162728\n",
      "Iteration 61, loss = 0.16663827\n",
      "Iteration 53, loss = 0.11455818\n",
      "Iteration 95, loss = 0.38615193\n",
      "Iteration 64, loss = 0.46699890\n",
      "Iteration 67, loss = 0.03381791\n",
      "Iteration 64, loss = 0.02699583\n",
      "Iteration 81, loss = 0.41368920\n",
      "Iteration 66, loss = 0.08585561\n",
      "Iteration 49, loss = 0.21603771\n",
      "Iteration 70, loss = 0.46912328\n",
      "Iteration 52, loss = 0.31381546\n",
      "Iteration 96, loss = 0.38621765\n",
      "Iteration 63, loss = 0.40513570\n",
      "Iteration 62, loss = 0.13895295\n",
      "Iteration 53, loss = 0.09764246\n",
      "Iteration 82, loss = 0.41270696\n",
      "Iteration 65, loss = 0.46519899\n",
      "Iteration 65, loss = 0.02671376\n",
      "Iteration 62, loss = 0.16749003\n",
      "Iteration 71, loss = 0.46701468\n",
      "Iteration 97, loss = 0.38436130\n",
      "Iteration 68, loss = 0.03443579\n",
      "Iteration 58, loss = 0.43132501\n",
      "Iteration 54, loss = 0.11299710\n",
      "Iteration 67, loss = 0.08094490\n",
      "Iteration 98, loss = 0.38238303\n",
      "Iteration 83, loss = 0.41412955\n",
      "Iteration 63, loss = 0.13654908\n",
      "Iteration 66, loss = 0.02706283\n",
      "Iteration 64, loss = 0.40450145\n",
      "Iteration 66, loss = 0.46441482\n",
      "Iteration 72, loss = 0.46623931\n",
      "Iteration 53, loss = 0.31431478\n",
      "Iteration 50, loss = 0.21156199\n",
      "Iteration 63, loss = 0.16331217\n",
      "Iteration 99, loss = 0.38088325\n",
      "Iteration 54, loss = 0.09964657\n",
      "Iteration 69, loss = 0.03232509\n",
      "Iteration 68, loss = 0.08140894\n",
      "Iteration 55, loss = 0.11252977\n",
      "Iteration 59, loss = 0.42994675\n",
      "Iteration 84, loss = 0.41256023\n",
      "Iteration 100, loss = 0.38246180\n",
      "Iteration 73, loss = 0.46542819\n",
      "Iteration 67, loss = 0.02562973\n",
      "Iteration 67, loss = 0.46402052\n",
      "Iteration 64, loss = 0.13693275\n",
      "Iteration 65, loss = 0.40391504\n",
      "Iteration 101, loss = 0.38227682\n",
      "Iteration 64, loss = 0.16407891\n",
      "Iteration 85, loss = 0.41123534\n",
      "Iteration 70, loss = 0.03162787\n",
      "Iteration 54, loss = 0.31157542\n",
      "Iteration 74, loss = 0.46503614\n",
      "Iteration 68, loss = 0.02432819\n",
      "Iteration 69, loss = 0.08041260\n",
      "Iteration 68, loss = 0.46382014\n",
      "Iteration 55, loss = 0.09710510\n",
      "Iteration 102, loss = 0.38277347\n",
      "Iteration 56, loss = 0.11390019\n",
      "Iteration 51, loss = 0.21219282\n",
      "Iteration 60, loss = 0.42878652\n",
      "Iteration 65, loss = 0.13546475\n",
      "Iteration 66, loss = 0.40285577\n",
      "Iteration 86, loss = 0.41120357\n",
      "Iteration 103, loss = 0.38132961\n",
      "Iteration 75, loss = 0.46486676\n",
      "Iteration 65, loss = 0.16321662\n",
      "Iteration 71, loss = 0.03128825\n",
      "Iteration 69, loss = 0.02397475\n",
      "Iteration 69, loss = 0.46306595\n",
      "Iteration 104, loss = 0.38084690\n",
      "Iteration 70, loss = 0.08007805\n",
      "Iteration 66, loss = 0.13329371\n",
      "Iteration 87, loss = 0.41029615\n",
      "Iteration 57, loss = 0.11078615\n",
      "Iteration 56, loss = 0.09518522\n",
      "Iteration 55, loss = 0.31121717\n",
      "Iteration 61, loss = 0.42724166\n",
      "Iteration 76, loss = 0.46103633\n",
      "Iteration 67, loss = 0.40395420\n",
      "Iteration 105, loss = 0.37999280\n",
      "Iteration 70, loss = 0.02659087\n",
      "Iteration 70, loss = 0.46212009\n",
      "Iteration 52, loss = 0.20922349\n",
      "Iteration 66, loss = 0.16232020\n",
      "Iteration 72, loss = 0.03125935\n",
      "Iteration 88, loss = 0.40992071\n",
      "Iteration 106, loss = 0.37889836\n",
      "Iteration 67, loss = 0.13378883\n",
      "Iteration 71, loss = 0.07991999\n",
      "Iteration 77, loss = 0.46119930\n",
      "Iteration 58, loss = 0.10982976\n",
      "Iteration 107, loss = 0.37909493\n",
      "Iteration 71, loss = 0.46150931\n",
      "Iteration 71, loss = 0.02406950\n",
      "Iteration 68, loss = 0.40275259\n",
      "Iteration 57, loss = 0.09457642\n",
      "Iteration 62, loss = 0.42783818\n",
      "Iteration 89, loss = 0.40993504\n",
      "Iteration 56, loss = 0.30935225\n",
      "Iteration 73, loss = 0.03067282\n",
      "Iteration 67, loss = 0.16155291\n",
      "Iteration 78, loss = 0.46116535\n",
      "Iteration 108, loss = 0.37923447\n",
      "Iteration 68, loss = 0.13387357\n",
      "Iteration 72, loss = 0.07751391\n",
      "Iteration 72, loss = 0.46090061\n",
      "Iteration 53, loss = 0.20713318\n",
      "Iteration 72, loss = 0.02374524\n",
      "Iteration 90, loss = 0.40796867\n",
      "Iteration 109, loss = 0.38186541\n",
      "Iteration 79, loss = 0.45953487\n",
      "Iteration 69, loss = 0.40040080\n",
      "Iteration 59, loss = 0.11008307\n",
      "Iteration 63, loss = 0.42659027\n",
      "Iteration 74, loss = 0.03168442\n",
      "Iteration 68, loss = 0.15989441\n",
      "Iteration 58, loss = 0.09556075\n",
      "Iteration 110, loss = 0.37685296\n",
      "Iteration 69, loss = 0.13265097\n",
      "Iteration 57, loss = 0.30967644\n",
      "Iteration 91, loss = 0.40740694Iteration 73, loss = 0.45982636\n",
      "\n",
      "Iteration 73, loss = 0.07709918\n",
      "Iteration 73, loss = 0.02472043\n",
      "Iteration 80, loss = 0.46108437\n",
      "Iteration 111, loss = 0.37618694\n",
      "Iteration 70, loss = 0.40084557\n",
      "Iteration 75, loss = 0.02922152\n",
      "Iteration 60, loss = 0.10901896\n",
      "Iteration 70, loss = 0.13190044\n",
      "Iteration 69, loss = 0.16049619\n",
      "Iteration 112, loss = 0.37593690\n",
      "Iteration 92, loss = 0.40874056\n",
      "Iteration 54, loss = 0.20489516\n",
      "Iteration 74, loss = 0.46098196\n",
      "Iteration 64, loss = 0.42640156\n",
      "Iteration 81, loss = 0.45702887\n",
      "Iteration 59, loss = 0.09648112\n",
      "Iteration 74, loss = 0.02277366\n",
      "Iteration 113, loss = 0.37682752\n",
      "Iteration 74, loss = 0.08018733\n",
      "Iteration 58, loss = 0.30717056\n",
      "Iteration 93, loss = 0.40778351\n",
      "Iteration 71, loss = 0.40067753\n",
      "Iteration 71, loss = 0.13031849\n",
      "Iteration 76, loss = 0.02983243\n",
      "Iteration 75, loss = 0.45858795\n",
      "Iteration 82, loss = 0.45627929\n",
      "Iteration 114, loss = 0.37490261\n",
      "Iteration 70, loss = 0.16018584\n",
      "Iteration 61, loss = 0.10842097\n",
      "Iteration 75, loss = 0.02220713\n",
      "Iteration 65, loss = 0.42464507\n",
      "Iteration 115, loss = 0.38228577\n",
      "Iteration 94, loss = 0.40664429\n",
      "Iteration 60, loss = 0.09274746\n",
      "Iteration 75, loss = 0.07570676\n",
      "Iteration 83, loss = 0.45603324\n",
      "Iteration 55, loss = 0.20339023\n",
      "Iteration 76, loss = 0.45698606\n",
      "Iteration 72, loss = 0.39847125\n",
      "Iteration 72, loss = 0.12938010\n",
      "Iteration 77, loss = 0.02837967\n",
      "Iteration 116, loss = 0.37685003\n",
      "Iteration 59, loss = 0.30728910\n",
      "Iteration 71, loss = 0.15832590\n",
      "Iteration 76, loss = 0.02396442\n",
      "Iteration 95, loss = 0.40570684\n",
      "Iteration 62, loss = 0.10671867\n",
      "Iteration 117, loss = 0.37353633\n",
      "Iteration 84, loss = 0.45646896\n",
      "Iteration 66, loss = 0.42348196\n",
      "Iteration 77, loss = 0.45743499\n",
      "Iteration 76, loss = 0.07747989\n",
      "Iteration 73, loss = 0.12868907\n",
      "Iteration 61, loss = 0.09423883\n",
      "Iteration 118, loss = 0.37421747\n",
      "Iteration 73, loss = 0.39926816\n",
      "Iteration 78, loss = 0.02888459\n",
      "Iteration 96, loss = 0.40567175\n",
      "Iteration 72, loss = 0.15668932\n",
      "Iteration 77, loss = 0.02161889\n",
      "Iteration 85, loss = 0.45486501\n",
      "Iteration 56, loss = 0.20150440\n",
      "Iteration 119, loss = 0.37266043\n",
      "Iteration 78, loss = 0.45707244\n",
      "Iteration 60, loss = 0.30545305\n",
      "Iteration 63, loss = 0.10731543\n",
      "Iteration 97, loss = 0.40548243\n",
      "Iteration 67, loss = 0.42378823\n",
      "Iteration 77, loss = 0.07638813\n",
      "Iteration 74, loss = 0.12786868\n",
      "Iteration 120, loss = 0.37285385\n",
      "Iteration 86, loss = 0.45477512\n",
      "Iteration 79, loss = 0.02959471\n",
      "Iteration 74, loss = 0.39921811\n",
      "Iteration 78, loss = 0.02115292\n",
      "Iteration 62, loss = 0.09177127\n",
      "Iteration 73, loss = 0.15872742\n",
      "Iteration 79, loss = 0.45670748\n",
      "Iteration 121, loss = 0.37190977\n",
      "Iteration 98, loss = 0.40562814\n",
      "Iteration 87, loss = 0.45565546\n",
      "Iteration 64, loss = 0.10425131\n",
      "Iteration 75, loss = 0.12796101\n",
      "Iteration 122, loss = 0.37443375\n",
      "Iteration 78, loss = 0.07468262\n",
      "Iteration 61, loss = 0.30387485\n",
      "Iteration 57, loss = 0.19974046\n",
      "Iteration 68, loss = 0.42281942\n",
      "Iteration 80, loss = 0.02869053\n",
      "Iteration 75, loss = 0.39663079\n",
      "Iteration 79, loss = 0.02318929\n",
      "Iteration 80, loss = 0.45634654\n",
      "Iteration 99, loss = 0.40393594\n",
      "Iteration 74, loss = 0.15496513\n",
      "Iteration 123, loss = 0.37131556\n",
      "Iteration 88, loss = 0.45194974\n",
      "Iteration 63, loss = 0.09088485\n",
      "Iteration 76, loss = 0.12756074\n",
      "Iteration 124, loss = 0.37166065\n",
      "Iteration 65, loss = 0.10531644\n",
      "Iteration 100, loss = 0.40360008\n",
      "Iteration 81, loss = 0.45607333\n",
      "Iteration 79, loss = 0.07365951\n",
      "Iteration 80, loss = 0.02130628\n",
      "Iteration 81, loss = 0.02699466\n",
      "Iteration 76, loss = 0.39664405\n",
      "Iteration 89, loss = 0.45207123\n",
      "Iteration 69, loss = 0.42192031\n",
      "Iteration 125, loss = 0.37252962\n",
      "Iteration 75, loss = 0.15526767\n",
      "Iteration 62, loss = 0.30257048\n",
      "Iteration 58, loss = 0.19887930\n",
      "Iteration 77, loss = 0.12615944\n",
      "Iteration 64, loss = 0.09048929\n",
      "Iteration 101, loss = 0.40333997\n",
      "Iteration 126, loss = 0.37068414\n",
      "Iteration 82, loss = 0.45359124\n",
      "Iteration 90, loss = 0.45276433\n",
      "Iteration 81, loss = 0.02018551\n",
      "Iteration 82, loss = 0.02770162\n",
      "Iteration 80, loss = 0.07266465\n",
      "Iteration 66, loss = 0.10547967\n",
      "Iteration 77, loss = 0.39598435\n",
      "Iteration 127, loss = 0.37159710\n",
      "Iteration 76, loss = 0.15406002\n",
      "Iteration 70, loss = 0.42189388\n",
      "Iteration 102, loss = 0.40306457\n",
      "Iteration 83, loss = 0.45430156\n",
      "Iteration 78, loss = 0.12596308\n",
      "Iteration 91, loss = 0.45049154\n",
      "Iteration 63, loss = 0.30257209\n",
      "Iteration 128, loss = 0.37275442\n",
      "Iteration 82, loss = 0.02092469\n",
      "Iteration 65, loss = 0.09097930\n",
      "Iteration 83, loss = 0.02670193\n",
      "Iteration 81, loss = 0.07311439\n",
      "Iteration 59, loss = 0.19868983\n",
      "Iteration 103, loss = 0.40213540\n",
      "Iteration 129, loss = 0.36873845\n",
      "Iteration 78, loss = 0.39603947\n",
      "Iteration 67, loss = 0.10331343\n",
      "Iteration 77, loss = 0.15290343\n",
      "Iteration 92, loss = 0.45026141\n",
      "Iteration 84, loss = 0.45486489\n",
      "Iteration 79, loss = 0.12227561\n",
      "Iteration 71, loss = 0.42180022\n",
      "Iteration 130, loss = 0.36854913\n",
      "Iteration 83, loss = 0.02087124\n",
      "Iteration 104, loss = 0.40296432\n",
      "Iteration 84, loss = 0.02651229\n",
      "Iteration 64, loss = 0.30034148\n",
      "Iteration 93, loss = 0.44874015\n",
      "Iteration 131, loss = 0.36869087\n",
      "Iteration 82, loss = 0.07269418\n",
      "Iteration 85, loss = 0.45254243\n",
      "Iteration 66, loss = 0.08940782\n",
      "Iteration 79, loss = 0.39388500\n",
      "Iteration 78, loss = 0.15365366\n",
      "Iteration 68, loss = 0.10310404\n",
      "Iteration 80, loss = 0.12514856\n",
      "Iteration 132, loss = 0.36826137\n",
      "Iteration 105, loss = 0.40166038\n",
      "Iteration 84, loss = 0.01960485\n",
      "Iteration 72, loss = 0.41887885\n",
      "Iteration 60, loss = 0.19318458\n",
      "Iteration 94, loss = 0.44899186\n",
      "Iteration 86, loss = 0.45340695\n",
      "Iteration 85, loss = 0.02674907\n",
      "Iteration 133, loss = 0.36833245\n",
      "Iteration 83, loss = 0.07117829\n",
      "Iteration 80, loss = 0.39561634\n",
      "Iteration 106, loss = 0.40067325\n",
      "Iteration 79, loss = 0.15164567\n",
      "Iteration 81, loss = 0.12451782\n",
      "Iteration 65, loss = 0.30006705\n",
      "Iteration 95, loss = 0.44908219\n",
      "Iteration 67, loss = 0.08942436\n",
      "Iteration 134, loss = 0.36787905\n",
      "Iteration 69, loss = 0.10110279\n",
      "Iteration 85, loss = 0.02032919\n",
      "Iteration 87, loss = 0.45136571\n",
      "Iteration 73, loss = 0.41963112\n",
      "Iteration 86, loss = 0.02536295\n",
      "Iteration 135, loss = 0.36769620\n",
      "Iteration 107, loss = 0.40084611\n",
      "Iteration 96, loss = 0.44661199\n",
      "Iteration 84, loss = 0.07209831\n",
      "Iteration 82, loss = 0.12215428\n",
      "Iteration 61, loss = 0.19399858\n",
      "Iteration 81, loss = 0.39348847\n",
      "Iteration 80, loss = 0.15617599\n",
      "Iteration 136, loss = 0.36663503\n",
      "Iteration 86, loss = 0.01918831\n",
      "Iteration 88, loss = 0.45126940\n",
      "Iteration 70, loss = 0.10016957\n",
      "Iteration 68, loss = 0.08828256\n",
      "Iteration 108, loss = 0.40051396\n",
      "Iteration 66, loss = 0.29958745\n",
      "Iteration 97, loss = 0.44552375\n",
      "Iteration 137, loss = 0.36694957\n",
      "Iteration 87, loss = 0.02618506\n",
      "Iteration 74, loss = 0.41812663\n",
      "Iteration 83, loss = 0.12190480\n",
      "Iteration 85, loss = 0.06952804\n",
      "Iteration 89, loss = 0.45166337\n",
      "Iteration 82, loss = 0.39248202\n",
      "Iteration 81, loss = 0.15151744\n",
      "Iteration 138, loss = 0.36639866\n",
      "Iteration 87, loss = 0.01991439\n",
      "Iteration 109, loss = 0.40030097\n",
      "Iteration 98, loss = 0.44517228\n",
      "Iteration 71, loss = 0.10124303\n",
      "Iteration 139, loss = 0.36741632\n",
      "Iteration 62, loss = 0.19192042\n",
      "Iteration 69, loss = 0.08634681\n",
      "Iteration 88, loss = 0.02461327\n",
      "Iteration 84, loss = 0.12197552\n",
      "Iteration 90, loss = 0.45264512\n",
      "Iteration 67, loss = 0.29872237\n",
      "Iteration 110, loss = 0.39855456\n",
      "Iteration 75, loss = 0.41646244\n",
      "Iteration 140, loss = 0.36702803\n",
      "Iteration 82, loss = 0.15028637\n",
      "Iteration 88, loss = 0.02037854\n",
      "Iteration 86, loss = 0.07093129\n",
      "Iteration 99, loss = 0.44412534\n",
      "Iteration 83, loss = 0.39474982\n",
      "Iteration 141, loss = 0.36519314\n",
      "Iteration 111, loss = 0.39872808\n",
      "Iteration 91, loss = 0.45121283\n",
      "Iteration 72, loss = 0.10040273\n",
      "Iteration 85, loss = 0.12073351\n",
      "Iteration 89, loss = 0.02516176\n",
      "Iteration 100, loss = 0.44526703\n",
      "Iteration 142, loss = 0.36485260\n",
      "Iteration 70, loss = 0.08719392\n",
      "Iteration 89, loss = 0.01873827\n",
      "Iteration 83, loss = 0.14812820\n",
      "Iteration 76, loss = 0.41697021\n",
      "Iteration 87, loss = 0.07108821\n",
      "Iteration 84, loss = 0.39126743\n",
      "Iteration 63, loss = 0.19051375\n",
      "Iteration 68, loss = 0.29706521\n",
      "Iteration 112, loss = 0.39855598\n",
      "Iteration 143, loss = 0.36616896\n",
      "Iteration 92, loss = 0.44953751\n",
      "Iteration 101, loss = 0.44420697\n",
      "Iteration 86, loss = 0.11937657\n",
      "Iteration 90, loss = 0.02554923\n",
      "Iteration 73, loss = 0.09883694\n",
      "Iteration 144, loss = 0.36797679\n",
      "Iteration 90, loss = 0.01981662\n",
      "Iteration 113, loss = 0.39901599\n",
      "Iteration 84, loss = 0.14899158\n",
      "Iteration 88, loss = 0.06886844\n",
      "Iteration 85, loss = 0.39235010\n",
      "Iteration 71, loss = 0.08849381\n",
      "Iteration 102, loss = 0.44270811\n",
      "Iteration 93, loss = 0.44933450\n",
      "Iteration 77, loss = 0.41682359\n",
      "Iteration 145, loss = 0.36464191\n",
      "Iteration 87, loss = 0.12016435\n",
      "Iteration 69, loss = 0.29751063\n",
      "Iteration 91, loss = 0.02494519\n",
      "Iteration 64, loss = 0.18964363\n",
      "Iteration 114, loss = 0.39839711\n",
      "Iteration 91, loss = 0.01759533\n",
      "Iteration 146, loss = 0.36435904\n",
      "Iteration 103, loss = 0.44147427\n",
      "Iteration 74, loss = 0.10154715\n",
      "Iteration 94, loss = 0.44886916\n",
      "Iteration 85, loss = 0.14971139\n",
      "Iteration 86, loss = 0.39162892\n",
      "Iteration 89, loss = 0.06952374\n",
      "Iteration 147, loss = 0.36360690\n",
      "Iteration 72, loss = 0.08549805\n",
      "Iteration 88, loss = 0.11845131\n",
      "Iteration 78, loss = 0.42001906\n",
      "Iteration 115, loss = 0.39709047\n",
      "Iteration 104, loss = 0.44066056\n",
      "Iteration 92, loss = 0.02325254\n",
      "Iteration 92, loss = 0.01690055\n",
      "Iteration 148, loss = 0.36244847\n",
      "Iteration 95, loss = 0.44870842\n",
      "Iteration 70, loss = 0.29502813\n",
      "Iteration 86, loss = 0.14646286\n",
      "Iteration 75, loss = 0.09641873\n",
      "Iteration 87, loss = 0.39044972\n",
      "Iteration 116, loss = 0.39661507\n",
      "Iteration 149, loss = 0.36112859\n",
      "Iteration 90, loss = 0.06780911\n",
      "Iteration 65, loss = 0.18861156\n",
      "Iteration 105, loss = 0.44203906\n",
      "Iteration 89, loss = 0.11741689\n",
      "Iteration 79, loss = 0.41586690\n",
      "Iteration 93, loss = 0.01780442\n",
      "Iteration 96, loss = 0.44920102\n",
      "Iteration 93, loss = 0.02461356\n",
      "Iteration 73, loss = 0.08475047\n",
      "Iteration 150, loss = 0.36548787\n",
      "Iteration 117, loss = 0.39676228\n",
      "Iteration 106, loss = 0.44196729\n",
      "Iteration 87, loss = 0.14809718\n",
      "Iteration 151, loss = 0.36668398\n",
      "Iteration 88, loss = 0.39000755\n",
      "Iteration 90, loss = 0.11710227\n",
      "Iteration 76, loss = 0.09680659\n",
      "Iteration 91, loss = 0.06840634\n",
      "Iteration 71, loss = 0.29447447\n",
      "Iteration 97, loss = 0.44852331\n",
      "Iteration 94, loss = 0.01767813\n",
      "Iteration 94, loss = 0.02391143\n",
      "Iteration 118, loss = 0.39682714\n",
      "Iteration 152, loss = 0.36528581\n",
      "Iteration 80, loss = 0.41406056\n",
      "Iteration 107, loss = 0.44041126\n",
      "Iteration 66, loss = 0.18754749\n",
      "Iteration 74, loss = 0.08816027\n",
      "Iteration 88, loss = 0.14595284\n",
      "Iteration 153, loss = 0.36209164\n",
      "Iteration 91, loss = 0.11732793\n",
      "Iteration 89, loss = 0.38947016\n",
      "Iteration 98, loss = 0.44634226\n",
      "Iteration 92, loss = 0.06783942\n",
      "Iteration 119, loss = 0.39480836\n",
      "Iteration 77, loss = 0.09581967\n",
      "Iteration 95, loss = 0.01958609\n",
      "Iteration 108, loss = 0.43751311\n",
      "Iteration 154, loss = 0.36115492\n",
      "Iteration 95, loss = 0.02217084\n",
      "Iteration 72, loss = 0.29324948\n",
      "Iteration 81, loss = 0.41436992\n",
      "Iteration 99, loss = 0.44757554\n",
      "Iteration 92, loss = 0.11607729\n",
      "Iteration 155, loss = 0.36084991\n",
      "Iteration 120, loss = 0.39528192\n",
      "Iteration 89, loss = 0.14626921\n",
      "Iteration 75, loss = 0.08383959\n",
      "Iteration 109, loss = 0.43959647\n",
      "Iteration 90, loss = 0.38909120\n",
      "Iteration 96, loss = 0.01849288\n",
      "Iteration 93, loss = 0.06989732\n",
      "Iteration 67, loss = 0.18420798\n",
      "Iteration 156, loss = 0.36120570\n",
      "Iteration 78, loss = 0.09624061\n",
      "Iteration 96, loss = 0.02225223\n",
      "Iteration 121, loss = 0.39582968\n",
      "Iteration 100, loss = 0.44683422\n",
      "Iteration 110, loss = 0.43835306\n",
      "Iteration 93, loss = 0.11653066\n",
      "Iteration 157, loss = 0.35890366\n",
      "Iteration 90, loss = 0.14529764\n",
      "Iteration 82, loss = 0.41272754\n",
      "Iteration 73, loss = 0.29407757\n",
      "Iteration 97, loss = 0.01759712\n",
      "Iteration 91, loss = 0.38972686\n",
      "Iteration 76, loss = 0.08281259\n",
      "Iteration 94, loss = 0.06673055\n",
      "Iteration 158, loss = 0.36172535\n",
      "Iteration 122, loss = 0.39505551\n",
      "Iteration 101, loss = 0.44534762\n",
      "Iteration 97, loss = 0.02389161\n",
      "Iteration 111, loss = 0.43747983\n",
      "Iteration 79, loss = 0.09577392\n",
      "Iteration 94, loss = 0.11556313\n",
      "Iteration 159, loss = 0.36183401\n",
      "Iteration 68, loss = 0.18544948\n",
      "Iteration 91, loss = 0.14452786\n",
      "Iteration 98, loss = 0.01515854\n",
      "Iteration 123, loss = 0.39382165\n",
      "Iteration 83, loss = 0.41314347\n",
      "Iteration 92, loss = 0.39002155\n",
      "Iteration 112, loss = 0.43706474\n",
      "Iteration 160, loss = 0.36025246\n",
      "Iteration 102, loss = 0.44489403\n",
      "Iteration 74, loss = 0.29277225\n",
      "Iteration 95, loss = 0.06994509\n",
      "Iteration 98, loss = 0.02198262\n",
      "Iteration 77, loss = 0.08103023\n",
      "Iteration 95, loss = 0.11462257\n",
      "Iteration 80, loss = 0.09527062\n",
      "Iteration 161, loss = 0.35832514\n",
      "Iteration 124, loss = 0.39352225\n",
      "Iteration 99, loss = 0.01528193\n",
      "Iteration 113, loss = 0.43725062\n",
      "Iteration 92, loss = 0.14529834\n",
      "Iteration 103, loss = 0.44526130\n",
      "Iteration 93, loss = 0.38720609\n",
      "Iteration 162, loss = 0.35805872\n",
      "Iteration 84, loss = 0.41169219\n",
      "Iteration 96, loss = 0.06643778\n",
      "Iteration 69, loss = 0.18344081\n",
      "Iteration 99, loss = 0.02183392\n",
      "Iteration 125, loss = 0.39395391\n",
      "Iteration 96, loss = 0.11610259\n",
      "Iteration 114, loss = 0.43699671\n",
      "Iteration 163, loss = 0.35804772\n",
      "Iteration 75, loss = 0.29223490\n",
      "Iteration 78, loss = 0.08327891\n",
      "Iteration 100, loss = 0.01648454\n",
      "Iteration 81, loss = 0.09297343\n",
      "Iteration 104, loss = 0.44496632\n",
      "Iteration 93, loss = 0.14349200\n",
      "Iteration 164, loss = 0.35969561\n",
      "Iteration 94, loss = 0.38794345\n",
      "Iteration 126, loss = 0.39374513\n",
      "Iteration 115, loss = 0.43555193\n",
      "Iteration 97, loss = 0.06426504\n",
      "Iteration 85, loss = 0.41124458\n",
      "Iteration 97, loss = 0.11432548\n",
      "Iteration 100, loss = 0.02299738\n",
      "Iteration 165, loss = 0.35872682\n",
      "Iteration 105, loss = 0.44370347\n",
      "Iteration 101, loss = 0.01672577\n",
      "Iteration 94, loss = 0.14449552\n",
      "Iteration 70, loss = 0.18268285\n",
      "Iteration 127, loss = 0.39216137\n",
      "Iteration 82, loss = 0.09374870\n",
      "Iteration 79, loss = 0.08168307\n",
      "Iteration 76, loss = 0.29009087\n",
      "Iteration 116, loss = 0.43420547\n",
      "Iteration 166, loss = 0.36046869\n",
      "Iteration 95, loss = 0.38635411\n",
      "Iteration 98, loss = 0.11144903\n",
      "Iteration 106, loss = 0.44486664\n",
      "Iteration 98, loss = 0.06547596\n",
      "Iteration 101, loss = 0.02350555\n",
      "Iteration 167, loss = 0.35819401\n",
      "Iteration 86, loss = 0.40959036\n",
      "Iteration 102, loss = 0.01592446\n",
      "Iteration 128, loss = 0.39305435\n",
      "Iteration 117, loss = 0.43490664\n",
      "Iteration 95, loss = 0.14337186\n",
      "Iteration 168, loss = 0.35935210\n",
      "Iteration 83, loss = 0.09369177\n",
      "Iteration 107, loss = 0.44341686\n",
      "Iteration 99, loss = 0.11209604\n",
      "Iteration 96, loss = 0.38707136\n",
      "Iteration 80, loss = 0.08086083\n",
      "Iteration 129, loss = 0.39170087\n",
      "Iteration 77, loss = 0.29111378\n",
      "Iteration 118, loss = 0.43398386\n",
      "Iteration 169, loss = 0.35703830\n",
      "Iteration 99, loss = 0.06821964\n",
      "Iteration 102, loss = 0.02246776\n",
      "Iteration 71, loss = 0.17964264\n",
      "Iteration 103, loss = 0.01587715\n",
      "Iteration 87, loss = 0.41110843\n",
      "Iteration 170, loss = 0.35812084\n",
      "Iteration 96, loss = 0.14113052\n",
      "Iteration 108, loss = 0.44284443\n",
      "Iteration 130, loss = 0.39112592\n",
      "Iteration 119, loss = 0.43292074\n",
      "Iteration 100, loss = 0.11311365\n",
      "Iteration 84, loss = 0.09243239\n",
      "Iteration 97, loss = 0.38473919\n",
      "Iteration 171, loss = 0.35945113\n",
      "Iteration 104, loss = 0.01562526\n",
      "Iteration 103, loss = 0.02103908\n",
      "Iteration 100, loss = 0.06683268\n",
      "Iteration 81, loss = 0.08072084\n",
      "Iteration 109, loss = 0.44266624\n",
      "Iteration 78, loss = 0.28817514\n",
      "Iteration 172, loss = 0.35797560\n",
      "Iteration 131, loss = 0.39091358\n",
      "Iteration 120, loss = 0.43281219\n",
      "Iteration 88, loss = 0.40919024\n",
      "Iteration 97, loss = 0.14000399\n",
      "Iteration 101, loss = 0.11229555\n",
      "Iteration 72, loss = 0.18070193\n",
      "Iteration 173, loss = 0.35601985\n",
      "Iteration 98, loss = 0.38532476\n",
      "Iteration 105, loss = 0.01623020\n",
      "Iteration 85, loss = 0.09131009\n",
      "Iteration 132, loss = 0.39049963\n",
      "Iteration 101, loss = 0.06292878\n",
      "Iteration 121, loss = 0.43191453\n",
      "Iteration 110, loss = 0.44378631\n",
      "Iteration 104, loss = 0.02172974\n",
      "Iteration 174, loss = 0.35513663\n",
      "Iteration 82, loss = 0.08049398\n",
      "Iteration 98, loss = 0.14077989\n",
      "Iteration 102, loss = 0.11115363\n",
      "Iteration 89, loss = 0.40851012\n",
      "Iteration 79, loss = 0.28804531\n",
      "Iteration 106, loss = 0.01544051\n",
      "Iteration 175, loss = 0.35350463\n",
      "Iteration 133, loss = 0.39223095\n",
      "Iteration 122, loss = 0.43291307\n",
      "Iteration 99, loss = 0.38647821\n",
      "Iteration 111, loss = 0.44182022\n",
      "Iteration 86, loss = 0.09283392\n",
      "Iteration 102, loss = 0.06309782\n",
      "Iteration 73, loss = 0.17852429\n",
      "Iteration 105, loss = 0.02231901\n",
      "Iteration 176, loss = 0.35680569\n",
      "Iteration 103, loss = 0.11218571\n",
      "Iteration 99, loss = 0.14032687\n",
      "Iteration 123, loss = 0.43155325\n",
      "Iteration 134, loss = 0.39059701\n",
      "Iteration 83, loss = 0.08151215\n",
      "Iteration 107, loss = 0.01445348\n",
      "Iteration 112, loss = 0.44099602\n",
      "Iteration 177, loss = 0.35431423\n",
      "Iteration 90, loss = 0.40864832\n",
      "Iteration 100, loss = 0.38476236\n",
      "Iteration 80, loss = 0.28789807\n",
      "Iteration 103, loss = 0.06270683\n",
      "Iteration 178, loss = 0.35444835\n",
      "Iteration 87, loss = 0.08994496\n",
      "Iteration 124, loss = 0.43173574\n",
      "Iteration 135, loss = 0.39108623\n",
      "Iteration 106, loss = 0.02015093\n",
      "Iteration 104, loss = 0.11215670\n",
      "Iteration 100, loss = 0.13810400\n",
      "Iteration 113, loss = 0.44062989\n",
      "Iteration 108, loss = 0.01484241\n",
      "Iteration 179, loss = 0.35876949\n",
      "Iteration 74, loss = 0.17685838\n",
      "Iteration 101, loss = 0.38444230\n",
      "Iteration 84, loss = 0.07794754\n",
      "Iteration 125, loss = 0.43417925\n",
      "Iteration 91, loss = 0.40800773\n",
      "Iteration 136, loss = 0.39033425\n",
      "Iteration 180, loss = 0.35382369\n",
      "Iteration 105, loss = 0.11158696\n",
      "Iteration 104, loss = 0.06238904\n",
      "Iteration 114, loss = 0.43993602\n",
      "Iteration 88, loss = 0.09084466\n",
      "Iteration 107, loss = 0.02042126\n",
      "Iteration 81, loss = 0.28716246\n",
      "Iteration 101, loss = 0.13943821\n",
      "Iteration 109, loss = 0.01542500\n",
      "Iteration 181, loss = 0.35289881\n",
      "Iteration 126, loss = 0.42946929\n",
      "Iteration 137, loss = 0.39028752\n",
      "Iteration 102, loss = 0.38354770\n",
      "Iteration 115, loss = 0.43990155\n",
      "Iteration 182, loss = 0.35304539\n",
      "Iteration 106, loss = 0.10782719\n",
      "Iteration 92, loss = 0.40680753\n",
      "Iteration 85, loss = 0.08040355\n",
      "Iteration 105, loss = 0.06425472\n",
      "Iteration 75, loss = 0.17654026\n",
      "Iteration 110, loss = 0.01380575\n",
      "Iteration 127, loss = 0.42800438\n",
      "Iteration 138, loss = 0.38932551\n",
      "Iteration 102, loss = 0.14127517\n",
      "Iteration 89, loss = 0.08971169\n",
      "Iteration 108, loss = 0.02157272\n",
      "Iteration 183, loss = 0.35281283\n",
      "Iteration 82, loss = 0.28544129\n",
      "Iteration 116, loss = 0.43893585\n",
      "Iteration 103, loss = 0.38442287\n",
      "Iteration 184, loss = 0.35386907\n",
      "Iteration 107, loss = 0.10904358\n",
      "Iteration 128, loss = 0.42935537\n",
      "Iteration 139, loss = 0.38830089\n",
      "Iteration 111, loss = 0.01378818\n",
      "Iteration 93, loss = 0.40744635\n",
      "Iteration 106, loss = 0.06166354\n",
      "Iteration 185, loss = 0.35404098\n",
      "Iteration 86, loss = 0.07915924\n",
      "Iteration 103, loss = 0.13634495\n",
      "Iteration 117, loss = 0.43914496\n",
      "Iteration 90, loss = 0.08917706\n",
      "Iteration 109, loss = 0.01964667\n",
      "Iteration 129, loss = 0.42842868\n",
      "Iteration 140, loss = 0.38790444\n",
      "Iteration 186, loss = 0.35446377\n",
      "Iteration 76, loss = 0.17863848\n",
      "Iteration 108, loss = 0.10844444\n",
      "Iteration 104, loss = 0.38313609\n",
      "Iteration 112, loss = 0.01614754\n",
      "Iteration 83, loss = 0.28606277\n",
      "Iteration 187, loss = 0.35331131\n",
      "Iteration 107, loss = 0.06189436\n",
      "Iteration 118, loss = 0.43984734\n",
      "Iteration 94, loss = 0.40718468\n",
      "Iteration 104, loss = 0.13696910\n",
      "Iteration 130, loss = 0.42757392\n",
      "Iteration 141, loss = 0.38878437\n",
      "Iteration 91, loss = 0.08789136\n",
      "Iteration 87, loss = 0.07718130\n",
      "Iteration 110, loss = 0.02157199\n",
      "Iteration 188, loss = 0.35158177\n",
      "Iteration 109, loss = 0.10786601\n",
      "Iteration 105, loss = 0.38487392\n",
      "Iteration 113, loss = 0.01412579\n",
      "Iteration 119, loss = 0.44012471\n",
      "Iteration 131, loss = 0.42753628\n",
      "Iteration 189, loss = 0.35493171\n",
      "Iteration 142, loss = 0.38861249\n",
      "Iteration 108, loss = 0.06125678\n",
      "Iteration 77, loss = 0.17447722\n",
      "Iteration 105, loss = 0.13666910\n",
      "Iteration 84, loss = 0.28340731\n",
      "Iteration 95, loss = 0.40675598\n",
      "Iteration 110, loss = 0.10811571\n",
      "Iteration 190, loss = 0.35024024\n",
      "Iteration 92, loss = 0.08949751\n",
      "Iteration 106, loss = 0.38242605\n",
      "Iteration 114, loss = 0.01274099\n",
      "Iteration 132, loss = 0.42628795\n",
      "Iteration 111, loss = 0.02069922\n",
      "Iteration 120, loss = 0.43843178\n",
      "Iteration 143, loss = 0.38609980\n",
      "Iteration 88, loss = 0.07835944\n",
      "Iteration 191, loss = 0.35011836\n",
      "Iteration 109, loss = 0.06102096\n",
      "Iteration 106, loss = 0.13891695\n",
      "Iteration 111, loss = 0.10915130\n",
      "Iteration 133, loss = 0.42535464\n",
      "Iteration 192, loss = 0.35139052\n",
      "Iteration 144, loss = 0.38773581\n",
      "Iteration 96, loss = 0.40490338\n",
      "Iteration 121, loss = 0.43809219\n",
      "Iteration 115, loss = 0.01344236\n",
      "Iteration 85, loss = 0.28382252\n",
      "Iteration 107, loss = 0.38151144\n",
      "Iteration 78, loss = 0.17450264\n",
      "Iteration 93, loss = 0.08693372\n",
      "Iteration 193, loss = 0.35089708\n",
      "Iteration 112, loss = 0.01958958\n",
      "Iteration 89, loss = 0.07717508\n",
      "Iteration 134, loss = 0.42714556\n",
      "Iteration 145, loss = 0.38782511\n",
      "Iteration 110, loss = 0.05951997\n",
      "Iteration 107, loss = 0.13679370\n",
      "Iteration 122, loss = 0.43825340\n",
      "Iteration 112, loss = 0.10710843\n",
      "Iteration 194, loss = 0.35065633\n",
      "Iteration 116, loss = 0.01530877\n",
      "Iteration 97, loss = 0.40702635\n",
      "Iteration 108, loss = 0.38117520\n",
      "Iteration 195, loss = 0.35410610\n",
      "Iteration 135, loss = 0.42650076\n",
      "Iteration 94, loss = 0.08673745\n",
      "Iteration 146, loss = 0.38766246\n",
      "Iteration 86, loss = 0.28443553\n",
      "Iteration 113, loss = 0.01779349\n",
      "Iteration 123, loss = 0.43637738\n",
      "Iteration 113, loss = 0.10586775\n",
      "Iteration 108, loss = 0.13438654\n",
      "Iteration 111, loss = 0.05964807\n",
      "Iteration 196, loss = 0.35104700\n",
      "Iteration 79, loss = 0.17288630\n",
      "Iteration 117, loss = 0.01307231\n",
      "Iteration 90, loss = 0.07610259\n",
      "Iteration 136, loss = 0.42548602\n",
      "Iteration 147, loss = 0.38711766\n",
      "Iteration 197, loss = 0.35034822\n",
      "Iteration 109, loss = 0.38144972\n",
      "Iteration 98, loss = 0.40392866\n",
      "Iteration 124, loss = 0.43669079\n",
      "Iteration 95, loss = 0.08576465\n",
      "Iteration 114, loss = 0.10761368\n",
      "Iteration 114, loss = 0.02022176\n",
      "Iteration 198, loss = 0.34880347\n",
      "Iteration 109, loss = 0.13488956\n",
      "Iteration 112, loss = 0.05942320\n",
      "Iteration 137, loss = 0.42489722\n",
      "Iteration 118, loss = 0.01342658\n",
      "Iteration 148, loss = 0.38598500\n",
      "Iteration 87, loss = 0.28348977\n",
      "Iteration 199, loss = 0.34909808\n",
      "Iteration 125, loss = 0.43732557\n",
      "Iteration 110, loss = 0.38116982\n",
      "Iteration 91, loss = 0.07938173\n",
      "Iteration 80, loss = 0.17240185\n",
      "Iteration 99, loss = 0.40287013\n",
      "Iteration 115, loss = 0.10565522\n",
      "Iteration 138, loss = 0.42354529\n",
      "Iteration 149, loss = 0.38604047\n",
      "Iteration 200, loss = 0.34926999\n",
      "Iteration 96, loss = 0.08585559\n",
      "Iteration 119, loss = 0.01267812\n",
      "Iteration 110, loss = 0.13436048\n",
      "Iteration 113, loss = 0.06162516\n",
      "Iteration 115, loss = 0.01946021\n",
      "Iteration 126, loss = 0.43713822\n",
      "Iteration 201, loss = 0.35048294\n",
      "Iteration 88, loss = 0.28213919\n",
      "Iteration 139, loss = 0.42695980\n",
      "Iteration 150, loss = 0.38609635\n",
      "Iteration 111, loss = 0.37992557\n",
      "Iteration 116, loss = 0.10507350\n",
      "Iteration 202, loss = 0.35086068\n",
      "Iteration 100, loss = 0.40249971\n",
      "Iteration 120, loss = 0.01377997\n",
      "Iteration 92, loss = 0.07574812\n",
      "Iteration 127, loss = 0.43593005\n",
      "Iteration 111, loss = 0.13472179\n",
      "Iteration 97, loss = 0.08665859\n",
      "Iteration 114, loss = 0.06018458\n",
      "Iteration 140, loss = 0.42436134\n",
      "Iteration 81, loss = 0.17079758\n",
      "Iteration 151, loss = 0.38456337\n",
      "Iteration 203, loss = 0.34725672\n",
      "Iteration 116, loss = 0.01874753\n",
      "Iteration 117, loss = 0.10452539\n",
      "Iteration 112, loss = 0.37995569\n",
      "Iteration 204, loss = 0.34792868\n",
      "Iteration 121, loss = 0.01248244\n",
      "Iteration 128, loss = 0.43552068\n",
      "Iteration 89, loss = 0.28204085\n",
      "Iteration 141, loss = 0.42319279\n",
      "Iteration 152, loss = 0.38514884\n",
      "Iteration 112, loss = 0.13531556\n",
      "Iteration 101, loss = 0.40211157\n",
      "Iteration 115, loss = 0.05900263\n",
      "Iteration 98, loss = 0.08554564\n",
      "Iteration 205, loss = 0.35181187\n",
      "Iteration 93, loss = 0.07669153\n",
      "Iteration 118, loss = 0.10364012\n",
      "Iteration 117, loss = 0.01765935\n",
      "Iteration 129, loss = 0.43586037\n",
      "Iteration 142, loss = 0.42286043\n",
      "Iteration 113, loss = 0.38161817\n",
      "Iteration 206, loss = 0.34791919\n",
      "Iteration 122, loss = 0.01292739\n",
      "Iteration 153, loss = 0.38495395\n",
      "Iteration 82, loss = 0.16906708\n",
      "Iteration 113, loss = 0.13432500\n",
      "Iteration 207, loss = 0.34618714\n",
      "Iteration 102, loss = 0.40328351\n",
      "Iteration 116, loss = 0.05792209\n",
      "Iteration 90, loss = 0.28162261\n",
      "Iteration 99, loss = 0.08407803\n",
      "Iteration 143, loss = 0.42132410\n",
      "Iteration 119, loss = 0.10473557\n",
      "Iteration 130, loss = 0.43425075\n",
      "Iteration 154, loss = 0.38482903\n",
      "Iteration 208, loss = 0.34734210\n",
      "Iteration 123, loss = 0.01370704\n",
      "Iteration 114, loss = 0.37850013\n",
      "Iteration 118, loss = 0.01877179\n",
      "Iteration 94, loss = 0.07514077\n",
      "Iteration 114, loss = 0.13480223\n",
      "Iteration 209, loss = 0.34874488\n",
      "Iteration 144, loss = 0.42087895\n",
      "Iteration 155, loss = 0.38532447\n",
      "Iteration 131, loss = 0.43549830\n",
      "Iteration 117, loss = 0.05841210\n",
      "Iteration 120, loss = 0.10496137\n",
      "Iteration 103, loss = 0.40135888\n",
      "Iteration 83, loss = 0.16858760\n",
      "Iteration 100, loss = 0.08510703\n",
      "Iteration 210, loss = 0.34933147\n",
      "Iteration 124, loss = 0.01194735\n",
      "Iteration 91, loss = 0.27967042\n",
      "Iteration 115, loss = 0.37971206\n",
      "Iteration 145, loss = 0.42596743\n",
      "Iteration 119, loss = 0.02113263\n",
      "Iteration 156, loss = 0.38422759\n",
      "Iteration 132, loss = 0.43526070\n",
      "Iteration 211, loss = 0.34656793\n",
      "Iteration 115, loss = 0.13241086\n",
      "Iteration 121, loss = 0.10695270\n",
      "Iteration 95, loss = 0.07508079\n",
      "Iteration 118, loss = 0.05987944\n",
      "Iteration 125, loss = 0.01231402\n",
      "Iteration 212, loss = 0.34748695\n",
      "Iteration 104, loss = 0.40162618\n",
      "Iteration 146, loss = 0.42161211\n",
      "Iteration 101, loss = 0.08410021\n",
      "Iteration 157, loss = 0.38454174\n",
      "Iteration 116, loss = 0.37822113\n",
      "Iteration 133, loss = 0.43332905\n",
      "Iteration 213, loss = 0.35403919\n",
      "Iteration 92, loss = 0.28108641\n",
      "Iteration 84, loss = 0.16818312\n",
      "Iteration 120, loss = 0.01796988\n",
      "Iteration 116, loss = 0.13139327\n",
      "Iteration 122, loss = 0.10263023\n",
      "Iteration 147, loss = 0.42046036\n",
      "Iteration 158, loss = 0.38312370\n",
      "Iteration 119, loss = 0.05782696\n",
      "Iteration 126, loss = 0.01295468\n",
      "Iteration 214, loss = 0.34679741\n",
      "Iteration 134, loss = 0.43263854\n",
      "Iteration 105, loss = 0.40002164\n",
      "Iteration 102, loss = 0.08322695\n",
      "Iteration 96, loss = 0.07502838\n",
      "Iteration 117, loss = 0.37885124\n",
      "Iteration 215, loss = 0.34596596\n",
      "Iteration 148, loss = 0.42087755\n",
      "Iteration 123, loss = 0.10327999\n",
      "Iteration 159, loss = 0.38420478\n",
      "Iteration 117, loss = 0.13667148\n",
      "Iteration 121, loss = 0.01813076\n",
      "Iteration 127, loss = 0.01300066\n",
      "Iteration 93, loss = 0.27997925\n",
      "Iteration 135, loss = 0.43297658\n",
      "Iteration 120, loss = 0.05747539\n",
      "Iteration 216, loss = 0.34540617\n",
      "Iteration 85, loss = 0.16727958\n",
      "Iteration 149, loss = 0.41927535\n",
      "Iteration 118, loss = 0.37927267\n",
      "Iteration 160, loss = 0.38409976\n",
      "Iteration 217, loss = 0.34617179\n",
      "Iteration 106, loss = 0.40010233\n",
      "Iteration 103, loss = 0.08213626\n",
      "Iteration 124, loss = 0.10000352\n",
      "Iteration 118, loss = 0.13114109\n",
      "Iteration 128, loss = 0.01118167\n",
      "Iteration 97, loss = 0.07321639\n",
      "Iteration 136, loss = 0.43373268\n",
      "Iteration 218, loss = 0.34625709\n",
      "Iteration 150, loss = 0.42013424\n",
      "Iteration 121, loss = 0.05749260\n",
      "Iteration 122, loss = 0.01827737\n",
      "Iteration 161, loss = 0.38242842\n",
      "Iteration 94, loss = 0.27861152\n",
      "Iteration 119, loss = 0.37834145\n",
      "Iteration 219, loss = 0.34496198\n",
      "Iteration 125, loss = 0.10182678\n",
      "Iteration 104, loss = 0.08387517\n",
      "Iteration 137, loss = 0.43490606\n",
      "Iteration 107, loss = 0.39951609\n",
      "Iteration 129, loss = 0.01146564\n",
      "Iteration 151, loss = 0.41904382\n",
      "Iteration 86, loss = 0.16607925\n",
      "Iteration 119, loss = 0.13221447\n",
      "Iteration 162, loss = 0.38331624\n",
      "Iteration 220, loss = 0.34595647\n",
      "Iteration 122, loss = 0.05576188\n",
      "Iteration 123, loss = 0.01650662\n",
      "Iteration 98, loss = 0.07477046\n",
      "Iteration 221, loss = 0.34851409\n",
      "Iteration 126, loss = 0.10001986\n",
      "Iteration 120, loss = 0.37729272\n",
      "Iteration 138, loss = 0.43184399\n",
      "Iteration 152, loss = 0.42099361\n",
      "Iteration 163, loss = 0.38178624\n",
      "Iteration 130, loss = 0.01214797\n",
      "Iteration 95, loss = 0.27808483\n",
      "Iteration 105, loss = 0.08241668\n",
      "Iteration 120, loss = 0.12968970\n",
      "Iteration 108, loss = 0.39913560\n",
      "Iteration 222, loss = 0.34578840\n",
      "Iteration 123, loss = 0.05696019\n",
      "Iteration 153, loss = 0.41752198\n",
      "Iteration 139, loss = 0.43182339\n",
      "Iteration 164, loss = 0.38248636\n",
      "Iteration 223, loss = 0.34584783\n",
      "Iteration 127, loss = 0.10153648\n",
      "Iteration 87, loss = 0.16619092\n",
      "Iteration 121, loss = 0.37683476\n",
      "Iteration 124, loss = 0.01800922\n",
      "Iteration 131, loss = 0.01270862\n",
      "Iteration 99, loss = 0.07419022\n",
      "Iteration 224, loss = 0.34442187\n",
      "Iteration 121, loss = 0.12988745\n",
      "Iteration 154, loss = 0.41892879\n",
      "Iteration 106, loss = 0.08336115\n",
      "Iteration 165, loss = 0.38405247\n",
      "Iteration 109, loss = 0.39998056\n",
      "Iteration 140, loss = 0.43140553\n",
      "Iteration 96, loss = 0.27723999\n",
      "Iteration 124, loss = 0.05566255\n",
      "Iteration 128, loss = 0.10401020\n",
      "Iteration 225, loss = 0.34301953\n",
      "Iteration 132, loss = 0.01206167\n",
      "Iteration 122, loss = 0.37742485\n",
      "Iteration 155, loss = 0.41728795\n",
      "Iteration 125, loss = 0.01813625\n",
      "Iteration 166, loss = 0.38165844\n",
      "Iteration 226, loss = 0.34218486\n",
      "Iteration 141, loss = 0.43139515\n",
      "Iteration 122, loss = 0.12869176\n",
      "Iteration 88, loss = 0.16551026\n",
      "Iteration 107, loss = 0.07980402\n",
      "Iteration 129, loss = 0.10155870\n",
      "Iteration 110, loss = 0.39828474\n",
      "Iteration 227, loss = 0.34575253\n",
      "Iteration 100, loss = 0.07280557\n",
      "Iteration 125, loss = 0.05454481\n",
      "Iteration 156, loss = 0.41754959\n",
      "Iteration 133, loss = 0.01219489\n",
      "Iteration 167, loss = 0.38060906\n",
      "Iteration 97, loss = 0.27704112\n",
      "Iteration 123, loss = 0.37636223\n",
      "Iteration 142, loss = 0.43149568\n",
      "Iteration 228, loss = 0.34525850\n",
      "Iteration 123, loss = 0.13248125\n",
      "Iteration 126, loss = 0.01786622\n",
      "Iteration 157, loss = 0.41810831\n",
      "Iteration 130, loss = 0.09906971\n",
      "Iteration 229, loss = 0.34448752\n",
      "Iteration 168, loss = 0.38224535\n",
      "Iteration 108, loss = 0.08256899\n",
      "Iteration 134, loss = 0.00994686\n",
      "Iteration 111, loss = 0.39781329\n",
      "Iteration 126, loss = 0.05487358\n",
      "Iteration 143, loss = 0.43075213\n",
      "Iteration 89, loss = 0.16354156\n",
      "Iteration 124, loss = 0.37698393\n",
      "Iteration 230, loss = 0.34427071\n",
      "Iteration 101, loss = 0.07110029\n",
      "Iteration 158, loss = 0.41696560\n",
      "Iteration 98, loss = 0.27664526\n",
      "Iteration 124, loss = 0.13100189\n",
      "Iteration 169, loss = 0.38092917\n",
      "Iteration 131, loss = 0.09920844\n",
      "Iteration 127, loss = 0.01771837\n",
      "Iteration 231, loss = 0.34446947\n",
      "Iteration 135, loss = 0.01169127\n",
      "Iteration 144, loss = 0.43091816\n",
      "Iteration 109, loss = 0.07935199\n",
      "Iteration 127, loss = 0.05506606\n",
      "Iteration 159, loss = 0.41698494\n",
      "Iteration 112, loss = 0.39664960\n",
      "Iteration 125, loss = 0.37550232\n",
      "Iteration 232, loss = 0.34263978\n",
      "Iteration 170, loss = 0.38238041\n",
      "Iteration 132, loss = 0.09848536\n",
      "Iteration 125, loss = 0.12802259\n",
      "Iteration 145, loss = 0.42997737\n",
      "Iteration 233, loss = 0.34533487\n",
      "Iteration 136, loss = 0.01092199\n",
      "Iteration 90, loss = 0.16170488\n",
      "Iteration 102, loss = 0.07258844\n",
      "Iteration 160, loss = 0.41551640\n",
      "Iteration 99, loss = 0.27570937\n",
      "Iteration 128, loss = 0.01685738\n",
      "Iteration 171, loss = 0.38030189\n",
      "Iteration 128, loss = 0.05470028\n",
      "Iteration 234, loss = 0.34425980\n",
      "Iteration 110, loss = 0.08078130\n",
      "Iteration 126, loss = 0.37568287\n",
      "Iteration 113, loss = 0.39595623\n",
      "Iteration 133, loss = 0.09983698\n",
      "Iteration 146, loss = 0.42996612\n",
      "Iteration 161, loss = 0.41569438\n",
      "Iteration 126, loss = 0.12764688\n",
      "Iteration 137, loss = 0.01384493\n",
      "Iteration 235, loss = 0.34213265\n",
      "Iteration 172, loss = 0.38103036\n",
      "Iteration 236, loss = 0.34202745\n",
      "Iteration 129, loss = 0.01660563\n",
      "Iteration 129, loss = 0.05426724\n",
      "Iteration 100, loss = 0.27486899\n",
      "Iteration 127, loss = 0.37480699\n",
      "Iteration 162, loss = 0.41592748\n",
      "Iteration 103, loss = 0.07095965\n",
      "Iteration 147, loss = 0.43025609\n",
      "Iteration 111, loss = 0.08012788\n",
      "Iteration 91, loss = 0.16413310\n",
      "Iteration 134, loss = 0.09971048\n",
      "Iteration 173, loss = 0.37911928\n",
      "Iteration 114, loss = 0.39889352\n",
      "Iteration 138, loss = 0.00866399\n",
      "Iteration 237, loss = 0.34351006\n",
      "Iteration 127, loss = 0.12897804\n",
      "Iteration 163, loss = 0.41518247\n",
      "Iteration 238, loss = 0.34297527\n",
      "Iteration 148, loss = 0.42985890\n",
      "Iteration 174, loss = 0.38119042\n",
      "Iteration 130, loss = 0.05400508\n",
      "Iteration 135, loss = 0.09881670\n",
      "Iteration 128, loss = 0.37666356\n",
      "Iteration 130, loss = 0.01764113\n",
      "Iteration 112, loss = 0.08089770\n",
      "Iteration 139, loss = 0.01160124\n",
      "Iteration 239, loss = 0.34423306\n",
      "Iteration 128, loss = 0.12745760\n",
      "Iteration 101, loss = 0.27526792\n",
      "Iteration 115, loss = 0.39768188\n",
      "Iteration 164, loss = 0.41394052\n",
      "Iteration 104, loss = 0.07174218\n",
      "Iteration 149, loss = 0.42992197\n",
      "Iteration 92, loss = 0.15968776\n",
      "Iteration 175, loss = 0.37963929\n",
      "Iteration 240, loss = 0.34180541\n",
      "Iteration 136, loss = 0.09873779\n",
      "Iteration 131, loss = 0.05624790\n",
      "Iteration 129, loss = 0.37458104\n",
      "Iteration 140, loss = 0.00975544\n",
      "Iteration 165, loss = 0.41439688\n",
      "Iteration 241, loss = 0.34060919\n",
      "Iteration 131, loss = 0.01598389\n",
      "Iteration 113, loss = 0.07870491\n",
      "Iteration 129, loss = 0.12750423\n",
      "Iteration 176, loss = 0.38044220\n",
      "Iteration 150, loss = 0.43112508\n",
      "Iteration 116, loss = 0.39466906\n",
      "Iteration 242, loss = 0.34801430\n",
      "Iteration 102, loss = 0.27436257\n",
      "Iteration 137, loss = 0.09824744\n",
      "Iteration 166, loss = 0.41491619\n",
      "Iteration 105, loss = 0.07202853\n",
      "Iteration 141, loss = 0.01145004\n",
      "Iteration 132, loss = 0.05497261\n",
      "Iteration 130, loss = 0.37335196\n",
      "Iteration 177, loss = 0.37984133\n",
      "Iteration 243, loss = 0.34633198\n",
      "Iteration 151, loss = 0.42947750\n",
      "Iteration 93, loss = 0.16040744\n",
      "Iteration 130, loss = 0.12690725\n",
      "Iteration 114, loss = 0.07969678\n",
      "Iteration 132, loss = 0.01588490\n",
      "Iteration 167, loss = 0.41349322\n",
      "Iteration 244, loss = 0.33975225\n",
      "Iteration 117, loss = 0.39496469\n",
      "Iteration 138, loss = 0.09705994\n",
      "Iteration 178, loss = 0.37829290\n",
      "Iteration 142, loss = 0.01053566\n",
      "Iteration 152, loss = 0.43105735\n",
      "Iteration 245, loss = 0.33937910\n",
      "Iteration 103, loss = 0.27404941\n",
      "Iteration 131, loss = 0.37359293\n",
      "Iteration 133, loss = 0.05372961\n",
      "Iteration 168, loss = 0.41420301\n",
      "Iteration 131, loss = 0.12616067\n",
      "Iteration 106, loss = 0.07150126\n",
      "Iteration 246, loss = 0.33927098\n",
      "Iteration 115, loss = 0.07918605\n",
      "Iteration 179, loss = 0.37837583\n",
      "Iteration 139, loss = 0.09710113\n",
      "Iteration 133, loss = 0.01754133\n",
      "Iteration 94, loss = 0.15970577\n",
      "Iteration 153, loss = 0.42721508\n",
      "Iteration 143, loss = 0.01040361\n",
      "Iteration 118, loss = 0.39432456\n",
      "Iteration 247, loss = 0.33971692\n",
      "Iteration 169, loss = 0.41225953\n",
      "Iteration 132, loss = 0.37454810\n",
      "Iteration 134, loss = 0.05325235\n",
      "Iteration 180, loss = 0.38040267\n",
      "Iteration 132, loss = 0.12761307\n",
      "Iteration 104, loss = 0.27336952\n",
      "Iteration 248, loss = 0.34094337\n",
      "Iteration 140, loss = 0.09537093\n",
      "Iteration 154, loss = 0.42717204\n",
      "Iteration 116, loss = 0.07822707\n",
      "Iteration 144, loss = 0.00996316\n",
      "Iteration 170, loss = 0.41142978\n",
      "Iteration 134, loss = 0.01587036\n",
      "Iteration 107, loss = 0.07199948\n",
      "Iteration 249, loss = 0.34364531\n",
      "Iteration 181, loss = 0.38033224\n",
      "Iteration 119, loss = 0.39522876\n",
      "Iteration 133, loss = 0.37387556\n",
      "Iteration 135, loss = 0.05725289\n",
      "Iteration 133, loss = 0.12638101\n",
      "Iteration 95, loss = 0.16012845\n",
      "Iteration 155, loss = 0.42747164\n",
      "Iteration 250, loss = 0.34193403\n",
      "Iteration 171, loss = 0.41277970\n",
      "Iteration 141, loss = 0.09612004\n",
      "Iteration 145, loss = 0.01319087\n",
      "Iteration 182, loss = 0.37875047\n",
      "Iteration 105, loss = 0.27367370\n",
      "Iteration 117, loss = 0.07784206\n",
      "Iteration 251, loss = 0.34464574\n",
      "Iteration 135, loss = 0.01648516\n",
      "Iteration 134, loss = 0.37248830\n",
      "Iteration 156, loss = 0.42707296\n",
      "Iteration 172, loss = 0.41195304\n",
      "Iteration 120, loss = 0.39565661\n",
      "Iteration 136, loss = 0.05147281\n",
      "Iteration 134, loss = 0.12668778\n",
      "Iteration 252, loss = 0.33826358\n",
      "Iteration 142, loss = 0.09706053\n",
      "Iteration 108, loss = 0.06820724\n",
      "Iteration 183, loss = 0.37712519\n",
      "Iteration 146, loss = 0.01080121\n",
      "Iteration 253, loss = 0.33985536\n",
      "Iteration 173, loss = 0.41086262\n",
      "Iteration 96, loss = 0.15625523\n",
      "Iteration 157, loss = 0.42690237\n",
      "Iteration 118, loss = 0.07749869\n",
      "Iteration 184, loss = 0.37712662\n",
      "Iteration 135, loss = 0.37242279\n",
      "Iteration 106, loss = 0.27200209\n",
      "Iteration 254, loss = 0.33830304\n",
      "Iteration 136, loss = 0.01660943\n",
      "Iteration 137, loss = 0.05044454\n",
      "Iteration 143, loss = 0.09435452\n",
      "Iteration 135, loss = 0.12421777\n",
      "Iteration 121, loss = 0.39290915\n",
      "Iteration 147, loss = 0.00905083\n",
      "Iteration 174, loss = 0.41108756\n",
      "Iteration 158, loss = 0.42902990\n",
      "Iteration 255, loss = 0.33968488\n",
      "Iteration 185, loss = 0.37784143\n",
      "Iteration 109, loss = 0.06903684\n",
      "Iteration 119, loss = 0.07724870\n",
      "Iteration 136, loss = 0.37366131\n",
      "Iteration 144, loss = 0.09614957\n",
      "Iteration 256, loss = 0.34035109\n",
      "Iteration 148, loss = 0.01052728\n",
      "Iteration 138, loss = 0.05248718\n",
      "Iteration 136, loss = 0.12343875\n",
      "Iteration 175, loss = 0.41141100\n",
      "Iteration 137, loss = 0.01556814\n",
      "Iteration 97, loss = 0.15737452\n",
      "Iteration 122, loss = 0.39281732\n",
      "Iteration 107, loss = 0.27379385\n",
      "Iteration 159, loss = 0.42960388\n",
      "Iteration 186, loss = 0.37780372\n",
      "Iteration 257, loss = 0.33821973\n",
      "Iteration 176, loss = 0.41182303\n",
      "Iteration 145, loss = 0.09518879\n",
      "Iteration 258, loss = 0.34061707\n",
      "Iteration 137, loss = 0.37305615\n",
      "Iteration 149, loss = 0.00963297\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 120, loss = 0.07681791\n",
      "Iteration 187, loss = 0.37696959\n",
      "Iteration 137, loss = 0.12538139\n",
      "Iteration 160, loss = 0.42832696\n",
      "Iteration 139, loss = 0.05044076\n",
      "Iteration 110, loss = 0.06992058\n",
      "Iteration 259, loss = 0.33923102\n",
      "Iteration 138, loss = 0.01660632\n",
      "Iteration 123, loss = 0.39279187\n",
      "Iteration 177, loss = 0.40961320\n",
      "Iteration 108, loss = 0.27050441\n",
      "Iteration 146, loss = 0.09522272\n",
      "Iteration 188, loss = 0.37772597\n",
      "Iteration 260, loss = 0.33946648\n",
      "Iteration 98, loss = 0.15778195\n",
      "Iteration 161, loss = 0.42572705\n",
      "Iteration 138, loss = 0.37301671\n",
      "Iteration 138, loss = 0.12322380\n",
      "Iteration 121, loss = 0.07598387\n",
      "Iteration 140, loss = 0.05125608\n",
      "Iteration 178, loss = 0.40910602\n",
      "Iteration 261, loss = 0.33801459\n",
      "Iteration 189, loss = 0.37655628\n",
      "Iteration 124, loss = 0.39295020\n",
      "Iteration 139, loss = 0.01699956\n",
      "Iteration 147, loss = 0.09386662\n",
      "Iteration 162, loss = 0.42688126\n",
      "Iteration 111, loss = 0.06897628\n",
      "Iteration 262, loss = 0.33989693\n",
      "Iteration 139, loss = 0.37125034\n",
      "Iteration 179, loss = 0.40909506\n",
      "Iteration 109, loss = 0.27061868\n",
      "Iteration 139, loss = 0.12612573\n",
      "Iteration 190, loss = 0.37725615\n",
      "Iteration 141, loss = 0.05061276\n",
      "Iteration 263, loss = 0.33774392\n",
      "Iteration 122, loss = 0.07578719\n",
      "Iteration 163, loss = 0.42673032\n",
      "Iteration 99, loss = 0.15536887\n",
      "Iteration 148, loss = 0.09418067\n",
      "Iteration 180, loss = 0.40924456\n",
      "Iteration 264, loss = 0.33856971\n",
      "Iteration 125, loss = 0.39222453\n",
      "Iteration 140, loss = 0.01485129\n",
      "Iteration 191, loss = 0.37697061\n",
      "Iteration 140, loss = 0.37284937\n",
      "Iteration 140, loss = 0.12397345\n",
      "Iteration 265, loss = 0.34086505\n",
      "Iteration 164, loss = 0.42535156\n",
      "Iteration 142, loss = 0.05048815\n",
      "Iteration 112, loss = 0.06821378\n",
      "Iteration 181, loss = 0.40912476\n",
      "Iteration 110, loss = 0.26930996\n",
      "Iteration 123, loss = 0.07739354\n",
      "Iteration 149, loss = 0.09209500\n",
      "Iteration 192, loss = 0.37726680\n",
      "Iteration 266, loss = 0.33763713\n",
      "Iteration 126, loss = 0.39113479\n",
      "Iteration 141, loss = 0.37163856\n",
      "Iteration 165, loss = 0.42451492\n",
      "Iteration 141, loss = 0.01458249\n",
      "Iteration 100, loss = 0.15494984\n",
      "Iteration 182, loss = 0.40821913\n",
      "Iteration 267, loss = 0.33759231\n",
      "Iteration 141, loss = 0.12258641\n",
      "Iteration 193, loss = 0.37672152\n",
      "Iteration 143, loss = 0.05116427\n",
      "Iteration 150, loss = 0.09314592\n",
      "Iteration 124, loss = 0.07450480\n",
      "Iteration 268, loss = 0.33725453\n",
      "Iteration 166, loss = 0.42735016\n",
      "Iteration 111, loss = 0.26920682\n",
      "Iteration 183, loss = 0.41217897\n",
      "Iteration 113, loss = 0.06883543\n",
      "Iteration 194, loss = 0.37520200\n",
      "Iteration 269, loss = 0.33695930\n",
      "Iteration 142, loss = 0.37073321\n",
      "Iteration 127, loss = 0.39273808\n",
      "Iteration 142, loss = 0.12554629\n",
      "Iteration 142, loss = 0.01503222\n",
      "Iteration 151, loss = 0.09673525\n",
      "Iteration 144, loss = 0.05023878\n",
      "Iteration 270, loss = 0.33803601\n",
      "Iteration 184, loss = 0.40811994\n",
      "Iteration 167, loss = 0.42466746\n",
      "Iteration 101, loss = 0.15463255\n",
      "Iteration 125, loss = 0.07486937\n",
      "Iteration 195, loss = 0.37502071\n",
      "Iteration 271, loss = 0.33752504\n",
      "Iteration 143, loss = 0.37063247\n",
      "Iteration 143, loss = 0.12262266\n",
      "Iteration 112, loss = 0.27167027\n",
      "Iteration 152, loss = 0.09147619\n",
      "Iteration 128, loss = 0.39056756\n",
      "Iteration 185, loss = 0.40739312\n",
      "Iteration 168, loss = 0.42433253\n",
      "Iteration 272, loss = 0.33684059\n",
      "Iteration 145, loss = 0.05118973\n",
      "Iteration 143, loss = 0.01591751\n",
      "Iteration 196, loss = 0.37635572\n",
      "Iteration 114, loss = 0.06733810\n",
      "Iteration 126, loss = 0.07572971\n",
      "Iteration 273, loss = 0.33958477\n",
      "Iteration 186, loss = 0.40485424\n",
      "Iteration 153, loss = 0.09330877\n",
      "Iteration 144, loss = 0.37142592\n",
      "Iteration 144, loss = 0.12075845\n",
      "Iteration 169, loss = 0.42446527\n",
      "Iteration 197, loss = 0.37650929\n",
      "Iteration 102, loss = 0.15495959\n",
      "Iteration 129, loss = 0.39011148\n",
      "Iteration 274, loss = 0.33742964\n",
      "Iteration 146, loss = 0.05097183\n",
      "Iteration 113, loss = 0.26824181\n",
      "Iteration 187, loss = 0.40610820\n",
      "Iteration 144, loss = 0.01471551\n",
      "Iteration 275, loss = 0.33673060\n",
      "Iteration 198, loss = 0.37551566\n",
      "Iteration 127, loss = 0.07391285\n",
      "Iteration 170, loss = 0.42584804\n",
      "Iteration 154, loss = 0.09399566\n",
      "Iteration 145, loss = 0.36996890\n",
      "Iteration 115, loss = 0.06816327\n",
      "Iteration 145, loss = 0.12283058\n",
      "Iteration 276, loss = 0.33566372\n",
      "Iteration 188, loss = 0.40811387\n",
      "Iteration 130, loss = 0.39032044\n",
      "Iteration 147, loss = 0.04910189\n",
      "Iteration 199, loss = 0.37470648\n",
      "Iteration 171, loss = 0.43036433\n",
      "Iteration 277, loss = 0.33756137\n",
      "Iteration 103, loss = 0.15268882\n",
      "Iteration 145, loss = 0.01582578\n",
      "Iteration 114, loss = 0.26879595\n",
      "Iteration 155, loss = 0.09085583\n",
      "Iteration 189, loss = 0.40759201\n",
      "Iteration 128, loss = 0.07358500\n",
      "Iteration 146, loss = 0.36963660\n",
      "Iteration 146, loss = 0.12234557\n",
      "Iteration 278, loss = 0.33920642\n",
      "Iteration 200, loss = 0.37573877\n",
      "Iteration 172, loss = 0.42349862\n",
      "Iteration 148, loss = 0.04956525\n",
      "Iteration 131, loss = 0.39100180\n",
      "Iteration 279, loss = 0.33664624\n",
      "Iteration 116, loss = 0.06785033\n",
      "Iteration 190, loss = 0.40631121\n",
      "Iteration 156, loss = 0.09139988\n",
      "Iteration 201, loss = 0.37484167\n",
      "Iteration 146, loss = 0.01433953\n",
      "Iteration 280, loss = 0.33637642\n",
      "Iteration 147, loss = 0.11934587\n",
      "Iteration 147, loss = 0.37014051\n",
      "Iteration 129, loss = 0.07335268\n",
      "Iteration 115, loss = 0.26912657\n",
      "Iteration 173, loss = 0.42351145\n",
      "Iteration 104, loss = 0.15294659\n",
      "Iteration 191, loss = 0.40612847\n",
      "Iteration 281, loss = 0.33601922\n",
      "Iteration 149, loss = 0.05059798\n",
      "Iteration 157, loss = 0.09128025\n",
      "Iteration 202, loss = 0.37447409\n",
      "Iteration 132, loss = 0.38964323\n",
      "Iteration 282, loss = 0.33531677\n",
      "Iteration 174, loss = 0.42402753\n",
      "Iteration 148, loss = 0.12061814\n",
      "Iteration 192, loss = 0.40591303\n",
      "Iteration 148, loss = 0.36945470\n",
      "Iteration 147, loss = 0.01459870\n",
      "Iteration 130, loss = 0.07467780\n",
      "Iteration 117, loss = 0.06822186\n",
      "Iteration 203, loss = 0.37512586\n",
      "Iteration 283, loss = 0.33430656\n",
      "Iteration 116, loss = 0.26797442\n",
      "Iteration 158, loss = 0.09196509\n",
      "Iteration 150, loss = 0.04749035\n",
      "Iteration 193, loss = 0.40935621\n",
      "Iteration 175, loss = 0.42372976\n",
      "Iteration 105, loss = 0.15251113\n",
      "Iteration 133, loss = 0.39078702\n",
      "Iteration 284, loss = 0.33726153\n",
      "Iteration 149, loss = 0.12095536\n",
      "Iteration 204, loss = 0.37268673\n",
      "Iteration 149, loss = 0.36876346\n",
      "Iteration 148, loss = 0.01530711\n",
      "Iteration 131, loss = 0.07247167\n",
      "Iteration 285, loss = 0.33450249\n",
      "Iteration 159, loss = 0.09010571\n",
      "Iteration 194, loss = 0.40612622\n",
      "Iteration 176, loss = 0.42356046\n",
      "Iteration 151, loss = 0.04843844\n",
      "Iteration 205, loss = 0.37507606\n",
      "Iteration 286, loss = 0.33844843\n",
      "Iteration 117, loss = 0.26730808\n",
      "Iteration 118, loss = 0.06572509\n",
      "Iteration 134, loss = 0.39066444\n",
      "Iteration 150, loss = 0.12234934\n",
      "Iteration 150, loss = 0.37021656\n",
      "Iteration 195, loss = 0.40618168\n",
      "Iteration 287, loss = 0.33479679\n",
      "Iteration 177, loss = 0.42351025\n",
      "Iteration 160, loss = 0.09323023\n",
      "Iteration 106, loss = 0.14993654\n",
      "Iteration 132, loss = 0.07249364\n",
      "Iteration 206, loss = 0.37407813\n",
      "Iteration 149, loss = 0.01501039\n",
      "Iteration 152, loss = 0.04862176\n",
      "Iteration 288, loss = 0.33446603\n",
      "Iteration 196, loss = 0.40252693\n",
      "Iteration 151, loss = 0.12115895\n",
      "Iteration 178, loss = 0.42312631\n",
      "Iteration 151, loss = 0.36804732\n",
      "Iteration 135, loss = 0.38806504\n",
      "Iteration 289, loss = 0.33550451\n",
      "Iteration 207, loss = 0.37342577\n",
      "Iteration 118, loss = 0.26651563\n",
      "Iteration 161, loss = 0.09066398\n",
      "Iteration 119, loss = 0.06605808\n",
      "Iteration 197, loss = 0.40359316\n",
      "Iteration 133, loss = 0.07286570\n",
      "Iteration 290, loss = 0.33700682\n",
      "Iteration 153, loss = 0.05100459\n",
      "Iteration 150, loss = 0.01362384\n",
      "Iteration 179, loss = 0.42263078\n",
      "Iteration 208, loss = 0.37334379\n",
      "Iteration 107, loss = 0.15044977\n",
      "Iteration 152, loss = 0.11821443\n",
      "Iteration 291, loss = 0.33503468\n",
      "Iteration 152, loss = 0.36797169\n",
      "Iteration 162, loss = 0.09017464\n",
      "Iteration 198, loss = 0.40369012\n",
      "Iteration 136, loss = 0.38796547\n",
      "Iteration 292, loss = 0.33714753\n",
      "Iteration 119, loss = 0.26545012\n",
      "Iteration 209, loss = 0.37426739\n",
      "Iteration 180, loss = 0.42242874\n",
      "Iteration 134, loss = 0.07231395\n",
      "Iteration 154, loss = 0.04917876\n",
      "Iteration 151, loss = 0.01369205\n",
      "Iteration 199, loss = 0.40430148\n",
      "Iteration 153, loss = 0.12041988\n",
      "Iteration 293, loss = 0.33331000\n",
      "Iteration 163, loss = 0.08923493\n",
      "Iteration 120, loss = 0.06491284\n",
      "Iteration 153, loss = 0.36709681\n",
      "Iteration 210, loss = 0.37327383\n",
      "Iteration 181, loss = 0.42484644\n",
      "Iteration 137, loss = 0.38747010\n",
      "Iteration 294, loss = 0.33287159\n",
      "Iteration 108, loss = 0.15077105\n",
      "Iteration 200, loss = 0.40282597\n",
      "Iteration 155, loss = 0.04625179\n",
      "Iteration 135, loss = 0.07259922\n",
      "Iteration 120, loss = 0.26594177\n",
      "Iteration 295, loss = 0.33418336\n",
      "Iteration 164, loss = 0.08874721\n",
      "Iteration 154, loss = 0.12050117\n",
      "Iteration 211, loss = 0.37265809\n",
      "Iteration 152, loss = 0.01457314\n",
      "Iteration 154, loss = 0.36850385\n",
      "Iteration 182, loss = 0.42379651\n",
      "Iteration 201, loss = 0.40374669\n",
      "Iteration 296, loss = 0.33634288\n",
      "Iteration 138, loss = 0.38828233\n",
      "Iteration 212, loss = 0.37350967\n",
      "Iteration 121, loss = 0.06722619\n",
      "Iteration 156, loss = 0.04810661\n",
      "Iteration 165, loss = 0.09114976\n",
      "Iteration 297, loss = 0.33393865\n",
      "Iteration 136, loss = 0.07271185\n",
      "Iteration 183, loss = 0.42310649\n",
      "Iteration 155, loss = 0.12081233\n",
      "Iteration 202, loss = 0.40255479\n",
      "Iteration 109, loss = 0.15016303\n",
      "Iteration 155, loss = 0.36734066\n",
      "Iteration 153, loss = 0.01415371\n",
      "Iteration 121, loss = 0.26593356\n",
      "Iteration 298, loss = 0.33294166\n",
      "Iteration 213, loss = 0.37293368\n",
      "Iteration 166, loss = 0.09134357\n",
      "Iteration 203, loss = 0.40373092\n",
      "Iteration 139, loss = 0.38850974\n",
      "Iteration 184, loss = 0.42111212\n",
      "Iteration 299, loss = 0.33258466\n",
      "Iteration 157, loss = 0.04805710\n",
      "Iteration 156, loss = 0.11732697\n",
      "Iteration 137, loss = 0.07098391\n",
      "Iteration 214, loss = 0.37381971\n",
      "Iteration 156, loss = 0.36715010\n",
      "Iteration 300, loss = 0.33565789\n",
      "Iteration 122, loss = 0.06497220\n",
      "Iteration 204, loss = 0.40275180\n",
      "Iteration 154, loss = 0.01385986\n",
      "Iteration 185, loss = 0.42027826\n",
      "Iteration 167, loss = 0.08757573\n",
      "Iteration 122, loss = 0.26561716\n",
      "Iteration 110, loss = 0.14815831\n",
      "Iteration 301, loss = 0.33384918\n",
      "Iteration 215, loss = 0.37126622\n",
      "Iteration 158, loss = 0.04622381\n",
      "Iteration 140, loss = 0.38699457\n",
      "Iteration 157, loss = 0.11891431\n",
      "Iteration 205, loss = 0.40330568\n",
      "Iteration 157, loss = 0.36794376\n",
      "Iteration 138, loss = 0.07064033\n",
      "Iteration 302, loss = 0.33318152\n",
      "Iteration 186, loss = 0.42129386\n",
      "Iteration 168, loss = 0.08778309\n",
      "Iteration 216, loss = 0.37322339\n",
      "Iteration 155, loss = 0.01381025\n",
      "Iteration 303, loss = 0.33230480\n",
      "Iteration 206, loss = 0.40271883\n",
      "Iteration 159, loss = 0.04836180\n",
      "Iteration 123, loss = 0.06543893\n",
      "Iteration 158, loss = 0.11796313\n",
      "Iteration 123, loss = 0.26463596\n",
      "Iteration 141, loss = 0.38646763\n",
      "Iteration 187, loss = 0.42072435\n",
      "Iteration 304, loss = 0.33269747\n",
      "Iteration 158, loss = 0.36643757\n",
      "Iteration 111, loss = 0.14959848\n",
      "Iteration 217, loss = 0.37180483\n",
      "Iteration 139, loss = 0.07170563\n",
      "Iteration 169, loss = 0.08900447\n",
      "Iteration 207, loss = 0.40271774\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 305, loss = 0.33410514\n",
      "Iteration 156, loss = 0.01409317\n",
      "Iteration 188, loss = 0.42043211\n",
      "Iteration 160, loss = 0.04761049\n",
      "Iteration 218, loss = 0.37122482\n",
      "Iteration 159, loss = 0.11575759\n",
      "Iteration 306, loss = 0.33286817\n",
      "Iteration 159, loss = 0.36675109\n",
      "Iteration 142, loss = 0.38629576\n",
      "Iteration 170, loss = 0.08777853\n",
      "Iteration 124, loss = 0.26525048\n",
      "Iteration 140, loss = 0.07079277\n",
      "Iteration 307, loss = 0.33468744\n",
      "Iteration 124, loss = 0.06535317\n",
      "Iteration 189, loss = 0.42196511\n",
      "Iteration 219, loss = 0.37169368\n",
      "Iteration 112, loss = 0.14868298\n",
      "Iteration 308, loss = 0.33406778\n",
      "Iteration 161, loss = 0.04691113\n",
      "Iteration 160, loss = 0.11880060\n",
      "Iteration 157, loss = 0.01393576\n",
      "Iteration 160, loss = 0.36717529\n",
      "Iteration 171, loss = 0.08761459\n",
      "Iteration 143, loss = 0.38487853\n",
      "Iteration 220, loss = 0.37265803\n",
      "Iteration 309, loss = 0.33263822\n",
      "Iteration 190, loss = 0.42214804\n",
      "Iteration 141, loss = 0.06954601\n",
      "Iteration 125, loss = 0.26279004\n",
      "Iteration 310, loss = 0.33108805\n",
      "Iteration 161, loss = 0.11783540\n",
      "Iteration 162, loss = 0.04957509\n",
      "Iteration 221, loss = 0.37187973\n",
      "Iteration 172, loss = 0.08786187\n",
      "Iteration 125, loss = 0.06409849\n",
      "Iteration 161, loss = 0.36621072\n",
      "Iteration 191, loss = 0.42022516\n",
      "Iteration 158, loss = 0.01383536\n",
      "Iteration 311, loss = 0.33371433\n",
      "Iteration 113, loss = 0.14700994\n",
      "Iteration 144, loss = 0.38509202\n",
      "Iteration 142, loss = 0.07052738\n",
      "Iteration 312, loss = 0.33642447\n",
      "Iteration 222, loss = 0.37035198\n",
      "Iteration 162, loss = 0.11597774\n",
      "Iteration 163, loss = 0.04781236\n",
      "Iteration 192, loss = 0.41941331\n",
      "Iteration 173, loss = 0.08684624\n",
      "Iteration 126, loss = 0.26356886\n",
      "Iteration 162, loss = 0.36620265\n",
      "Iteration 313, loss = 0.33155377\n",
      "Iteration 159, loss = 0.01334246\n",
      "Iteration 223, loss = 0.37101661\n",
      "Iteration 145, loss = 0.38535191\n",
      "Iteration 314, loss = 0.33004221\n",
      "Iteration 193, loss = 0.41999633\n",
      "Iteration 143, loss = 0.06953118\n",
      "Iteration 126, loss = 0.06249250\n",
      "Iteration 114, loss = 0.14487726\n",
      "Iteration 174, loss = 0.08851374\n",
      "Iteration 163, loss = 0.11980495\n",
      "Iteration 164, loss = 0.04502320\n",
      "Iteration 315, loss = 0.32988280\n",
      "Iteration 224, loss = 0.37071832\n",
      "Iteration 163, loss = 0.36689865\n",
      "Iteration 127, loss = 0.26264850\n",
      "Iteration 160, loss = 0.01432443\n",
      "Iteration 194, loss = 0.42001998\n",
      "Iteration 316, loss = 0.33400361\n",
      "Iteration 146, loss = 0.38477272\n",
      "Iteration 175, loss = 0.08674038\n",
      "Iteration 225, loss = 0.37272980\n",
      "Iteration 144, loss = 0.06975315\n",
      "Iteration 164, loss = 0.11658243\n",
      "Iteration 317, loss = 0.33251229\n",
      "Iteration 165, loss = 0.04724144\n",
      "Iteration 164, loss = 0.36675207\n",
      "Iteration 195, loss = 0.42035706\n",
      "Iteration 127, loss = 0.06548819\n",
      "Iteration 115, loss = 0.14579593\n",
      "Iteration 318, loss = 0.33170195\n",
      "Iteration 226, loss = 0.37047143\n",
      "Iteration 176, loss = 0.08687754\n",
      "Iteration 161, loss = 0.01260127\n",
      "Iteration 128, loss = 0.26298440\n",
      "Iteration 147, loss = 0.38552087\n",
      "Iteration 165, loss = 0.11518322\n",
      "Iteration 319, loss = 0.32930320\n",
      "Iteration 145, loss = 0.07065869\n",
      "Iteration 166, loss = 0.04579417\n",
      "Iteration 196, loss = 0.41994422\n",
      "Iteration 165, loss = 0.36468634\n",
      "Iteration 227, loss = 0.37024424\n",
      "Iteration 320, loss = 0.33099743\n",
      "Iteration 177, loss = 0.08546987\n",
      "Iteration 321, loss = 0.33245408\n",
      "Iteration 197, loss = 0.41797964\n",
      "Iteration 166, loss = 0.11512384\n",
      "Iteration 162, loss = 0.01222949\n",
      "Iteration 228, loss = 0.36952724\n",
      "Iteration 128, loss = 0.06373760\n",
      "Iteration 116, loss = 0.14554831\n",
      "Iteration 148, loss = 0.38749111\n",
      "Iteration 146, loss = 0.06877227\n",
      "Iteration 167, loss = 0.04757493\n",
      "Iteration 129, loss = 0.26210449\n",
      "Iteration 166, loss = 0.36532579\n",
      "Iteration 322, loss = 0.33140313\n",
      "Iteration 178, loss = 0.08628594\n",
      "Iteration 229, loss = 0.37086030\n",
      "Iteration 198, loss = 0.41952906\n",
      "Iteration 323, loss = 0.32930825\n",
      "Iteration 167, loss = 0.11646140\n",
      "Iteration 163, loss = 0.01374784\n",
      "Iteration 168, loss = 0.04558912\n",
      "Iteration 149, loss = 0.38351346\n",
      "Iteration 167, loss = 0.36395635\n",
      "Iteration 147, loss = 0.06837613\n",
      "Iteration 324, loss = 0.32977597\n",
      "Iteration 230, loss = 0.37064148\n",
      "Iteration 179, loss = 0.08651625\n",
      "Iteration 199, loss = 0.41998967\n",
      "Iteration 130, loss = 0.26332468\n",
      "Iteration 117, loss = 0.14563559\n",
      "Iteration 129, loss = 0.06375239\n",
      "Iteration 325, loss = 0.33027548\n",
      "Iteration 168, loss = 0.11563939\n",
      "Iteration 231, loss = 0.37028182\n",
      "Iteration 326, loss = 0.32927224\n",
      "Iteration 169, loss = 0.04427400\n",
      "Iteration 200, loss = 0.42016442\n",
      "Iteration 168, loss = 0.36410084\n",
      "Iteration 180, loss = 0.08526858\n",
      "Iteration 164, loss = 0.01192330\n",
      "Iteration 148, loss = 0.06928620\n",
      "Iteration 150, loss = 0.38254064\n",
      "Iteration 327, loss = 0.33155534\n",
      "Iteration 232, loss = 0.36892673\n",
      "Iteration 131, loss = 0.26108696\n",
      "Iteration 169, loss = 0.11714563\n",
      "Iteration 201, loss = 0.41958285\n",
      "Iteration 328, loss = 0.33230692\n",
      "Iteration 118, loss = 0.14459042\n",
      "Iteration 181, loss = 0.08603746\n",
      "Iteration 170, loss = 0.04547216\n",
      "Iteration 169, loss = 0.36445068\n",
      "Iteration 130, loss = 0.06361471\n",
      "Iteration 233, loss = 0.36887750\n",
      "Iteration 149, loss = 0.06850417\n",
      "Iteration 151, loss = 0.38401029\n",
      "Iteration 165, loss = 0.01461492\n",
      "Iteration 329, loss = 0.33182985\n",
      "Iteration 202, loss = 0.41940059\n",
      "Iteration 170, loss = 0.11404940\n",
      "Iteration 330, loss = 0.33033276\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 182, loss = 0.08789412\n",
      "Iteration 132, loss = 0.26250844\n",
      "Iteration 234, loss = 0.37084185\n",
      "Iteration 170, loss = 0.36374606\n",
      "Iteration 171, loss = 0.04482440\n",
      "Iteration 203, loss = 0.42071646\n",
      "Iteration 150, loss = 0.06838431\n",
      "Iteration 152, loss = 0.38365637\n",
      "Iteration 166, loss = 0.01258620\n",
      "Iteration 119, loss = 0.14171520\n",
      "Iteration 171, loss = 0.11384903\n",
      "Iteration 235, loss = 0.36965352\n",
      "Iteration 131, loss = 0.06546282\n",
      "Iteration 183, loss = 0.08506568\n",
      "Iteration 171, loss = 0.36450359\n",
      "Iteration 172, loss = 0.04511730\n",
      "Iteration 204, loss = 0.41699158\n",
      "Iteration 133, loss = 0.25963057\n",
      "Iteration 236, loss = 0.36998957\n",
      "Iteration 151, loss = 0.06770206\n",
      "Iteration 153, loss = 0.38203754\n",
      "Iteration 172, loss = 0.11531020\n",
      "Iteration 167, loss = 0.01239822\n",
      "Iteration 184, loss = 0.08399296\n",
      "Iteration 205, loss = 0.41830553\n",
      "Iteration 172, loss = 0.36378113\n",
      "Iteration 237, loss = 0.36876956\n",
      "Iteration 173, loss = 0.04415692\n",
      "Iteration 120, loss = 0.14314208\n",
      "Iteration 132, loss = 0.06115390\n",
      "Iteration 185, loss = 0.08436954\n",
      "Iteration 152, loss = 0.06889487\n",
      "Iteration 173, loss = 0.11455581\n",
      "Iteration 134, loss = 0.26235586\n",
      "Iteration 206, loss = 0.41653846\n",
      "Iteration 154, loss = 0.38250393\n",
      "Iteration 238, loss = 0.36945029\n",
      "Iteration 168, loss = 0.01299139\n",
      "Iteration 173, loss = 0.36404352\n",
      "Iteration 174, loss = 0.04499939\n",
      "Iteration 186, loss = 0.08429310\n",
      "Iteration 239, loss = 0.36764299\n",
      "Iteration 207, loss = 0.41658586\n",
      "Iteration 174, loss = 0.11394366\n",
      "Iteration 121, loss = 0.14158939\n",
      "Iteration 153, loss = 0.06729634\n",
      "Iteration 155, loss = 0.38183535\n",
      "Iteration 133, loss = 0.06223657\n",
      "Iteration 169, loss = 0.01308037\n",
      "Iteration 174, loss = 0.36278399\n",
      "Iteration 135, loss = 0.26107197\n",
      "Iteration 240, loss = 0.36824014\n",
      "Iteration 175, loss = 0.04301992\n",
      "Iteration 208, loss = 0.41765795\n",
      "Iteration 187, loss = 0.08336199\n",
      "Iteration 175, loss = 0.11392715\n",
      "Iteration 154, loss = 0.06643113\n",
      "Iteration 241, loss = 0.36965030\n",
      "Iteration 209, loss = 0.41782471\n",
      "Iteration 156, loss = 0.38225344\n",
      "Iteration 175, loss = 0.36211417\n",
      "Iteration 170, loss = 0.01348286\n",
      "Iteration 176, loss = 0.04256065\n",
      "Iteration 188, loss = 0.08455726\n",
      "Iteration 122, loss = 0.14209366\n",
      "Iteration 136, loss = 0.25874894\n",
      "Iteration 176, loss = 0.11363069\n",
      "Iteration 242, loss = 0.36782630\n",
      "Iteration 134, loss = 0.06226593\n",
      "Iteration 210, loss = 0.41642016\n",
      "Iteration 155, loss = 0.06688402\n",
      "Iteration 176, loss = 0.36342212\n",
      "Iteration 189, loss = 0.08555483\n",
      "Iteration 157, loss = 0.38061430\n",
      "Iteration 177, loss = 0.04592254\n",
      "Iteration 243, loss = 0.36887163\n",
      "Iteration 171, loss = 0.01288280\n",
      "Iteration 177, loss = 0.11805051\n",
      "Iteration 211, loss = 0.41696195\n",
      "Iteration 137, loss = 0.25780233\n",
      "Iteration 123, loss = 0.14155003\n",
      "Iteration 244, loss = 0.36834098\n",
      "Iteration 156, loss = 0.06684386\n",
      "Iteration 190, loss = 0.08464127\n",
      "Iteration 177, loss = 0.36246689\n",
      "Iteration 135, loss = 0.06153047\n",
      "Iteration 178, loss = 0.04886821\n",
      "Iteration 158, loss = 0.38133729\n",
      "Iteration 212, loss = 0.41662522\n",
      "Iteration 178, loss = 0.11719020\n",
      "Iteration 172, loss = 0.01182042\n",
      "Iteration 245, loss = 0.36940694\n",
      "Iteration 191, loss = 0.08397849\n",
      "Iteration 178, loss = 0.36223528\n",
      "Iteration 138, loss = 0.25861510\n",
      "Iteration 213, loss = 0.41596134\n",
      "Iteration 157, loss = 0.06539928\n",
      "Iteration 179, loss = 0.04227189\n",
      "Iteration 246, loss = 0.36746323\n",
      "Iteration 124, loss = 0.14058654\n",
      "Iteration 159, loss = 0.38253747\n",
      "Iteration 179, loss = 0.11133727\n",
      "Iteration 173, loss = 0.01232369\n",
      "Iteration 192, loss = 0.08271424\n",
      "Iteration 136, loss = 0.05843069\n",
      "Iteration 214, loss = 0.41576532\n",
      "Iteration 179, loss = 0.36241998\n",
      "Iteration 247, loss = 0.36768637\n",
      "Iteration 158, loss = 0.06751727\n",
      "Iteration 180, loss = 0.04134081\n",
      "Iteration 139, loss = 0.25933358\n",
      "Iteration 180, loss = 0.11193150\n",
      "Iteration 160, loss = 0.38120130\n",
      "Iteration 215, loss = 0.41662497\n",
      "Iteration 193, loss = 0.08450383\n",
      "Iteration 248, loss = 0.36823599\n",
      "Iteration 174, loss = 0.01266735\n",
      "Iteration 125, loss = 0.13978832\n",
      "Iteration 180, loss = 0.36196167\n",
      "Iteration 181, loss = 0.04276611\n",
      "Iteration 159, loss = 0.06659344\n",
      "Iteration 216, loss = 0.41756980\n",
      "Iteration 181, loss = 0.11164884\n",
      "Iteration 249, loss = 0.36700278\n",
      "Iteration 137, loss = 0.06214485\n",
      "Iteration 194, loss = 0.08371082\n",
      "Iteration 161, loss = 0.38022698\n",
      "Iteration 140, loss = 0.26022966\n",
      "Iteration 175, loss = 0.01248470\n",
      "Iteration 181, loss = 0.36113394\n",
      "Iteration 217, loss = 0.42130549\n",
      "Iteration 250, loss = 0.36645360\n",
      "Iteration 182, loss = 0.04237948\n",
      "Iteration 182, loss = 0.11214800\n",
      "Iteration 160, loss = 0.06557726\n",
      "Iteration 195, loss = 0.08290317\n",
      "Iteration 126, loss = 0.13800035\n",
      "Iteration 162, loss = 0.38203433\n",
      "Iteration 251, loss = 0.36635773\n",
      "Iteration 218, loss = 0.41617849\n",
      "Iteration 182, loss = 0.36221023\n",
      "Iteration 138, loss = 0.05920818\n",
      "Iteration 141, loss = 0.25833658\n",
      "Iteration 176, loss = 0.01204001\n",
      "Iteration 196, loss = 0.08244275\n",
      "Iteration 183, loss = 0.04351992\n",
      "Iteration 183, loss = 0.11208426\n",
      "Iteration 252, loss = 0.36738106\n",
      "Iteration 161, loss = 0.06634430\n",
      "Iteration 219, loss = 0.41490444\n",
      "Iteration 163, loss = 0.38076969\n",
      "Iteration 183, loss = 0.36075771\n",
      "Iteration 127, loss = 0.14044881\n",
      "Iteration 197, loss = 0.08064868\n",
      "Iteration 253, loss = 0.36724980\n",
      "Iteration 177, loss = 0.01239834\n",
      "Iteration 184, loss = 0.11457237\n",
      "Iteration 184, loss = 0.04349179\n",
      "Iteration 142, loss = 0.25782357\n",
      "Iteration 220, loss = 0.41386319\n",
      "Iteration 162, loss = 0.06458424\n",
      "Iteration 139, loss = 0.06226280\n",
      "Iteration 254, loss = 0.36660137\n",
      "Iteration 184, loss = 0.36134943\n",
      "Iteration 164, loss = 0.37904876\n",
      "Iteration 198, loss = 0.08314425\n",
      "Iteration 221, loss = 0.41492263\n",
      "Iteration 185, loss = 0.10949753\n",
      "Iteration 185, loss = 0.04288876\n",
      "Iteration 178, loss = 0.01159495\n",
      "Iteration 255, loss = 0.36736960\n",
      "Iteration 128, loss = 0.13856072\n",
      "Iteration 163, loss = 0.06645430\n",
      "Iteration 143, loss = 0.25699902\n",
      "Iteration 199, loss = 0.08160553\n",
      "Iteration 222, loss = 0.41560190\n",
      "Iteration 185, loss = 0.36116842\n",
      "Iteration 165, loss = 0.37861992\n",
      "Iteration 140, loss = 0.06033974\n",
      "Iteration 186, loss = 0.11087486\n",
      "Iteration 256, loss = 0.36718173\n",
      "Iteration 186, loss = 0.04070905\n",
      "Iteration 179, loss = 0.01156550\n",
      "Iteration 223, loss = 0.41901683\n",
      "Iteration 164, loss = 0.06334970\n",
      "Iteration 200, loss = 0.08287640\n",
      "Iteration 257, loss = 0.36650875\n",
      "Iteration 186, loss = 0.36013779\n",
      "Iteration 144, loss = 0.25621219\n",
      "Iteration 129, loss = 0.13741586\n",
      "Iteration 187, loss = 0.11055961\n",
      "Iteration 166, loss = 0.38154863\n",
      "Iteration 187, loss = 0.04141972\n",
      "Iteration 224, loss = 0.41646986\n",
      "Iteration 258, loss = 0.36607090\n",
      "Iteration 201, loss = 0.08046549\n",
      "Iteration 180, loss = 0.01266599\n",
      "Iteration 141, loss = 0.05996810\n",
      "Iteration 165, loss = 0.06524236\n",
      "Iteration 187, loss = 0.36113460\n",
      "Iteration 188, loss = 0.10991446\n",
      "Iteration 225, loss = 0.41371412\n",
      "Iteration 259, loss = 0.36558241\n",
      "Iteration 145, loss = 0.25684466\n",
      "Iteration 188, loss = 0.04060891\n",
      "Iteration 167, loss = 0.37790006\n",
      "Iteration 202, loss = 0.08455304\n",
      "Iteration 130, loss = 0.13631699\n",
      "Iteration 181, loss = 0.01220078\n",
      "Iteration 188, loss = 0.36122325\n",
      "Iteration 226, loss = 0.41360748\n",
      "Iteration 166, loss = 0.06501254\n",
      "Iteration 260, loss = 0.36551338\n",
      "Iteration 189, loss = 0.11103246\n",
      "Iteration 189, loss = 0.04246655\n",
      "Iteration 203, loss = 0.08020843\n",
      "Iteration 142, loss = 0.06034753\n",
      "Iteration 168, loss = 0.37846245\n",
      "Iteration 261, loss = 0.36600129\n",
      "Iteration 146, loss = 0.25759354\n",
      "Iteration 227, loss = 0.41446168\n",
      "Iteration 189, loss = 0.36095432\n",
      "Iteration 182, loss = 0.01163639\n",
      "Iteration 167, loss = 0.06383327\n",
      "Iteration 190, loss = 0.11420245\n",
      "Iteration 131, loss = 0.13729319\n",
      "Iteration 204, loss = 0.07988971\n",
      "Iteration 262, loss = 0.36575529\n",
      "Iteration 190, loss = 0.03949319\n",
      "Iteration 228, loss = 0.41464624\n",
      "Iteration 169, loss = 0.37910644\n",
      "Iteration 190, loss = 0.35952888\n",
      "Iteration 143, loss = 0.05785653\n",
      "Iteration 147, loss = 0.25747974\n",
      "Iteration 263, loss = 0.36457505\n",
      "Iteration 191, loss = 0.10986568\n",
      "Iteration 168, loss = 0.06283512\n",
      "Iteration 183, loss = 0.01100178\n",
      "Iteration 205, loss = 0.08047054\n",
      "Iteration 229, loss = 0.41391883\n",
      "Iteration 191, loss = 0.04325343\n",
      "Iteration 264, loss = 0.36633658\n",
      "Iteration 170, loss = 0.37756414\n",
      "Iteration 132, loss = 0.13677314\n",
      "Iteration 191, loss = 0.36129685\n",
      "Iteration 230, loss = 0.41434865\n",
      "Iteration 206, loss = 0.08072119\n",
      "Iteration 192, loss = 0.11174589\n",
      "Iteration 169, loss = 0.06587556\n",
      "Iteration 184, loss = 0.01246664\n",
      "Iteration 148, loss = 0.25549524\n",
      "Iteration 192, loss = 0.04346543\n",
      "Iteration 265, loss = 0.36535899\n",
      "Iteration 144, loss = 0.05946074\n",
      "Iteration 231, loss = 0.41458952\n",
      "Iteration 192, loss = 0.35888580\n",
      "Iteration 171, loss = 0.37836325\n",
      "Iteration 207, loss = 0.08286758\n",
      "Iteration 193, loss = 0.10892436\n",
      "Iteration 266, loss = 0.36548759\n",
      "Iteration 133, loss = 0.13470561\n",
      "Iteration 170, loss = 0.06425764\n",
      "Iteration 193, loss = 0.04150660\n",
      "Iteration 185, loss = 0.01067182\n",
      "Iteration 232, loss = 0.41563217\n",
      "Iteration 149, loss = 0.25534132\n",
      "Iteration 267, loss = 0.36539363\n",
      "Iteration 193, loss = 0.35860067\n",
      "Iteration 208, loss = 0.08121369\n",
      "Iteration 172, loss = 0.38097906\n",
      "Iteration 194, loss = 0.11005205\n",
      "Iteration 145, loss = 0.05761595\n",
      "Iteration 233, loss = 0.41780818\n",
      "Iteration 171, loss = 0.06383400\n",
      "Iteration 194, loss = 0.03968593\n",
      "Iteration 268, loss = 0.36651037\n",
      "Iteration 186, loss = 0.01222486\n",
      "Iteration 209, loss = 0.07920013\n",
      "Iteration 194, loss = 0.36200197\n",
      "Iteration 134, loss = 0.13617912\n",
      "Iteration 195, loss = 0.11015095\n",
      "Iteration 234, loss = 0.41321375\n",
      "Iteration 150, loss = 0.25512054\n",
      "Iteration 173, loss = 0.37706337\n",
      "Iteration 269, loss = 0.36435390\n",
      "Iteration 195, loss = 0.03935467\n",
      "Iteration 172, loss = 0.06237743\n",
      "Iteration 210, loss = 0.07888265\n",
      "Iteration 235, loss = 0.41326648\n",
      "Iteration 187, loss = 0.01169246\n",
      "Iteration 146, loss = 0.05939042\n",
      "Iteration 195, loss = 0.35934393\n",
      "Iteration 270, loss = 0.36491390\n",
      "Iteration 196, loss = 0.10895485\n",
      "Iteration 174, loss = 0.37610245\n",
      "Iteration 151, loss = 0.25554398\n",
      "Iteration 135, loss = 0.13451377\n",
      "Iteration 211, loss = 0.08067146\n",
      "Iteration 196, loss = 0.03816823\n",
      "Iteration 236, loss = 0.41248345\n",
      "Iteration 271, loss = 0.36407693\n",
      "Iteration 173, loss = 0.06257508\n",
      "Iteration 196, loss = 0.35795436\n",
      "Iteration 197, loss = 0.10907023\n",
      "Iteration 188, loss = 0.01133095\n",
      "Iteration 272, loss = 0.36359515\n",
      "Iteration 237, loss = 0.41509103\n",
      "Iteration 212, loss = 0.07948016\n",
      "Iteration 147, loss = 0.05967648\n",
      "Iteration 175, loss = 0.37713097\n",
      "Iteration 197, loss = 0.04146379\n",
      "Iteration 152, loss = 0.25532193\n",
      "Iteration 174, loss = 0.06390377\n",
      "Iteration 197, loss = 0.35912367\n",
      "Iteration 198, loss = 0.10992253\n",
      "Iteration 273, loss = 0.36510641\n",
      "Iteration 136, loss = 0.13365296\n",
      "Iteration 238, loss = 0.41246915\n",
      "Iteration 189, loss = 0.01232168\n",
      "Iteration 213, loss = 0.08064689\n",
      "Iteration 198, loss = 0.04125627\n",
      "Iteration 176, loss = 0.37955092\n",
      "Iteration 274, loss = 0.36504902\n",
      "Iteration 198, loss = 0.35813240\n",
      "Iteration 239, loss = 0.41209466\n",
      "Iteration 175, loss = 0.06401092\n",
      "Iteration 199, loss = 0.10646673\n",
      "Iteration 148, loss = 0.05725687\n",
      "Iteration 153, loss = 0.25346823\n",
      "Iteration 214, loss = 0.07949086\n",
      "Iteration 190, loss = 0.01014372\n",
      "Iteration 275, loss = 0.36282227\n",
      "Iteration 137, loss = 0.13406195\n",
      "Iteration 199, loss = 0.04063421\n",
      "Iteration 240, loss = 0.41146760\n",
      "Iteration 177, loss = 0.37744170\n",
      "Iteration 199, loss = 0.35847677\n",
      "Iteration 200, loss = 0.10728103\n",
      "Iteration 176, loss = 0.06277746\n",
      "Iteration 215, loss = 0.07863927\n",
      "Iteration 276, loss = 0.36346158\n",
      "Iteration 241, loss = 0.41226379\n",
      "Iteration 154, loss = 0.25301954\n",
      "Iteration 191, loss = 0.01117639\n",
      "Iteration 200, loss = 0.03865919\n",
      "Iteration 149, loss = 0.05745622\n",
      "Iteration 200, loss = 0.35900445\n",
      "Iteration 277, loss = 0.36302584\n",
      "Iteration 201, loss = 0.10960883\n",
      "Iteration 178, loss = 0.37528133\n",
      "Iteration 216, loss = 0.07913762\n",
      "Iteration 177, loss = 0.06171781\n",
      "Iteration 242, loss = 0.41521452\n",
      "Iteration 138, loss = 0.13342335\n",
      "Iteration 278, loss = 0.36502753\n",
      "Iteration 192, loss = 0.01177288\n",
      "Iteration 201, loss = 0.03979973\n",
      "Iteration 155, loss = 0.25309871\n",
      "Iteration 217, loss = 0.07911670\n",
      "Iteration 201, loss = 0.35746012\n",
      "Iteration 202, loss = 0.10809468\n",
      "Iteration 243, loss = 0.41302414\n",
      "Iteration 179, loss = 0.37511431\n",
      "Iteration 279, loss = 0.36413543\n",
      "Iteration 178, loss = 0.06138818\n",
      "Iteration 150, loss = 0.05785956\n",
      "Iteration 218, loss = 0.07799871\n",
      "Iteration 244, loss = 0.41082683\n",
      "Iteration 202, loss = 0.03960057\n",
      "Iteration 139, loss = 0.13296222\n",
      "Iteration 193, loss = 0.01077668\n",
      "Iteration 202, loss = 0.35766089\n",
      "Iteration 203, loss = 0.10735617\n",
      "Iteration 280, loss = 0.36273344\n",
      "Iteration 156, loss = 0.25435228\n",
      "Iteration 180, loss = 0.37539337\n",
      "Iteration 179, loss = 0.06214211\n",
      "Iteration 245, loss = 0.41229863\n",
      "Iteration 219, loss = 0.07751129\n",
      "Iteration 281, loss = 0.36254036\n",
      "Iteration 203, loss = 0.04027334\n",
      "Iteration 203, loss = 0.35767926\n",
      "Iteration 204, loss = 0.10780507\n",
      "Iteration 151, loss = 0.05673826\n",
      "Iteration 194, loss = 0.01173942\n",
      "Iteration 246, loss = 0.41086105\n",
      "Iteration 140, loss = 0.13289868\n",
      "Iteration 282, loss = 0.36370797\n",
      "Iteration 181, loss = 0.37586418\n",
      "Iteration 180, loss = 0.06107287\n",
      "Iteration 220, loss = 0.08035260\n",
      "Iteration 157, loss = 0.25409091\n",
      "Iteration 204, loss = 0.04013689\n",
      "Iteration 204, loss = 0.35868899\n",
      "Iteration 205, loss = 0.10604832\n",
      "Iteration 247, loss = 0.41336841\n",
      "Iteration 283, loss = 0.36286597\n",
      "Iteration 195, loss = 0.01014471\n",
      "Iteration 221, loss = 0.07788909\n",
      "Iteration 182, loss = 0.37772722\n",
      "Iteration 152, loss = 0.05789601\n",
      "Iteration 181, loss = 0.06154918\n",
      "Iteration 284, loss = 0.36226101\n",
      "Iteration 248, loss = 0.41502015\n",
      "Iteration 205, loss = 0.35771645\n",
      "Iteration 205, loss = 0.03960581\n",
      "Iteration 206, loss = 0.10654692\n",
      "Iteration 158, loss = 0.25339517\n",
      "Iteration 141, loss = 0.13260037\n",
      "Iteration 222, loss = 0.08068165\n",
      "Iteration 196, loss = 0.01116340\n",
      "Iteration 285, loss = 0.36181176\n",
      "Iteration 249, loss = 0.41729385\n",
      "Iteration 183, loss = 0.37533907\n",
      "Iteration 182, loss = 0.06248992\n",
      "Iteration 206, loss = 0.35621342\n",
      "Iteration 207, loss = 0.10518016\n",
      "Iteration 206, loss = 0.04047322\n",
      "Iteration 153, loss = 0.05609383\n",
      "Iteration 286, loss = 0.36248318\n",
      "Iteration 223, loss = 0.07833606\n",
      "Iteration 159, loss = 0.25247233\n",
      "Iteration 250, loss = 0.41214166\n",
      "Iteration 197, loss = 0.01080829\n",
      "Iteration 142, loss = 0.13094692\n",
      "Iteration 184, loss = 0.37410824\n",
      "Iteration 183, loss = 0.06089429\n",
      "Iteration 287, loss = 0.36201211\n",
      "Iteration 207, loss = 0.35697389\n",
      "Iteration 208, loss = 0.10831002\n",
      "Iteration 207, loss = 0.03693124\n",
      "Iteration 224, loss = 0.07662396\n",
      "Iteration 251, loss = 0.40964246\n",
      "Iteration 288, loss = 0.36211149\n",
      "Iteration 160, loss = 0.25189696\n",
      "Iteration 198, loss = 0.01095262\n",
      "Iteration 154, loss = 0.05754965\n",
      "Iteration 208, loss = 0.35779925\n",
      "Iteration 209, loss = 0.10761610\n",
      "Iteration 252, loss = 0.41076843\n",
      "Iteration 185, loss = 0.37523387\n",
      "Iteration 184, loss = 0.06146303\n",
      "Iteration 208, loss = 0.03742872\n",
      "Iteration 225, loss = 0.07770127\n",
      "Iteration 143, loss = 0.13133541\n",
      "Iteration 289, loss = 0.36236098\n",
      "Iteration 253, loss = 0.41029655\n",
      "Iteration 199, loss = 0.01144363\n",
      "Iteration 209, loss = 0.35703653\n",
      "Iteration 226, loss = 0.07835099\n",
      "Iteration 210, loss = 0.10434682\n",
      "Iteration 290, loss = 0.36335689\n",
      "Iteration 161, loss = 0.25567895\n",
      "Iteration 209, loss = 0.03911265\n",
      "Iteration 185, loss = 0.06026034\n",
      "Iteration 186, loss = 0.37678685\n",
      "Iteration 155, loss = 0.05853820\n",
      "Iteration 254, loss = 0.41008314\n",
      "Iteration 144, loss = 0.13059327\n",
      "Iteration 291, loss = 0.36105781\n",
      "Iteration 227, loss = 0.07633156\n",
      "Iteration 210, loss = 0.35561232\n",
      "Iteration 211, loss = 0.11068852\n",
      "Iteration 200, loss = 0.01133285\n",
      "Iteration 210, loss = 0.03857789\n",
      "Iteration 186, loss = 0.06201174\n",
      "Iteration 255, loss = 0.41151740\n",
      "Iteration 187, loss = 0.37289891\n",
      "Iteration 162, loss = 0.25244724\n",
      "Iteration 292, loss = 0.36093391\n",
      "Iteration 228, loss = 0.07679389\n",
      "Iteration 212, loss = 0.10840688\n",
      "Iteration 211, loss = 0.35705886\n",
      "Iteration 256, loss = 0.41126989\n",
      "Iteration 156, loss = 0.05421726\n",
      "Iteration 211, loss = 0.03819113\n",
      "Iteration 293, loss = 0.36164009\n",
      "Iteration 201, loss = 0.01171816\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 145, loss = 0.13231247\n",
      "Iteration 187, loss = 0.06103357\n",
      "Iteration 188, loss = 0.37416619\n",
      "Iteration 229, loss = 0.07651972\n",
      "Iteration 163, loss = 0.24991691\n",
      "Iteration 257, loss = 0.41473627\n",
      "Iteration 294, loss = 0.36168953\n",
      "Iteration 213, loss = 0.10597964\n",
      "Iteration 212, loss = 0.35746481\n",
      "Iteration 212, loss = 0.03962452\n",
      "Iteration 188, loss = 0.06123183\n",
      "Iteration 230, loss = 0.07782153\n",
      "Iteration 295, loss = 0.36201088\n",
      "Iteration 258, loss = 0.41356766\n",
      "Iteration 189, loss = 0.37518148\n",
      "Iteration 157, loss = 0.05589650\n",
      "Iteration 146, loss = 0.13091560\n",
      "Iteration 214, loss = 0.10543913\n",
      "Iteration 213, loss = 0.35595831\n",
      "Iteration 164, loss = 0.25139773\n",
      "Iteration 213, loss = 0.03864922\n",
      "Iteration 296, loss = 0.36246163\n",
      "Iteration 259, loss = 0.40908886\n",
      "Iteration 231, loss = 0.07567925\n",
      "Iteration 189, loss = 0.05966271\n",
      "Iteration 190, loss = 0.37453792\n",
      "Iteration 215, loss = 0.10318724\n",
      "Iteration 297, loss = 0.35997170\n",
      "Iteration 214, loss = 0.35608285\n",
      "Iteration 260, loss = 0.40872812\n",
      "Iteration 214, loss = 0.03914850\n",
      "Iteration 232, loss = 0.07684422\n",
      "Iteration 158, loss = 0.05668855\n",
      "Iteration 147, loss = 0.12856429\n",
      "Iteration 165, loss = 0.25039246\n",
      "Iteration 298, loss = 0.36113867\n",
      "Iteration 190, loss = 0.06007706\n",
      "Iteration 191, loss = 0.37392281\n",
      "Iteration 216, loss = 0.10482455\n",
      "Iteration 261, loss = 0.41039331\n",
      "Iteration 215, loss = 0.35556432\n",
      "Iteration 233, loss = 0.07677024\n",
      "Iteration 215, loss = 0.03643953\n",
      "Iteration 299, loss = 0.36163433\n",
      "Iteration 262, loss = 0.41044473\n",
      "Iteration 191, loss = 0.05918762\n",
      "Iteration 166, loss = 0.25086967\n",
      "Iteration 217, loss = 0.10499391\n",
      "Iteration 192, loss = 0.37295357\n",
      "Iteration 159, loss = 0.05525413\n",
      "Iteration 148, loss = 0.12848111\n",
      "Iteration 216, loss = 0.35594239\n",
      "Iteration 300, loss = 0.36242731\n",
      "Iteration 234, loss = 0.07777142\n",
      "Iteration 216, loss = 0.03802476\n",
      "Iteration 263, loss = 0.41071274\n",
      "Iteration 301, loss = 0.36542003\n",
      "Iteration 218, loss = 0.10562763\n",
      "Iteration 192, loss = 0.06153429\n",
      "Iteration 235, loss = 0.07730754\n",
      "Iteration 217, loss = 0.35564076\n",
      "Iteration 193, loss = 0.37318422\n",
      "Iteration 167, loss = 0.25049069\n",
      "Iteration 264, loss = 0.41564105\n",
      "Iteration 217, loss = 0.03783887\n",
      "Iteration 302, loss = 0.36071736\n",
      "Iteration 149, loss = 0.12752993\n",
      "Iteration 160, loss = 0.05497852\n",
      "Iteration 219, loss = 0.10639447\n",
      "Iteration 236, loss = 0.07370221\n",
      "Iteration 265, loss = 0.40889639\n",
      "Iteration 193, loss = 0.05909076\n",
      "Iteration 218, loss = 0.35561353\n",
      "Iteration 303, loss = 0.35892179\n",
      "Iteration 194, loss = 0.37423044\n",
      "Iteration 218, loss = 0.03864289\n",
      "Iteration 168, loss = 0.25102271\n",
      "Iteration 266, loss = 0.40988972\n",
      "Iteration 237, loss = 0.07622103\n",
      "Iteration 220, loss = 0.10409377\n",
      "Iteration 304, loss = 0.35963188\n",
      "Iteration 219, loss = 0.35384300\n",
      "Iteration 194, loss = 0.05844139\n",
      "Iteration 150, loss = 0.12775564\n",
      "Iteration 161, loss = 0.05426747\n",
      "Iteration 195, loss = 0.37296436\n",
      "Iteration 219, loss = 0.03931032\n",
      "Iteration 267, loss = 0.40882777\n",
      "Iteration 305, loss = 0.35907784\n",
      "Iteration 238, loss = 0.07630546\n",
      "Iteration 221, loss = 0.10235398\n",
      "Iteration 169, loss = 0.24884676\n",
      "Iteration 220, loss = 0.35564689\n",
      "Iteration 195, loss = 0.05987286\n",
      "Iteration 268, loss = 0.40952041\n",
      "Iteration 306, loss = 0.36002819\n",
      "Iteration 220, loss = 0.03754017\n",
      "Iteration 196, loss = 0.37212011\n",
      "Iteration 239, loss = 0.07524875\n",
      "Iteration 151, loss = 0.12776756\n",
      "Iteration 222, loss = 0.10323562\n",
      "Iteration 162, loss = 0.05557102\n",
      "Iteration 307, loss = 0.36186191\n",
      "Iteration 269, loss = 0.40926455\n",
      "Iteration 221, loss = 0.35500235\n",
      "Iteration 170, loss = 0.25023103\n",
      "Iteration 196, loss = 0.05975108\n",
      "Iteration 240, loss = 0.07686270\n",
      "Iteration 221, loss = 0.03739008\n",
      "Iteration 197, loss = 0.37336724\n",
      "Iteration 308, loss = 0.35961064\n",
      "Iteration 223, loss = 0.10400611\n",
      "Iteration 270, loss = 0.41050748\n",
      "Iteration 222, loss = 0.35418100\n",
      "Iteration 152, loss = 0.12826147\n",
      "Iteration 309, loss = 0.35980627\n",
      "Iteration 241, loss = 0.07360358\n",
      "Iteration 197, loss = 0.05810706\n",
      "Iteration 163, loss = 0.05595437\n",
      "Iteration 171, loss = 0.25119643\n",
      "Iteration 222, loss = 0.03735612\n",
      "Iteration 271, loss = 0.41241066\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 224, loss = 0.10565940\n",
      "Iteration 198, loss = 0.37184208\n",
      "Iteration 310, loss = 0.36211531\n",
      "Iteration 223, loss = 0.35467445\n",
      "Iteration 242, loss = 0.07459276\n",
      "Iteration 198, loss = 0.05852969\n",
      "Iteration 223, loss = 0.03550122\n",
      "Iteration 225, loss = 0.10251634\n",
      "Iteration 311, loss = 0.35935666\n",
      "Iteration 153, loss = 0.12640060\n",
      "Iteration 172, loss = 0.24960471\n",
      "Iteration 199, loss = 0.37169496\n",
      "Iteration 224, loss = 0.35394057\n",
      "Iteration 164, loss = 0.05371280\n",
      "Iteration 243, loss = 0.07419415\n",
      "Iteration 312, loss = 0.35897796\n",
      "Iteration 199, loss = 0.05842596\n",
      "Iteration 224, loss = 0.03614463\n",
      "Iteration 226, loss = 0.10267499\n",
      "Iteration 200, loss = 0.37351224\n",
      "Iteration 225, loss = 0.35508396\n",
      "Iteration 244, loss = 0.07532757\n",
      "Iteration 313, loss = 0.36053369\n",
      "Iteration 173, loss = 0.25037322\n",
      "Iteration 154, loss = 0.12710523\n",
      "Iteration 227, loss = 0.10128814\n",
      "Iteration 225, loss = 0.04030560\n",
      "Iteration 165, loss = 0.05501941\n",
      "Iteration 200, loss = 0.06018393\n",
      "Iteration 314, loss = 0.35915759\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 245, loss = 0.07463486\n",
      "Iteration 226, loss = 0.35514397\n",
      "Iteration 201, loss = 0.37076951\n",
      "Iteration 174, loss = 0.24813844\n",
      "Iteration 228, loss = 0.10409665\n",
      "Iteration 226, loss = 0.03490005\n",
      "Iteration 246, loss = 0.07368205\n",
      "Iteration 201, loss = 0.05883988\n",
      "Iteration 155, loss = 0.12557357\n",
      "Iteration 227, loss = 0.35310098\n",
      "Iteration 202, loss = 0.37111100\n",
      "Iteration 166, loss = 0.05362621\n",
      "Iteration 229, loss = 0.10340132\n",
      "Iteration 247, loss = 0.07434630\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 227, loss = 0.03542484\n",
      "Iteration 175, loss = 0.24845593\n",
      "Iteration 202, loss = 0.05740592\n",
      "Iteration 228, loss = 0.35467983\n",
      "Iteration 203, loss = 0.37418724\n",
      "Iteration 156, loss = 0.12680300\n",
      "Iteration 230, loss = 0.09984901\n",
      "Iteration 228, loss = 0.03859621\n",
      "Iteration 167, loss = 0.05369986\n",
      "Iteration 229, loss = 0.35291211\n",
      "Iteration 203, loss = 0.05832496\n",
      "Iteration 176, loss = 0.24884557\n",
      "Iteration 204, loss = 0.37199890\n",
      "Iteration 231, loss = 0.10152701\n",
      "Iteration 229, loss = 0.03746006\n",
      "Iteration 230, loss = 0.35382425\n",
      "Iteration 157, loss = 0.12574139\n",
      "Iteration 204, loss = 0.05746962\n",
      "Iteration 168, loss = 0.05247029\n",
      "Iteration 232, loss = 0.10303316\n",
      "Iteration 177, loss = 0.24830917\n",
      "Iteration 205, loss = 0.37160563\n",
      "Iteration 230, loss = 0.03537901\n",
      "Iteration 231, loss = 0.35249069\n",
      "Iteration 205, loss = 0.05750327\n",
      "Iteration 233, loss = 0.10165905\n",
      "Iteration 158, loss = 0.12370900\n",
      "Iteration 206, loss = 0.37030914\n",
      "Iteration 231, loss = 0.03580854\n",
      "Iteration 178, loss = 0.24776648\n",
      "Iteration 232, loss = 0.35405663\n",
      "Iteration 169, loss = 0.05298679\n",
      "Iteration 206, loss = 0.05705970\n",
      "Iteration 234, loss = 0.10296128\n",
      "Iteration 207, loss = 0.37068755\n",
      "Iteration 232, loss = 0.03506866\n",
      "Iteration 233, loss = 0.35295109\n",
      "Iteration 159, loss = 0.12346429\n",
      "Iteration 179, loss = 0.24766186\n",
      "Iteration 207, loss = 0.05872057\n",
      "Iteration 235, loss = 0.10108740\n",
      "Iteration 170, loss = 0.05378637\n",
      "Iteration 208, loss = 0.37037595\n",
      "Iteration 234, loss = 0.35199920\n",
      "Iteration 233, loss = 0.03683872\n",
      "Iteration 236, loss = 0.10206635\n",
      "Iteration 180, loss = 0.24925075\n",
      "Iteration 208, loss = 0.05842673\n",
      "Iteration 160, loss = 0.12702528\n",
      "Iteration 235, loss = 0.35370631\n",
      "Iteration 209, loss = 0.36989429\n",
      "Iteration 234, loss = 0.03606263\n",
      "Iteration 171, loss = 0.05417794\n",
      "Iteration 237, loss = 0.10174602\n",
      "Iteration 209, loss = 0.05607465\n",
      "Iteration 181, loss = 0.24515632\n",
      "Iteration 236, loss = 0.35249528\n",
      "Iteration 235, loss = 0.03564539\n",
      "Iteration 210, loss = 0.37192874\n",
      "Iteration 161, loss = 0.12361684\n",
      "Iteration 238, loss = 0.10021354\n",
      "Iteration 210, loss = 0.05777393\n",
      "Iteration 172, loss = 0.05240220\n",
      "Iteration 237, loss = 0.35220367\n",
      "Iteration 236, loss = 0.03554297\n",
      "Iteration 182, loss = 0.24593228\n",
      "Iteration 211, loss = 0.36836120\n",
      "Iteration 239, loss = 0.10026776\n",
      "Iteration 211, loss = 0.05696767\n",
      "Iteration 162, loss = 0.12399113\n",
      "Iteration 238, loss = 0.35174732\n",
      "Iteration 237, loss = 0.03506781\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 240, loss = 0.10123042\n",
      "Iteration 212, loss = 0.36817207\n",
      "Iteration 173, loss = 0.05302745\n",
      "Iteration 183, loss = 0.24724523\n",
      "Iteration 212, loss = 0.05627128\n",
      "Iteration 239, loss = 0.35325758\n",
      "Iteration 241, loss = 0.09883061\n",
      "Iteration 163, loss = 0.12352040\n",
      "Iteration 213, loss = 0.36995568\n",
      "Iteration 240, loss = 0.35259924\n",
      "Iteration 184, loss = 0.24784774\n",
      "Iteration 213, loss = 0.05571422\n",
      "Iteration 174, loss = 0.05188046\n",
      "Iteration 242, loss = 0.09979344\n",
      "Iteration 214, loss = 0.36860706\n",
      "Iteration 164, loss = 0.12321914\n",
      "Iteration 241, loss = 0.35134506\n",
      "Iteration 214, loss = 0.05593205\n",
      "Iteration 243, loss = 0.09901690\n",
      "Iteration 185, loss = 0.24720583\n",
      "Iteration 215, loss = 0.36989056\n",
      "Iteration 175, loss = 0.05193635\n",
      "Iteration 242, loss = 0.35249954\n",
      "Iteration 244, loss = 0.10411630\n",
      "Iteration 215, loss = 0.05600126\n",
      "Iteration 165, loss = 0.12184049\n",
      "Iteration 186, loss = 0.24518616\n",
      "Iteration 216, loss = 0.36896364\n",
      "Iteration 243, loss = 0.35260220\n",
      "Iteration 245, loss = 0.09877219\n",
      "Iteration 216, loss = 0.05648463\n",
      "Iteration 176, loss = 0.05251611\n",
      "Iteration 244, loss = 0.35103613\n",
      "Iteration 187, loss = 0.24637303\n",
      "Iteration 217, loss = 0.36872299\n",
      "Iteration 166, loss = 0.12411249\n",
      "Iteration 246, loss = 0.09858501\n",
      "Iteration 217, loss = 0.05389330\n",
      "Iteration 245, loss = 0.35067883\n",
      "Iteration 177, loss = 0.05129416\n",
      "Iteration 247, loss = 0.09825623\n",
      "Iteration 218, loss = 0.36777330\n",
      "Iteration 188, loss = 0.24466340\n",
      "Iteration 218, loss = 0.05753246\n",
      "Iteration 167, loss = 0.12193920\n",
      "Iteration 246, loss = 0.35013411\n",
      "Iteration 248, loss = 0.10008908\n",
      "Iteration 219, loss = 0.36856766\n",
      "Iteration 178, loss = 0.05219221\n",
      "Iteration 219, loss = 0.05667629\n",
      "Iteration 189, loss = 0.24626633\n",
      "Iteration 247, loss = 0.35119108\n",
      "Iteration 249, loss = 0.10018625\n",
      "Iteration 168, loss = 0.12275756\n",
      "Iteration 220, loss = 0.36746255\n",
      "Iteration 220, loss = 0.05421857\n",
      "Iteration 248, loss = 0.35069430\n",
      "Iteration 190, loss = 0.24493928\n",
      "Iteration 250, loss = 0.10077825\n",
      "Iteration 179, loss = 0.05270858\n",
      "Iteration 221, loss = 0.37183006\n",
      "Iteration 169, loss = 0.12154765\n",
      "Iteration 221, loss = 0.05733699\n",
      "Iteration 249, loss = 0.35257615\n",
      "Iteration 251, loss = 0.09754144\n",
      "Iteration 191, loss = 0.24505950\n",
      "Iteration 222, loss = 0.36745101\n",
      "Iteration 180, loss = 0.05073405\n",
      "Iteration 250, loss = 0.35135286\n",
      "Iteration 222, loss = 0.05499690\n",
      "Iteration 252, loss = 0.09852401\n",
      "Iteration 170, loss = 0.12093528\n",
      "Iteration 192, loss = 0.24893418\n",
      "Iteration 251, loss = 0.34894607\n",
      "Iteration 223, loss = 0.36691033\n",
      "Iteration 253, loss = 0.09740805\n",
      "Iteration 223, loss = 0.05556434\n",
      "Iteration 181, loss = 0.05133612\n",
      "Iteration 171, loss = 0.12100085\n",
      "Iteration 252, loss = 0.35160261\n",
      "Iteration 193, loss = 0.24439770\n",
      "Iteration 254, loss = 0.09832786\n",
      "Iteration 224, loss = 0.36795835\n",
      "Iteration 224, loss = 0.05484733\n",
      "Iteration 253, loss = 0.34999473\n",
      "Iteration 255, loss = 0.10282026\n",
      "Iteration 182, loss = 0.05320461\n",
      "Iteration 225, loss = 0.36732462\n",
      "Iteration 225, loss = 0.05478990\n",
      "Iteration 194, loss = 0.24386631\n",
      "Iteration 172, loss = 0.11993863\n",
      "Iteration 254, loss = 0.34979315\n",
      "Iteration 256, loss = 0.09770612\n",
      "Iteration 226, loss = 0.36648595\n",
      "Iteration 226, loss = 0.05441138\n",
      "Iteration 183, loss = 0.05151118\n",
      "Iteration 195, loss = 0.24421088\n",
      "Iteration 255, loss = 0.34954075\n",
      "Iteration 257, loss = 0.09609438\n",
      "Iteration 173, loss = 0.12124138\n",
      "Iteration 227, loss = 0.36773338\n",
      "Iteration 227, loss = 0.05524248\n",
      "Iteration 256, loss = 0.34921139\n",
      "Iteration 258, loss = 0.09718161\n",
      "Iteration 196, loss = 0.24560505\n",
      "Iteration 184, loss = 0.05295644\n",
      "Iteration 228, loss = 0.36688745\n",
      "Iteration 228, loss = 0.05416984\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 174, loss = 0.11941865\n",
      "Iteration 257, loss = 0.34981198\n",
      "Iteration 259, loss = 0.09784679\n",
      "Iteration 197, loss = 0.24311161\n",
      "Iteration 229, loss = 0.36610346\n",
      "Iteration 258, loss = 0.34960843\n",
      "Iteration 185, loss = 0.04985593\n",
      "Iteration 260, loss = 0.09738694\n",
      "Iteration 175, loss = 0.12238069\n",
      "Iteration 259, loss = 0.34884953\n",
      "Iteration 230, loss = 0.36885385\n",
      "Iteration 198, loss = 0.24240671\n",
      "Iteration 261, loss = 0.10185143\n",
      "Iteration 186, loss = 0.05026734\n",
      "Iteration 260, loss = 0.35037730\n",
      "Iteration 176, loss = 0.12032275\n",
      "Iteration 262, loss = 0.09603673\n",
      "Iteration 231, loss = 0.36570927\n",
      "Iteration 199, loss = 0.24269835\n",
      "Iteration 261, loss = 0.34919174\n",
      "Iteration 263, loss = 0.09720614\n",
      "Iteration 232, loss = 0.36746422\n",
      "Iteration 187, loss = 0.05026052\n",
      "Iteration 177, loss = 0.11703542\n",
      "Iteration 200, loss = 0.24389654\n",
      "Iteration 262, loss = 0.34756386\n",
      "Iteration 264, loss = 0.09695172\n",
      "Iteration 233, loss = 0.36507744\n",
      "Iteration 188, loss = 0.05235391\n",
      "Iteration 263, loss = 0.34872902\n",
      "Iteration 265, loss = 0.09626116\n",
      "Iteration 201, loss = 0.24223240\n",
      "Iteration 178, loss = 0.11883008\n",
      "Iteration 234, loss = 0.36615659\n",
      "Iteration 266, loss = 0.09622206\n",
      "Iteration 264, loss = 0.34919255\n",
      "Iteration 202, loss = 0.24246207\n",
      "Iteration 189, loss = 0.05020755\n",
      "Iteration 235, loss = 0.36599329\n",
      "Iteration 179, loss = 0.12069220\n",
      "Iteration 267, loss = 0.09732327\n",
      "Iteration 265, loss = 0.35036294\n",
      "Iteration 236, loss = 0.36632509\n",
      "Iteration 203, loss = 0.24368022\n",
      "Iteration 268, loss = 0.09578988\n",
      "Iteration 266, loss = 0.35019874\n",
      "Iteration 190, loss = 0.04904646\n",
      "Iteration 180, loss = 0.11598607\n",
      "Iteration 237, loss = 0.36413120\n",
      "Iteration 269, loss = 0.09891740\n",
      "Iteration 267, loss = 0.34834562\n",
      "Iteration 204, loss = 0.24264972\n",
      "Iteration 191, loss = 0.04935187\n",
      "Iteration 181, loss = 0.12037723\n",
      "Iteration 238, loss = 0.36522921\n",
      "Iteration 270, loss = 0.09744725\n",
      "Iteration 268, loss = 0.34808690\n",
      "Iteration 205, loss = 0.24137320\n",
      "Iteration 271, loss = 0.09475370\n",
      "Iteration 239, loss = 0.36546364\n",
      "Iteration 269, loss = 0.34734385\n",
      "Iteration 192, loss = 0.04862960\n",
      "Iteration 182, loss = 0.11826499\n",
      "Iteration 206, loss = 0.24054508\n",
      "Iteration 272, loss = 0.09389113\n",
      "Iteration 240, loss = 0.36488750\n",
      "Iteration 270, loss = 0.34714163\n",
      "Iteration 273, loss = 0.09557900\n",
      "Iteration 193, loss = 0.05113685\n",
      "Iteration 183, loss = 0.11710711\n",
      "Iteration 271, loss = 0.34663902\n",
      "Iteration 207, loss = 0.24330385\n",
      "Iteration 241, loss = 0.36571353\n",
      "Iteration 274, loss = 0.09315269\n",
      "Iteration 272, loss = 0.34702964\n",
      "Iteration 242, loss = 0.36520734\n",
      "Iteration 208, loss = 0.24078422\n",
      "Iteration 194, loss = 0.04922046\n",
      "Iteration 184, loss = 0.11791283\n",
      "Iteration 275, loss = 0.09615240\n",
      "Iteration 273, loss = 0.34857452\n",
      "Iteration 243, loss = 0.36436254\n",
      "Iteration 276, loss = 0.09589681\n",
      "Iteration 209, loss = 0.24270805\n",
      "Iteration 274, loss = 0.35079700\n",
      "Iteration 195, loss = 0.04872903\n",
      "Iteration 185, loss = 0.11630109\n",
      "Iteration 244, loss = 0.36425080\n",
      "Iteration 277, loss = 0.09565876\n",
      "Iteration 275, loss = 0.34858445\n",
      "Iteration 210, loss = 0.24014114\n",
      "Iteration 245, loss = 0.36540952\n",
      "Iteration 278, loss = 0.09437892\n",
      "Iteration 186, loss = 0.11652740\n",
      "Iteration 196, loss = 0.04921359\n",
      "Iteration 276, loss = 0.34654660\n",
      "Iteration 211, loss = 0.24241455\n",
      "Iteration 279, loss = 0.09395909\n",
      "Iteration 246, loss = 0.36639941\n",
      "Iteration 277, loss = 0.34698266\n",
      "Iteration 187, loss = 0.11639286\n",
      "Iteration 197, loss = 0.04860997\n",
      "Iteration 280, loss = 0.09429062\n",
      "Iteration 212, loss = 0.24049736\n",
      "Iteration 247, loss = 0.36370056\n",
      "Iteration 278, loss = 0.34674819\n",
      "Iteration 281, loss = 0.09503388\n",
      "Iteration 188, loss = 0.11849553\n",
      "Iteration 198, loss = 0.04931260\n",
      "Iteration 279, loss = 0.34665936\n",
      "Iteration 248, loss = 0.36146192\n",
      "Iteration 213, loss = 0.23980586\n",
      "Iteration 282, loss = 0.09422386\n",
      "Iteration 280, loss = 0.34676635\n",
      "Iteration 249, loss = 0.36365074\n",
      "Iteration 189, loss = 0.11738943\n",
      "Iteration 214, loss = 0.24203413\n",
      "Iteration 283, loss = 0.09432600\n",
      "Iteration 199, loss = 0.04950517\n",
      "Iteration 281, loss = 0.34851243\n",
      "Iteration 250, loss = 0.36399449\n",
      "Iteration 284, loss = 0.09662220\n",
      "Iteration 215, loss = 0.23994149\n",
      "Iteration 282, loss = 0.34779645\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 190, loss = 0.11452194\n",
      "Iteration 200, loss = 0.04901901\n",
      "Iteration 251, loss = 0.36389292\n",
      "Iteration 285, loss = 0.09396535\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 216, loss = 0.23912503\n",
      "Iteration 252, loss = 0.36222493\n",
      "Iteration 191, loss = 0.11344627\n",
      "Iteration 201, loss = 0.04759852\n",
      "Iteration 253, loss = 0.36515957\n",
      "Iteration 217, loss = 0.23947712\n",
      "Iteration 192, loss = 0.11360663\n",
      "Iteration 202, loss = 0.04811358\n",
      "Iteration 254, loss = 0.36217258\n",
      "Iteration 218, loss = 0.24071487\n",
      "Iteration 193, loss = 0.11327230\n",
      "Iteration 255, loss = 0.36210748\n",
      "Iteration 203, loss = 0.04951079\n",
      "Iteration 219, loss = 0.23895320\n",
      "Iteration 256, loss = 0.36168931\n",
      "Iteration 194, loss = 0.11728617\n",
      "Iteration 204, loss = 0.04572676\n",
      "Iteration 220, loss = 0.23870452\n",
      "Iteration 257, loss = 0.36170113\n",
      "Iteration 195, loss = 0.11207799\n",
      "Iteration 221, loss = 0.24059045\n",
      "Iteration 258, loss = 0.36488630\n",
      "Iteration 205, loss = 0.04847997\n",
      "Iteration 259, loss = 0.36130254\n",
      "Iteration 196, loss = 0.11407446\n",
      "Iteration 222, loss = 0.23822303\n",
      "Iteration 206, loss = 0.04707796\n",
      "Iteration 260, loss = 0.36237516\n",
      "Iteration 223, loss = 0.23877206\n",
      "Iteration 197, loss = 0.11509389\n",
      "Iteration 261, loss = 0.36054321\n",
      "Iteration 207, loss = 0.04682944\n",
      "Iteration 224, loss = 0.23831754\n",
      "Iteration 198, loss = 0.11217334\n",
      "Iteration 262, loss = 0.36187584\n",
      "Iteration 208, loss = 0.04684005\n",
      "Iteration 225, loss = 0.23956493\n",
      "Iteration 263, loss = 0.36201050\n",
      "Iteration 199, loss = 0.11295830\n",
      "Iteration 209, loss = 0.04804026\n",
      "Iteration 226, loss = 0.23610477\n",
      "Iteration 264, loss = 0.36296750\n",
      "Iteration 200, loss = 0.11429052\n",
      "Iteration 265, loss = 0.35881261\n",
      "Iteration 227, loss = 0.24013343\n",
      "Iteration 210, loss = 0.04629393\n",
      "Iteration 266, loss = 0.35991505\n",
      "Iteration 201, loss = 0.11118346\n",
      "Iteration 228, loss = 0.24122860\n",
      "Iteration 211, loss = 0.04705147\n",
      "Iteration 267, loss = 0.36044124\n",
      "Iteration 202, loss = 0.11125552\n",
      "Iteration 229, loss = 0.23675422\n",
      "Iteration 268, loss = 0.36072393\n",
      "Iteration 212, loss = 0.04756787\n",
      "Iteration 230, loss = 0.23722204\n",
      "Iteration 203, loss = 0.11070412\n",
      "Iteration 269, loss = 0.36025518\n",
      "Iteration 213, loss = 0.04635059\n",
      "Iteration 231, loss = 0.23581692\n",
      "Iteration 270, loss = 0.36181466\n",
      "Iteration 204, loss = 0.11164740\n",
      "Iteration 271, loss = 0.35930930\n",
      "Iteration 214, loss = 0.04590369\n",
      "Iteration 232, loss = 0.23626940\n",
      "Iteration 205, loss = 0.11038867\n",
      "Iteration 272, loss = 0.35992394\n",
      "Iteration 233, loss = 0.23760583\n",
      "Iteration 215, loss = 0.04454006\n",
      "Iteration 206, loss = 0.11369574\n",
      "Iteration 273, loss = 0.36084883\n",
      "Iteration 234, loss = 0.23717409\n",
      "Iteration 216, loss = 0.04495780\n",
      "Iteration 274, loss = 0.35977810\n",
      "Iteration 207, loss = 0.11435891\n",
      "Iteration 235, loss = 0.23722037\n",
      "Iteration 275, loss = 0.36026697\n",
      "Iteration 217, loss = 0.04735310\n",
      "Iteration 208, loss = 0.10994581\n",
      "Iteration 236, loss = 0.23595466\n",
      "Iteration 276, loss = 0.35811673\n",
      "Iteration 218, loss = 0.04608639\n",
      "Iteration 237, loss = 0.23598590\n",
      "Iteration 277, loss = 0.35873564\n",
      "Iteration 209, loss = 0.10901484\n",
      "Iteration 278, loss = 0.35953635\n",
      "Iteration 219, loss = 0.04554911\n",
      "Iteration 238, loss = 0.23875233\n",
      "Iteration 210, loss = 0.11254120\n",
      "Iteration 279, loss = 0.35919805\n",
      "Iteration 239, loss = 0.23564623\n",
      "Iteration 220, loss = 0.04579664\n",
      "Iteration 280, loss = 0.35953781\n",
      "Iteration 211, loss = 0.10890254\n",
      "Iteration 240, loss = 0.23441443\n",
      "Iteration 281, loss = 0.36052393\n",
      "Iteration 221, loss = 0.04902724\n",
      "Iteration 212, loss = 0.10900792\n",
      "Iteration 241, loss = 0.23562185\n",
      "Iteration 282, loss = 0.35990636\n",
      "Iteration 222, loss = 0.04605665\n",
      "Iteration 213, loss = 0.10844212\n",
      "Iteration 283, loss = 0.35846670\n",
      "Iteration 242, loss = 0.23510179\n",
      "Iteration 223, loss = 0.04552085\n",
      "Iteration 284, loss = 0.35822851\n",
      "Iteration 214, loss = 0.10801547\n",
      "Iteration 243, loss = 0.23459850\n",
      "Iteration 285, loss = 0.36056421\n",
      "Iteration 224, loss = 0.04542411\n",
      "Iteration 244, loss = 0.23776213\n",
      "Iteration 215, loss = 0.11028670\n",
      "Iteration 286, loss = 0.35798031\n",
      "Iteration 245, loss = 0.23521624\n",
      "Iteration 225, loss = 0.04473029\n",
      "Iteration 287, loss = 0.35751599\n",
      "Iteration 216, loss = 0.10704446\n",
      "Iteration 246, loss = 0.23498856\n",
      "Iteration 288, loss = 0.35758853\n",
      "Iteration 226, loss = 0.04373645\n",
      "Iteration 217, loss = 0.10907961\n",
      "Iteration 289, loss = 0.35882791\n",
      "Iteration 247, loss = 0.23455624\n",
      "Iteration 227, loss = 0.04546625\n",
      "Iteration 218, loss = 0.10766916\n",
      "Iteration 290, loss = 0.35621272\n",
      "Iteration 248, loss = 0.23400932\n",
      "Iteration 291, loss = 0.35729826\n",
      "Iteration 228, loss = 0.04366675\n",
      "Iteration 219, loss = 0.10813356\n",
      "Iteration 249, loss = 0.23673889\n",
      "Iteration 292, loss = 0.35744991\n",
      "Iteration 229, loss = 0.04337389\n",
      "Iteration 220, loss = 0.10896787\n",
      "Iteration 250, loss = 0.23498469\n",
      "Iteration 293, loss = 0.35796986\n",
      "Iteration 251, loss = 0.23390895\n",
      "Iteration 230, loss = 0.04434686\n",
      "Iteration 294, loss = 0.35611598\n",
      "Iteration 221, loss = 0.10671867\n",
      "Iteration 295, loss = 0.35758204\n",
      "Iteration 252, loss = 0.23455492\n",
      "Iteration 231, loss = 0.04598983\n",
      "Iteration 222, loss = 0.10565751\n",
      "Iteration 296, loss = 0.36100850\n",
      "Iteration 253, loss = 0.23394286\n",
      "Iteration 232, loss = 0.04215009\n",
      "Iteration 223, loss = 0.10712897\n",
      "Iteration 297, loss = 0.35612693\n",
      "Iteration 254, loss = 0.23303400\n",
      "Iteration 298, loss = 0.35542172\n",
      "Iteration 224, loss = 0.10999686\n",
      "Iteration 233, loss = 0.04526294\n",
      "Iteration 255, loss = 0.23370240\n",
      "Iteration 299, loss = 0.35622625\n",
      "Iteration 225, loss = 0.10707421\n",
      "Iteration 234, loss = 0.04273824\n",
      "Iteration 256, loss = 0.23413410\n",
      "Iteration 300, loss = 0.35577042\n",
      "Iteration 301, loss = 0.35796931\n",
      "Iteration 226, loss = 0.10480100\n",
      "Iteration 257, loss = 0.23435241\n",
      "Iteration 235, loss = 0.04293432\n",
      "Iteration 302, loss = 0.35818270\n",
      "Iteration 258, loss = 0.23416407\n",
      "Iteration 227, loss = 0.10524525\n",
      "Iteration 236, loss = 0.04293066\n",
      "Iteration 303, loss = 0.35937502\n",
      "Iteration 259, loss = 0.23315450\n",
      "Iteration 228, loss = 0.10631514\n",
      "Iteration 304, loss = 0.35696539\n",
      "Iteration 237, loss = 0.04351329\n",
      "Iteration 260, loss = 0.23426054\n",
      "Iteration 305, loss = 0.35464545\n",
      "Iteration 229, loss = 0.10507270\n",
      "Iteration 238, loss = 0.04292479\n",
      "Iteration 261, loss = 0.23413122\n",
      "Iteration 306, loss = 0.35353001\n",
      "Iteration 230, loss = 0.10566290\n",
      "Iteration 239, loss = 0.04195576\n",
      "Iteration 307, loss = 0.35474293\n",
      "Iteration 262, loss = 0.23237827\n",
      "Iteration 308, loss = 0.35611236\n",
      "Iteration 231, loss = 0.10382822\n",
      "Iteration 240, loss = 0.04281717\n",
      "Iteration 263, loss = 0.23310243\n",
      "Iteration 309, loss = 0.35689990\n",
      "Iteration 232, loss = 0.10663651\n",
      "Iteration 264, loss = 0.23580139\n",
      "Iteration 241, loss = 0.04277970\n",
      "Iteration 310, loss = 0.35486692\n",
      "Iteration 265, loss = 0.23464987\n",
      "Iteration 233, loss = 0.10486276\n",
      "Iteration 311, loss = 0.35364751\n",
      "Iteration 242, loss = 0.04377975\n",
      "Iteration 266, loss = 0.23025022\n",
      "Iteration 312, loss = 0.35509887\n",
      "Iteration 234, loss = 0.10310082\n",
      "Iteration 243, loss = 0.04581965\n",
      "Iteration 313, loss = 0.35469000\n",
      "Iteration 267, loss = 0.23192360\n",
      "Iteration 235, loss = 0.10444979\n",
      "Iteration 314, loss = 0.35802320\n",
      "Iteration 244, loss = 0.04327406\n",
      "Iteration 268, loss = 0.23182084\n",
      "Iteration 315, loss = 0.35554154\n",
      "Iteration 236, loss = 0.10375750\n",
      "Iteration 269, loss = 0.23272020\n",
      "Iteration 245, loss = 0.04087366\n",
      "Iteration 316, loss = 0.35408049\n",
      "Iteration 237, loss = 0.10478773\n",
      "Iteration 270, loss = 0.23078689\n",
      "Iteration 317, loss = 0.35370133\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 246, loss = 0.04295484\n",
      "Iteration 238, loss = 0.10676801\n",
      "Iteration 271, loss = 0.23156484\n",
      "Iteration 247, loss = 0.04342964\n",
      "Iteration 272, loss = 0.23232722\n",
      "Iteration 239, loss = 0.10327246\n",
      "Iteration 248, loss = 0.04014840\n",
      "Iteration 273, loss = 0.23089909\n",
      "Iteration 240, loss = 0.10165747\n",
      "Iteration 249, loss = 0.04130947\n",
      "Iteration 274, loss = 0.23167901\n",
      "Iteration 241, loss = 0.10406404\n",
      "Iteration 275, loss = 0.23242169\n",
      "Iteration 250, loss = 0.04057664\n",
      "Iteration 242, loss = 0.10118079\n",
      "Iteration 276, loss = 0.23220491\n",
      "Iteration 251, loss = 0.04438659\n",
      "Iteration 243, loss = 0.10207239\n",
      "Iteration 277, loss = 0.22971620\n",
      "Iteration 252, loss = 0.04126988\n",
      "Iteration 244, loss = 0.10367810\n",
      "Iteration 278, loss = 0.23057141\n",
      "Iteration 253, loss = 0.04020669\n",
      "Iteration 279, loss = 0.23108269\n",
      "Iteration 245, loss = 0.10071059\n",
      "Iteration 254, loss = 0.04122739\n",
      "Iteration 280, loss = 0.22955261\n",
      "Iteration 246, loss = 0.10080973\n",
      "Iteration 281, loss = 0.23041286\n",
      "Iteration 255, loss = 0.04095880\n",
      "Iteration 247, loss = 0.10244427\n",
      "Iteration 282, loss = 0.22972507\n",
      "Iteration 256, loss = 0.04370934\n",
      "Iteration 248, loss = 0.10264530\n",
      "Iteration 283, loss = 0.23112654\n",
      "Iteration 257, loss = 0.04043801\n",
      "Iteration 249, loss = 0.10852603\n",
      "Iteration 284, loss = 0.23143799\n",
      "Iteration 258, loss = 0.04100774\n",
      "Iteration 250, loss = 0.10025747\n",
      "Iteration 285, loss = 0.22818019\n",
      "Iteration 259, loss = 0.04052306\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 286, loss = 0.23019934\n",
      "Iteration 251, loss = 0.09820796\n",
      "Iteration 287, loss = 0.23048827\n",
      "Iteration 252, loss = 0.10012222\n",
      "Iteration 288, loss = 0.22825006\n",
      "Iteration 253, loss = 0.10103864\n",
      "Iteration 289, loss = 0.23020139\n",
      "Iteration 254, loss = 0.10199102\n",
      "Iteration 290, loss = 0.22955401\n",
      "Iteration 255, loss = 0.10085400\n",
      "Iteration 291, loss = 0.22925263\n",
      "Iteration 256, loss = 0.10035697\n",
      "Iteration 292, loss = 0.23002377\n",
      "Iteration 293, loss = 0.22939384\n",
      "Iteration 257, loss = 0.10542510\n",
      "Iteration 294, loss = 0.22833456\n",
      "Iteration 258, loss = 0.09688822\n",
      "Iteration 295, loss = 0.22999190\n",
      "Iteration 259, loss = 0.09832918\n",
      "Iteration 296, loss = 0.22990885\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 260, loss = 0.09973720\n",
      "Iteration 261, loss = 0.09755178\n",
      "Iteration 262, loss = 0.10148147\n",
      "Iteration 263, loss = 0.09737015\n",
      "Iteration 264, loss = 0.10084142\n",
      "Iteration 265, loss = 0.09738458\n",
      "Iteration 266, loss = 0.09745897\n",
      "Iteration 267, loss = 0.09854773\n",
      "Iteration 268, loss = 0.09959049\n",
      "Iteration 269, loss = 0.09914716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy on training set: 0.79\n"
     ]
    }
   ],
   "source": [
    "# create the MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,50, 20, 10,), verbose=True, max_iter=max_iterations)\n",
    "\n",
    "# create the one-vs-one classifier\n",
    "ovo_classifier2 = OneVsOneClassifier(mlp, n_jobs=-1)\n",
    "\n",
    "ovo_classifier2.fit(X_train, t_train)\n",
    "\n",
    "# save the trained classifier to a file\n",
    "with open('./models/model3.pkl', 'wb') as f:\n",
    "    pickle.dump(ovo_classifier2, f)\n",
    "\n",
    "print('Accuracy on training set: {:.2f}'.format(ovo_classifier2.score(X_train, t_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.34411095\n",
      "Iteration 1, loss = 0.32512819\n",
      "Iteration 1, loss = 0.37371014\n",
      "Iteration 1, loss = 0.39269787\n",
      "Iteration 1, loss = 0.12196569\n",
      "Iteration 1, loss = 0.34388105\n",
      "Iteration 1, loss = 0.38673875\n",
      "Iteration 2, loss = 0.32074693\n",
      "Iteration 2, loss = 0.23732521\n",
      "Iteration 2, loss = 0.29906529\n",
      "Iteration 2, loss = 0.02006114\n",
      "Iteration 2, loss = 0.37375025\n",
      "Iteration 2, loss = 0.34492984\n",
      "Iteration 2, loss = 0.32479961\n",
      "Iteration 3, loss = 0.31671945\n",
      "Iteration 3, loss = 0.27185903\n",
      "Iteration 3, loss = 0.21236817\n",
      "Iteration 3, loss = 0.01166630\n",
      "Iteration 3, loss = 0.35913438\n",
      "Iteration 3, loss = 0.32013863\n",
      "Iteration 3, loss = 0.33220744\n",
      "Iteration 4, loss = 0.31358842\n",
      "Iteration 4, loss = 0.25714814\n",
      "Iteration 4, loss = 0.00842149\n",
      "Iteration 4, loss = 0.19614766\n",
      "Iteration 4, loss = 0.35311298\n",
      "Iteration 4, loss = 0.31688314\n",
      "Iteration 4, loss = 0.32320260\n",
      "Iteration 5, loss = 0.31033417\n",
      "Iteration 5, loss = 0.00583580\n",
      "Iteration 5, loss = 0.24702940\n",
      "Iteration 5, loss = 0.18430117\n",
      "Iteration 5, loss = 0.34788885\n",
      "Iteration 5, loss = 0.31352050\n",
      "Iteration 5, loss = 0.31595195\n",
      "Iteration 6, loss = 0.00506861\n",
      "Iteration 6, loss = 0.30647862\n",
      "Iteration 6, loss = 0.34387709\n",
      "Iteration 6, loss = 0.30991233\n",
      "Iteration 6, loss = 0.31007449\n",
      "Iteration 6, loss = 0.17480319\n",
      "Iteration 6, loss = 0.23903802\n",
      "Iteration 7, loss = 0.00469622\n",
      "Iteration 7, loss = 0.34079216\n",
      "Iteration 7, loss = 0.30448433\n",
      "Iteration 7, loss = 0.30563708\n",
      "Iteration 7, loss = 0.30263173\n",
      "Iteration 8, loss = 0.00322183\n",
      "Iteration 7, loss = 0.16755080\n",
      "Iteration 8, loss = 0.33806883\n",
      "Iteration 7, loss = 0.23291904\n",
      "Iteration 8, loss = 0.29955545\n",
      "Iteration 8, loss = 0.30086270\n",
      "Iteration 9, loss = 0.00314050\n",
      "Iteration 9, loss = 0.33580893\n",
      "Iteration 8, loss = 0.29770019\n",
      "Iteration 9, loss = 0.29531886\n",
      "Iteration 10, loss = 0.00269960\n",
      "Iteration 9, loss = 0.29615485\n",
      "Iteration 10, loss = 0.33365361\n",
      "Iteration 8, loss = 0.16083324\n",
      "Iteration 11, loss = 0.00261919\n",
      "Iteration 9, loss = 0.29176507\n",
      "Iteration 8, loss = 0.22799198\n",
      "Iteration 10, loss = 0.29202948\n",
      "Iteration 10, loss = 0.29231324\n",
      "Iteration 11, loss = 0.33180712\n",
      "Iteration 12, loss = 0.00173332\n",
      "Iteration 9, loss = 0.15577516\n",
      "Iteration 11, loss = 0.28850754\n",
      "Iteration 12, loss = 0.33004803\n",
      "Iteration 11, loss = 0.28861739\n",
      "Iteration 10, loss = 0.28665617\n",
      "Iteration 13, loss = 0.00233111\n",
      "Iteration 9, loss = 0.22347153\n",
      "Iteration 14, loss = 0.00162444\n",
      "Iteration 12, loss = 0.28562762\n",
      "Iteration 12, loss = 0.28606061\n",
      "Iteration 13, loss = 0.32820289\n",
      "Iteration 11, loss = 0.28108011\n",
      "Iteration 10, loss = 0.15123144\n",
      "Iteration 15, loss = 0.00172348\n",
      "Iteration 13, loss = 0.28365454\n",
      "Iteration 13, loss = 0.28379487\n",
      "Iteration 16, loss = 0.00193028\n",
      "Iteration 14, loss = 0.32651433\n",
      "Iteration 12, loss = 0.27766957\n",
      "Iteration 10, loss = 0.21947310\n",
      "Iteration 14, loss = 0.28134446\n",
      "Iteration 17, loss = 0.00142380\n",
      "Iteration 14, loss = 0.28160348\n",
      "Iteration 11, loss = 0.14725442\n",
      "Iteration 18, loss = 0.00145661\n",
      "Iteration 15, loss = 0.32483156\n",
      "Iteration 15, loss = 0.27955353\n",
      "Iteration 13, loss = 0.27466857\n",
      "Iteration 19, loss = 0.00122617\n",
      "Iteration 15, loss = 0.27989521\n",
      "Iteration 16, loss = 0.27793682\n",
      "Iteration 11, loss = 0.21551786\n",
      "Iteration 20, loss = 0.00169519\n",
      "Iteration 16, loss = 0.32351150\n",
      "Iteration 14, loss = 0.27192472\n",
      "Iteration 12, loss = 0.14387037\n",
      "Iteration 16, loss = 0.27792783\n",
      "Iteration 17, loss = 0.27613346\n",
      "Iteration 21, loss = 0.00122051\n",
      "Iteration 22, loss = 0.00129980\n",
      "Iteration 17, loss = 0.32221545\n",
      "Iteration 18, loss = 0.27480821\n",
      "Iteration 15, loss = 0.26993813\n",
      "Iteration 17, loss = 0.27648916\n",
      "Iteration 12, loss = 0.21269132\n",
      "Iteration 23, loss = 0.00106522\n",
      "Iteration 13, loss = 0.14127937\n",
      "Iteration 19, loss = 0.27417237\n",
      "Iteration 18, loss = 0.32093830\n",
      "Iteration 16, loss = 0.26765278\n",
      "Iteration 24, loss = 0.00122843\n",
      "Iteration 18, loss = 0.27421746\n",
      "Iteration 20, loss = 0.27274931\n",
      "Iteration 13, loss = 0.20988246\n",
      "Iteration 25, loss = 0.00125971\n",
      "Iteration 19, loss = 0.31979185\n",
      "Iteration 17, loss = 0.26630321\n",
      "Iteration 21, loss = 0.27149515\n",
      "Iteration 14, loss = 0.13808283\n",
      "Iteration 19, loss = 0.27313080\n",
      "Iteration 26, loss = 0.00124490\n",
      "Iteration 22, loss = 0.27057965\n",
      "Iteration 20, loss = 0.31885206\n",
      "Iteration 18, loss = 0.26474615\n",
      "Iteration 20, loss = 0.27133653\n",
      "Iteration 27, loss = 0.00125193\n",
      "Iteration 14, loss = 0.20695770\n",
      "Iteration 23, loss = 0.26947636\n",
      "Iteration 15, loss = 0.13591355\n",
      "Iteration 28, loss = 0.00105427\n",
      "Iteration 19, loss = 0.26280638\n",
      "Iteration 21, loss = 0.31746054\n",
      "Iteration 21, loss = 0.27006598\n",
      "Iteration 24, loss = 0.26837765\n",
      "Iteration 29, loss = 0.00114622\n",
      "Iteration 25, loss = 0.26731343\n",
      "Iteration 15, loss = 0.20398781\n",
      "Iteration 20, loss = 0.26160764\n",
      "Iteration 22, loss = 0.31675047\n",
      "Iteration 22, loss = 0.26941608\n",
      "Iteration 16, loss = 0.13309731\n",
      "Iteration 30, loss = 0.00109997\n",
      "Iteration 26, loss = 0.26679913\n",
      "Iteration 21, loss = 0.26006518\n",
      "Iteration 23, loss = 0.26819713\n",
      "Iteration 31, loss = 0.00116496\n",
      "Iteration 23, loss = 0.31584222\n",
      "Iteration 27, loss = 0.26604545\n",
      "Iteration 16, loss = 0.20180165\n",
      "Iteration 17, loss = 0.13091645\n",
      "Iteration 32, loss = 0.00101069\n",
      "Iteration 22, loss = 0.25889869\n",
      "Iteration 24, loss = 0.26731167\n",
      "Iteration 28, loss = 0.26489688\n",
      "Iteration 24, loss = 0.31475135\n",
      "Iteration 33, loss = 0.00101471\n",
      "Iteration 29, loss = 0.26427496\n",
      "Iteration 23, loss = 0.25727955\n",
      "Iteration 25, loss = 0.26605761\n",
      "Iteration 18, loss = 0.12838130\n",
      "Iteration 17, loss = 0.19945025\n",
      "Iteration 34, loss = 0.00083249\n",
      "Iteration 30, loss = 0.26367005\n",
      "Iteration 25, loss = 0.31399285\n",
      "Iteration 24, loss = 0.25645105\n",
      "Iteration 26, loss = 0.26507604\n",
      "Iteration 35, loss = 0.00042463\n",
      "Iteration 31, loss = 0.26285248\n",
      "Iteration 19, loss = 0.12637453\n",
      "Iteration 36, loss = 0.00127950\n",
      "Iteration 18, loss = 0.19776888\n",
      "Iteration 26, loss = 0.31335131\n",
      "Iteration 25, loss = 0.25534445\n",
      "Iteration 27, loss = 0.26442481\n",
      "Iteration 32, loss = 0.26201118\n",
      "Iteration 37, loss = 0.00089541\n",
      "Iteration 33, loss = 0.26144228\n",
      "Iteration 26, loss = 0.25454640\n",
      "Iteration 28, loss = 0.26326411\n",
      "Iteration 27, loss = 0.31274510\n",
      "Iteration 20, loss = 0.12444588\n",
      "Iteration 38, loss = 0.00099344\n",
      "Iteration 19, loss = 0.19586644\n",
      "Iteration 34, loss = 0.26050858\n",
      "Iteration 27, loss = 0.25299305\n",
      "Iteration 29, loss = 0.26248773\n",
      "Iteration 39, loss = 0.00101773\n",
      "Iteration 28, loss = 0.31195802\n",
      "Iteration 35, loss = 0.26014909\n",
      "Iteration 21, loss = 0.12276580\n",
      "Iteration 40, loss = 0.00097706\n",
      "Iteration 28, loss = 0.25235501\n",
      "Iteration 30, loss = 0.26149245\n",
      "Iteration 20, loss = 0.19419776\n",
      "Iteration 36, loss = 0.25957751\n",
      "Iteration 29, loss = 0.31121481\n",
      "Iteration 41, loss = 0.00047078\n",
      "Iteration 37, loss = 0.25902818\n",
      "Iteration 31, loss = 0.26067502\n",
      "Iteration 29, loss = 0.25150645\n",
      "Iteration 22, loss = 0.12120528\n",
      "Iteration 42, loss = 0.00097644\n",
      "Iteration 38, loss = 0.25868744\n",
      "Iteration 30, loss = 0.31050006\n",
      "Iteration 21, loss = 0.19222095\n",
      "Iteration 32, loss = 0.26045555\n",
      "Iteration 30, loss = 0.25088397\n",
      "Iteration 43, loss = 0.00089827\n",
      "Iteration 39, loss = 0.25807006\n",
      "Iteration 23, loss = 0.11923442\n",
      "Iteration 33, loss = 0.25958444\n",
      "Iteration 31, loss = 0.30985476\n",
      "Iteration 31, loss = 0.24962724\n",
      "Iteration 44, loss = 0.00116568\n",
      "Iteration 40, loss = 0.25782061\n",
      "Iteration 22, loss = 0.19108479\n",
      "Iteration 34, loss = 0.25863335\n",
      "Iteration 45, loss = 0.00043645\n",
      "Iteration 41, loss = 0.25719953\n",
      "Iteration 32, loss = 0.24932044\n",
      "Iteration 32, loss = 0.30928160\n",
      "Iteration 24, loss = 0.11801741\n",
      "Iteration 42, loss = 0.25658484\n",
      "Iteration 46, loss = 0.00152282\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 35, loss = 0.25797180\n",
      "Iteration 33, loss = 0.24840272\n",
      "Iteration 23, loss = 0.18909490\n",
      "Iteration 33, loss = 0.30893875\n",
      "Iteration 43, loss = 0.25635788\n",
      "Iteration 36, loss = 0.25739489\n",
      "Iteration 25, loss = 0.11641040\n",
      "Iteration 34, loss = 0.24810394\n",
      "Iteration 44, loss = 0.25573470\n",
      "Iteration 34, loss = 0.30828448\n",
      "Iteration 24, loss = 0.18807690\n",
      "Iteration 37, loss = 0.25706885\n",
      "Iteration 35, loss = 0.24685599\n",
      "Iteration 45, loss = 0.25527812\n",
      "Iteration 26, loss = 0.11508643\n",
      "Iteration 38, loss = 0.25649041\n",
      "Iteration 46, loss = 0.25508886\n",
      "Iteration 35, loss = 0.30766368\n",
      "Iteration 36, loss = 0.24650174\n",
      "Iteration 25, loss = 0.18633394\n",
      "Iteration 47, loss = 0.25448905\n",
      "Iteration 39, loss = 0.25649536\n",
      "Iteration 37, loss = 0.24562121\n",
      "Iteration 27, loss = 0.11445351\n",
      "Iteration 36, loss = 0.30724594\n",
      "Iteration 48, loss = 0.25414137\n",
      "Iteration 40, loss = 0.25543873\n",
      "Iteration 26, loss = 0.18518681\n",
      "Iteration 38, loss = 0.24517695\n",
      "Iteration 49, loss = 0.25398790\n",
      "Iteration 37, loss = 0.30689212\n",
      "Iteration 41, loss = 0.25517297\n",
      "Iteration 28, loss = 0.11240437\n",
      "Iteration 50, loss = 0.25354776\n",
      "Iteration 39, loss = 0.24438466\n",
      "Iteration 27, loss = 0.18348047\n",
      "Iteration 42, loss = 0.25433648\n",
      "Iteration 38, loss = 0.30636322\n",
      "Iteration 51, loss = 0.25301060\n",
      "Iteration 40, loss = 0.24398523\n",
      "Iteration 29, loss = 0.11173763\n",
      "Iteration 52, loss = 0.25255917\n",
      "Iteration 43, loss = 0.25410759\n",
      "Iteration 39, loss = 0.30597607\n",
      "Iteration 41, loss = 0.24354553\n",
      "Iteration 28, loss = 0.18332612\n",
      "Iteration 53, loss = 0.25221283\n",
      "Iteration 44, loss = 0.25311144\n",
      "Iteration 30, loss = 0.11008722\n",
      "Iteration 42, loss = 0.24333480\n",
      "Iteration 54, loss = 0.25207593\n",
      "Iteration 40, loss = 0.30547133\n",
      "Iteration 45, loss = 0.25314949\n",
      "Iteration 55, loss = 0.25206548\n",
      "Iteration 29, loss = 0.18135715\n",
      "Iteration 43, loss = 0.24246512\n",
      "Iteration 41, loss = 0.30488614\n",
      "Iteration 31, loss = 0.10969899\n",
      "Iteration 46, loss = 0.25246822\n",
      "Iteration 56, loss = 0.25138964\n",
      "Iteration 44, loss = 0.24186899\n",
      "Iteration 57, loss = 0.25108698\n",
      "Iteration 47, loss = 0.25215086\n",
      "Iteration 30, loss = 0.18039715\n",
      "Iteration 42, loss = 0.30447012\n",
      "Iteration 32, loss = 0.10840215\n",
      "Iteration 45, loss = 0.24149454\n",
      "Iteration 58, loss = 0.25093275\n",
      "Iteration 48, loss = 0.25186281\n",
      "Iteration 43, loss = 0.30397791\n",
      "Iteration 59, loss = 0.25073613\n",
      "Iteration 46, loss = 0.24098199\n",
      "Iteration 31, loss = 0.17935258\n",
      "Iteration 49, loss = 0.25131204\n",
      "Iteration 33, loss = 0.10758994\n",
      "Iteration 60, loss = 0.25012354\n",
      "Iteration 47, loss = 0.24073776\n",
      "Iteration 44, loss = 0.30362792\n",
      "Iteration 61, loss = 0.25019445\n",
      "Iteration 50, loss = 0.25079505\n",
      "Iteration 32, loss = 0.17812941\n",
      "Iteration 48, loss = 0.24071000\n",
      "Iteration 34, loss = 0.10648940\n",
      "Iteration 62, loss = 0.24988508\n",
      "Iteration 45, loss = 0.30331927\n",
      "Iteration 51, loss = 0.25058971\n",
      "Iteration 63, loss = 0.24996646\n",
      "Iteration 49, loss = 0.23995725\n",
      "Iteration 33, loss = 0.17701550\n",
      "Iteration 52, loss = 0.25015098\n",
      "Iteration 46, loss = 0.30291169\n",
      "Iteration 64, loss = 0.24928728\n",
      "Iteration 35, loss = 0.10593987\n",
      "Iteration 50, loss = 0.23941573\n",
      "Iteration 53, loss = 0.24997867\n",
      "Iteration 65, loss = 0.24909283\n",
      "Iteration 47, loss = 0.30245725\n",
      "Iteration 51, loss = 0.23908455\n",
      "Iteration 34, loss = 0.17652453\n",
      "Iteration 66, loss = 0.24888076\n",
      "Iteration 36, loss = 0.10518318\n",
      "Iteration 54, loss = 0.24931625\n",
      "Iteration 48, loss = 0.30224318\n",
      "Iteration 52, loss = 0.23836912\n",
      "Iteration 67, loss = 0.24880915\n",
      "Iteration 55, loss = 0.24888840\n",
      "Iteration 35, loss = 0.17548767\n",
      "Iteration 68, loss = 0.24861133\n",
      "Iteration 37, loss = 0.10407712\n",
      "Iteration 53, loss = 0.23861961\n",
      "Iteration 49, loss = 0.30197915\n",
      "Iteration 56, loss = 0.24861990\n",
      "Iteration 69, loss = 0.24835789\n",
      "Iteration 54, loss = 0.23734218\n",
      "Iteration 36, loss = 0.17415339\n",
      "Iteration 57, loss = 0.24829762\n",
      "Iteration 70, loss = 0.24784194\n",
      "Iteration 38, loss = 0.10319568\n",
      "Iteration 50, loss = 0.30174108\n",
      "Iteration 55, loss = 0.23743775\n",
      "Iteration 71, loss = 0.24747385\n",
      "Iteration 58, loss = 0.24821969\n",
      "Iteration 51, loss = 0.30130870\n",
      "Iteration 37, loss = 0.17387718\n",
      "Iteration 72, loss = 0.24743942\n",
      "Iteration 39, loss = 0.10294803\n",
      "Iteration 56, loss = 0.23714826\n",
      "Iteration 59, loss = 0.24770583\n",
      "Iteration 73, loss = 0.24721978\n",
      "Iteration 52, loss = 0.30077147\n",
      "Iteration 57, loss = 0.23700754\n",
      "Iteration 60, loss = 0.24724749\n",
      "Iteration 74, loss = 0.24745077\n",
      "Iteration 38, loss = 0.17273970\n",
      "Iteration 40, loss = 0.10172835\n",
      "Iteration 58, loss = 0.23625022\n",
      "Iteration 53, loss = 0.30075780\n",
      "Iteration 61, loss = 0.24694096\n",
      "Iteration 75, loss = 0.24653699\n",
      "Iteration 76, loss = 0.24676908\n",
      "Iteration 59, loss = 0.23588789\n",
      "Iteration 39, loss = 0.17249893\n",
      "Iteration 41, loss = 0.10133446\n",
      "Iteration 62, loss = 0.24681930\n",
      "Iteration 54, loss = 0.30037325\n",
      "Iteration 77, loss = 0.24652602\n",
      "Iteration 60, loss = 0.23571971\n",
      "Iteration 63, loss = 0.24634153\n",
      "Iteration 78, loss = 0.24630071\n",
      "Iteration 55, loss = 0.30031545\n",
      "Iteration 42, loss = 0.10033783\n",
      "Iteration 40, loss = 0.17131960\n",
      "Iteration 61, loss = 0.23501960\n",
      "Iteration 64, loss = 0.24589259\n",
      "Iteration 79, loss = 0.24625292\n",
      "Iteration 56, loss = 0.29998224\n",
      "Iteration 62, loss = 0.23534563\n",
      "Iteration 80, loss = 0.24600313\n",
      "Iteration 65, loss = 0.24601915\n",
      "Iteration 43, loss = 0.09971513\n",
      "Iteration 41, loss = 0.17088330\n",
      "Iteration 81, loss = 0.24552069\n",
      "Iteration 57, loss = 0.29922434\n",
      "Iteration 63, loss = 0.23440844\n",
      "Iteration 66, loss = 0.24550998\n",
      "Iteration 82, loss = 0.24552799\n",
      "Iteration 44, loss = 0.09907063\n",
      "Iteration 64, loss = 0.23465069\n",
      "Iteration 42, loss = 0.17014682\n",
      "Iteration 67, loss = 0.24548292\n",
      "Iteration 58, loss = 0.29936592\n",
      "Iteration 83, loss = 0.24533327\n",
      "Iteration 65, loss = 0.23404085\n",
      "Iteration 68, loss = 0.24477754\n",
      "Iteration 84, loss = 0.24549407\n",
      "Iteration 59, loss = 0.29912394\n",
      "Iteration 45, loss = 0.09889784\n",
      "Iteration 43, loss = 0.16931251\n",
      "Iteration 85, loss = 0.24502311\n",
      "Iteration 69, loss = 0.24512262\n",
      "Iteration 66, loss = 0.23386584\n",
      "Iteration 86, loss = 0.24523857\n",
      "Iteration 60, loss = 0.29875066\n",
      "Iteration 70, loss = 0.24474619\n",
      "Iteration 67, loss = 0.23345794\n",
      "Iteration 46, loss = 0.09808892\n",
      "Iteration 44, loss = 0.16869253\n",
      "Iteration 87, loss = 0.24454970\n",
      "Iteration 61, loss = 0.29837230\n",
      "Iteration 71, loss = 0.24409562\n",
      "Iteration 68, loss = 0.23326752\n",
      "Iteration 88, loss = 0.24479706\n",
      "Iteration 47, loss = 0.09735016\n",
      "Iteration 45, loss = 0.16811711\n",
      "Iteration 72, loss = 0.24395066\n",
      "Iteration 89, loss = 0.24442593\n",
      "Iteration 69, loss = 0.23299184\n",
      "Iteration 62, loss = 0.29803990\n",
      "Iteration 90, loss = 0.24428599\n",
      "Iteration 73, loss = 0.24388444\n",
      "Iteration 70, loss = 0.23302670\n",
      "Iteration 63, loss = 0.29780261\n",
      "Iteration 48, loss = 0.09731346\n",
      "Iteration 46, loss = 0.16718734\n",
      "Iteration 91, loss = 0.24388978\n",
      "Iteration 74, loss = 0.24336204\n",
      "Iteration 71, loss = 0.23203365\n",
      "Iteration 92, loss = 0.24403060\n",
      "Iteration 64, loss = 0.29771575\n",
      "Iteration 75, loss = 0.24334367\n",
      "Iteration 49, loss = 0.09660371\n",
      "Iteration 72, loss = 0.23214165\n",
      "Iteration 47, loss = 0.16716903\n",
      "Iteration 93, loss = 0.24379700\n",
      "Iteration 76, loss = 0.24341879\n",
      "Iteration 65, loss = 0.29752356\n",
      "Iteration 94, loss = 0.24365452\n",
      "Iteration 73, loss = 0.23185530\n",
      "Iteration 50, loss = 0.09592887\n",
      "Iteration 95, loss = 0.24331837\n",
      "Iteration 48, loss = 0.16641379\n",
      "Iteration 77, loss = 0.24346415\n",
      "Iteration 66, loss = 0.29732318\n",
      "Iteration 74, loss = 0.23184677\n",
      "Iteration 96, loss = 0.24345071\n",
      "Iteration 78, loss = 0.24308332\n",
      "Iteration 75, loss = 0.23112159\n",
      "Iteration 51, loss = 0.09539660\n",
      "Iteration 97, loss = 0.24292487\n",
      "Iteration 49, loss = 0.16580124\n",
      "Iteration 67, loss = 0.29693309\n",
      "Iteration 79, loss = 0.24273433\n",
      "Iteration 98, loss = 0.24307036\n",
      "Iteration 76, loss = 0.23121697\n",
      "Iteration 68, loss = 0.29676291\n",
      "Iteration 80, loss = 0.24241422\n",
      "Iteration 99, loss = 0.24330268\n",
      "Iteration 52, loss = 0.09516203\n",
      "Iteration 50, loss = 0.16510102\n",
      "Iteration 77, loss = 0.23049245\n",
      "Iteration 100, loss = 0.24236588\n",
      "Iteration 81, loss = 0.24219819\n",
      "Iteration 69, loss = 0.29650830\n",
      "Iteration 78, loss = 0.23046586\n",
      "Iteration 101, loss = 0.24274866\n",
      "Iteration 53, loss = 0.09459284\n",
      "Iteration 51, loss = 0.16491349\n",
      "Iteration 82, loss = 0.24211556\n",
      "Iteration 102, loss = 0.24246654\n",
      "Iteration 70, loss = 0.29638862\n",
      "Iteration 79, loss = 0.23085424\n",
      "Iteration 83, loss = 0.24197099\n",
      "Iteration 103, loss = 0.24228929\n",
      "Iteration 54, loss = 0.09410949\n",
      "Iteration 52, loss = 0.16457416\n",
      "Iteration 80, loss = 0.22976714\n",
      "Iteration 71, loss = 0.29599581\n",
      "Iteration 84, loss = 0.24177985\n",
      "Iteration 104, loss = 0.24223448\n",
      "Iteration 81, loss = 0.22972272\n",
      "Iteration 105, loss = 0.24196174\n",
      "Iteration 72, loss = 0.29609621\n",
      "Iteration 85, loss = 0.24124013\n",
      "Iteration 55, loss = 0.09349817\n",
      "Iteration 53, loss = 0.16363333\n",
      "Iteration 106, loss = 0.24200896\n",
      "Iteration 82, loss = 0.22952212\n",
      "Iteration 86, loss = 0.24144827\n",
      "Iteration 73, loss = 0.29621998\n",
      "Iteration 107, loss = 0.24202392\n",
      "Iteration 56, loss = 0.09329843\n",
      "Iteration 83, loss = 0.22955833\n",
      "Iteration 54, loss = 0.16311001\n",
      "Iteration 87, loss = 0.24128685\n",
      "Iteration 108, loss = 0.24126884\n",
      "Iteration 74, loss = 0.29556360\n",
      "Iteration 84, loss = 0.22934097\n",
      "Iteration 88, loss = 0.24095198\n",
      "Iteration 109, loss = 0.24155022\n",
      "Iteration 57, loss = 0.09279433\n",
      "Iteration 55, loss = 0.16276624\n",
      "Iteration 110, loss = 0.24135157\n",
      "Iteration 85, loss = 0.22941814\n",
      "Iteration 75, loss = 0.29539579\n",
      "Iteration 89, loss = 0.24147502\n",
      "Iteration 111, loss = 0.24162041\n",
      "Iteration 86, loss = 0.22882519\n",
      "Iteration 58, loss = 0.09214983\n",
      "Iteration 90, loss = 0.24008782\n",
      "Iteration 76, loss = 0.29517641\n",
      "Iteration 56, loss = 0.16245081\n",
      "Iteration 112, loss = 0.24103198\n",
      "Iteration 87, loss = 0.22840937\n",
      "Iteration 91, loss = 0.24053716\n",
      "Iteration 113, loss = 0.24136910\n",
      "Iteration 77, loss = 0.29516607\n",
      "Iteration 59, loss = 0.09176181\n",
      "Iteration 88, loss = 0.22848722\n",
      "Iteration 114, loss = 0.24109114\n",
      "Iteration 57, loss = 0.16211250\n",
      "Iteration 92, loss = 0.24082807\n",
      "Iteration 78, loss = 0.29458668\n",
      "Iteration 115, loss = 0.24071076\n",
      "Iteration 93, loss = 0.24029733\n",
      "Iteration 89, loss = 0.22847720\n",
      "Iteration 60, loss = 0.09230982\n",
      "Iteration 116, loss = 0.24078995\n",
      "Iteration 58, loss = 0.16154864\n",
      "Iteration 79, loss = 0.29483312\n",
      "Iteration 94, loss = 0.24016848\n",
      "Iteration 90, loss = 0.22801030\n",
      "Iteration 117, loss = 0.24075687\n",
      "Iteration 61, loss = 0.09061219\n",
      "Iteration 95, loss = 0.23991957\n",
      "Iteration 80, loss = 0.29455012\n",
      "Iteration 118, loss = 0.24053421\n",
      "Iteration 91, loss = 0.22774076\n",
      "Iteration 59, loss = 0.16087456\n",
      "Iteration 119, loss = 0.24041417\n",
      "Iteration 96, loss = 0.23996703\n",
      "Iteration 92, loss = 0.22809641\n",
      "Iteration 81, loss = 0.29432041\n",
      "Iteration 62, loss = 0.09090540\n",
      "Iteration 120, loss = 0.24017084\n",
      "Iteration 97, loss = 0.23947181\n",
      "Iteration 60, loss = 0.16066260\n",
      "Iteration 93, loss = 0.22741987\n",
      "Iteration 121, loss = 0.24032145\n",
      "Iteration 82, loss = 0.29437522\n",
      "Iteration 98, loss = 0.23956740\n",
      "Iteration 63, loss = 0.09092998\n",
      "Iteration 94, loss = 0.22714365\n",
      "Iteration 122, loss = 0.24018225\n",
      "Iteration 61, loss = 0.15985602\n",
      "Iteration 83, loss = 0.29402814\n",
      "Iteration 99, loss = 0.23913036\n",
      "Iteration 123, loss = 0.23998304\n",
      "Iteration 95, loss = 0.22758236\n",
      "Iteration 64, loss = 0.09034733\n",
      "Iteration 124, loss = 0.24023099\n",
      "Iteration 100, loss = 0.23923590\n",
      "Iteration 84, loss = 0.29409943\n",
      "Iteration 96, loss = 0.22727575\n",
      "Iteration 62, loss = 0.15980687\n",
      "Iteration 125, loss = 0.23981318\n",
      "Iteration 101, loss = 0.23892549\n",
      "Iteration 97, loss = 0.22727179\n",
      "Iteration 65, loss = 0.09005005\n",
      "Iteration 85, loss = 0.29367473\n",
      "Iteration 126, loss = 0.23963096\n",
      "Iteration 63, loss = 0.15938027\n",
      "Iteration 102, loss = 0.23884619\n",
      "Iteration 127, loss = 0.23941416\n",
      "Iteration 98, loss = 0.22621899\n",
      "Iteration 86, loss = 0.29377344\n",
      "Iteration 103, loss = 0.23889532\n",
      "Iteration 66, loss = 0.08991488\n",
      "Iteration 128, loss = 0.24015017\n",
      "Iteration 99, loss = 0.22712702\n",
      "Iteration 64, loss = 0.15918983\n",
      "Iteration 129, loss = 0.23891936\n",
      "Iteration 87, loss = 0.29358607\n",
      "Iteration 104, loss = 0.23867918\n",
      "Iteration 100, loss = 0.22580046\n",
      "Iteration 130, loss = 0.23999927\n",
      "Iteration 67, loss = 0.09023170\n",
      "Iteration 105, loss = 0.23866460\n",
      "Iteration 88, loss = 0.29329261\n",
      "Iteration 65, loss = 0.15843394\n",
      "Iteration 131, loss = 0.23848635\n",
      "Iteration 101, loss = 0.22631091\n",
      "Iteration 106, loss = 0.23805703\n",
      "Iteration 132, loss = 0.23912721\n",
      "Iteration 68, loss = 0.08880643\n",
      "Iteration 89, loss = 0.29331795\n",
      "Iteration 102, loss = 0.22600542\n",
      "Iteration 66, loss = 0.15833261\n",
      "Iteration 133, loss = 0.23913776\n",
      "Iteration 107, loss = 0.23922548\n",
      "Iteration 103, loss = 0.22605708\n",
      "Iteration 90, loss = 0.29317483\n",
      "Iteration 134, loss = 0.23899536\n",
      "Iteration 69, loss = 0.08871130\n",
      "Iteration 108, loss = 0.23838175\n",
      "Iteration 135, loss = 0.23877097\n",
      "Iteration 67, loss = 0.15827005\n",
      "Iteration 104, loss = 0.22539375\n",
      "Iteration 91, loss = 0.29311368\n",
      "Iteration 109, loss = 0.23792292\n",
      "Iteration 136, loss = 0.23870136\n",
      "Iteration 70, loss = 0.08824466\n",
      "Iteration 105, loss = 0.22618108\n",
      "Iteration 110, loss = 0.23810957\n",
      "Iteration 137, loss = 0.23836509\n",
      "Iteration 92, loss = 0.29280376\n",
      "Iteration 68, loss = 0.15756623\n",
      "Iteration 106, loss = 0.22536376\n",
      "Iteration 138, loss = 0.23884579\n",
      "Iteration 111, loss = 0.23776005\n",
      "Iteration 71, loss = 0.08823442\n",
      "Iteration 93, loss = 0.29289114\n",
      "Iteration 139, loss = 0.23865878\n",
      "Iteration 107, loss = 0.22534509\n",
      "Iteration 112, loss = 0.23749625\n",
      "Iteration 69, loss = 0.15743509\n",
      "Iteration 140, loss = 0.23855994\n",
      "Iteration 94, loss = 0.29242608\n",
      "Iteration 72, loss = 0.08809578\n",
      "Iteration 108, loss = 0.22532727\n",
      "Iteration 113, loss = 0.23730322\n",
      "Iteration 141, loss = 0.23821942\n",
      "Iteration 70, loss = 0.15687442\n",
      "Iteration 109, loss = 0.22462545\n",
      "Iteration 114, loss = 0.23747338\n",
      "Iteration 95, loss = 0.29235368\n",
      "Iteration 142, loss = 0.23838319\n",
      "Iteration 73, loss = 0.08705228\n",
      "Iteration 115, loss = 0.23742124\n",
      "Iteration 110, loss = 0.22517189\n",
      "Iteration 143, loss = 0.23819343\n",
      "Iteration 96, loss = 0.29208461\n",
      "Iteration 71, loss = 0.15648489\n",
      "Iteration 144, loss = 0.23816040\n",
      "Iteration 116, loss = 0.23707090\n",
      "Iteration 111, loss = 0.22459820\n",
      "Iteration 74, loss = 0.08718542\n",
      "Iteration 97, loss = 0.29213596\n",
      "Iteration 145, loss = 0.23801890\n",
      "Iteration 117, loss = 0.23713047\n",
      "Iteration 112, loss = 0.22474156\n",
      "Iteration 72, loss = 0.15662130\n",
      "Iteration 146, loss = 0.23760627\n",
      "Iteration 75, loss = 0.08719028\n",
      "Iteration 98, loss = 0.29211950\n",
      "Iteration 118, loss = 0.23701985\n",
      "Iteration 113, loss = 0.22469490\n",
      "Iteration 147, loss = 0.23796411\n",
      "Iteration 73, loss = 0.15603286\n",
      "Iteration 119, loss = 0.23680528\n",
      "Iteration 148, loss = 0.23790660\n",
      "Iteration 114, loss = 0.22417106\n",
      "Iteration 99, loss = 0.29203956\n",
      "Iteration 76, loss = 0.08672726\n",
      "Iteration 149, loss = 0.23762139\n",
      "Iteration 120, loss = 0.23714273\n",
      "Iteration 115, loss = 0.22446522\n",
      "Iteration 74, loss = 0.15568358\n",
      "Iteration 100, loss = 0.29181041\n",
      "Iteration 150, loss = 0.23799731\n",
      "Iteration 121, loss = 0.23688401\n",
      "Iteration 77, loss = 0.08665379\n",
      "Iteration 116, loss = 0.22382424\n",
      "Iteration 151, loss = 0.23740459\n",
      "Iteration 101, loss = 0.29168685\n",
      "Iteration 122, loss = 0.23639743\n",
      "Iteration 75, loss = 0.15547608\n",
      "Iteration 117, loss = 0.22406157\n",
      "Iteration 152, loss = 0.23758725\n",
      "Iteration 78, loss = 0.08632766\n",
      "Iteration 123, loss = 0.23654305\n",
      "Iteration 102, loss = 0.29153691\n",
      "Iteration 153, loss = 0.23703314\n",
      "Iteration 118, loss = 0.22394184\n",
      "Iteration 76, loss = 0.15550046\n",
      "Iteration 124, loss = 0.23689376\n",
      "Iteration 154, loss = 0.23735645\n",
      "Iteration 103, loss = 0.29141220\n",
      "Iteration 119, loss = 0.22358654\n",
      "Iteration 79, loss = 0.08629717\n",
      "Iteration 155, loss = 0.23749151\n",
      "Iteration 125, loss = 0.23573430\n",
      "Iteration 120, loss = 0.22375824\n",
      "Iteration 77, loss = 0.15476749\n",
      "Iteration 156, loss = 0.23729534\n",
      "Iteration 104, loss = 0.29124867\n",
      "Iteration 126, loss = 0.23611453\n",
      "Iteration 80, loss = 0.08578864\n",
      "Iteration 157, loss = 0.23702425\n",
      "Iteration 121, loss = 0.22334046\n",
      "Iteration 105, loss = 0.29117679\n",
      "Iteration 127, loss = 0.23640096\n",
      "Iteration 78, loss = 0.15461967\n",
      "Iteration 158, loss = 0.23720269\n",
      "Iteration 122, loss = 0.22357941\n",
      "Iteration 81, loss = 0.08541517\n",
      "Iteration 128, loss = 0.23569105\n",
      "Iteration 159, loss = 0.23720934\n",
      "Iteration 106, loss = 0.29101972\n",
      "Iteration 123, loss = 0.22331204\n",
      "Iteration 79, loss = 0.15423413\n",
      "Iteration 160, loss = 0.23739267\n",
      "Iteration 129, loss = 0.23615348\n",
      "Iteration 82, loss = 0.08532421\n",
      "Iteration 107, loss = 0.29090144\n",
      "Iteration 161, loss = 0.23678846\n",
      "Iteration 124, loss = 0.22330967\n",
      "Iteration 130, loss = 0.23590020\n",
      "Iteration 162, loss = 0.23684823\n",
      "Iteration 80, loss = 0.15470028\n",
      "Iteration 108, loss = 0.29117432\n",
      "Iteration 125, loss = 0.22326187\n",
      "Iteration 131, loss = 0.23582596\n",
      "Iteration 83, loss = 0.08527839\n",
      "Iteration 163, loss = 0.23662136\n",
      "Iteration 126, loss = 0.22289010\n",
      "Iteration 109, loss = 0.29060458\n",
      "Iteration 164, loss = 0.23666273\n",
      "Iteration 132, loss = 0.23579891\n",
      "Iteration 81, loss = 0.15408159\n",
      "Iteration 84, loss = 0.08486503\n",
      "Iteration 165, loss = 0.23693130\n",
      "Iteration 127, loss = 0.22273755\n",
      "Iteration 133, loss = 0.23510105\n",
      "Iteration 110, loss = 0.29055257\n",
      "Iteration 166, loss = 0.23659933\n",
      "Iteration 82, loss = 0.15382500\n",
      "Iteration 128, loss = 0.22310290\n",
      "Iteration 134, loss = 0.23569218\n",
      "Iteration 85, loss = 0.08443248\n",
      "Iteration 167, loss = 0.23634529\n",
      "Iteration 111, loss = 0.29048455\n",
      "Iteration 129, loss = 0.22300483\n",
      "Iteration 135, loss = 0.23505574\n",
      "Iteration 168, loss = 0.23659156\n",
      "Iteration 83, loss = 0.15414334\n",
      "Iteration 112, loss = 0.29032667\n",
      "Iteration 130, loss = 0.22259342\n",
      "Iteration 169, loss = 0.23671944\n",
      "Iteration 86, loss = 0.08441092\n",
      "Iteration 136, loss = 0.23553975\n",
      "Iteration 170, loss = 0.23678577\n",
      "Iteration 113, loss = 0.29018640\n",
      "Iteration 131, loss = 0.22245997\n",
      "Iteration 137, loss = 0.23512485\n",
      "Iteration 84, loss = 0.15334374\n",
      "Iteration 171, loss = 0.23611811\n",
      "Iteration 87, loss = 0.08417572\n",
      "Iteration 132, loss = 0.22227807\n",
      "Iteration 138, loss = 0.23512019\n",
      "Iteration 114, loss = 0.29018748\n",
      "Iteration 172, loss = 0.23689939\n",
      "Iteration 85, loss = 0.15308606\n",
      "Iteration 139, loss = 0.23535868\n",
      "Iteration 133, loss = 0.22210523\n",
      "Iteration 173, loss = 0.23585182\n",
      "Iteration 88, loss = 0.08431798\n",
      "Iteration 115, loss = 0.29013275\n",
      "Iteration 174, loss = 0.23649436\n",
      "Iteration 140, loss = 0.23520819\n",
      "Iteration 134, loss = 0.22215632\n",
      "Iteration 86, loss = 0.15278812\n",
      "Iteration 175, loss = 0.23596603\n",
      "Iteration 116, loss = 0.28990985\n",
      "Iteration 89, loss = 0.08365425\n",
      "Iteration 141, loss = 0.23497728\n",
      "Iteration 135, loss = 0.22196848\n",
      "Iteration 176, loss = 0.23596968\n",
      "Iteration 117, loss = 0.28968496\n",
      "Iteration 142, loss = 0.23495571\n",
      "Iteration 136, loss = 0.22227516\n",
      "Iteration 87, loss = 0.15263442\n",
      "Iteration 177, loss = 0.23608740\n",
      "Iteration 90, loss = 0.08336570\n",
      "Iteration 143, loss = 0.23482632\n",
      "Iteration 178, loss = 0.23561184\n",
      "Iteration 137, loss = 0.22194503\n",
      "Iteration 118, loss = 0.28974648\n",
      "Iteration 88, loss = 0.15240605\n",
      "Iteration 179, loss = 0.23554444\n",
      "Iteration 144, loss = 0.23437531\n",
      "Iteration 138, loss = 0.22176187\n",
      "Iteration 91, loss = 0.08349361\n",
      "Iteration 119, loss = 0.28980359\n",
      "Iteration 180, loss = 0.23569423\n",
      "Iteration 145, loss = 0.23514277\n",
      "Iteration 139, loss = 0.22131575\n",
      "Iteration 181, loss = 0.23589878\n",
      "Iteration 89, loss = 0.15298913\n",
      "Iteration 120, loss = 0.28966377\n",
      "Iteration 92, loss = 0.08302838\n",
      "Iteration 146, loss = 0.23445574\n",
      "Iteration 182, loss = 0.23637369\n",
      "Iteration 140, loss = 0.22181196\n",
      "Iteration 147, loss = 0.23426650\n",
      "Iteration 183, loss = 0.23532556\n",
      "Iteration 121, loss = 0.28967480\n",
      "Iteration 90, loss = 0.15207604\n",
      "Iteration 141, loss = 0.22156662\n",
      "Iteration 93, loss = 0.08317316\n",
      "Iteration 184, loss = 0.23543099\n",
      "Iteration 148, loss = 0.23431022\n",
      "Iteration 122, loss = 0.28916358\n",
      "Iteration 142, loss = 0.22130561\n",
      "Iteration 185, loss = 0.23524817\n",
      "Iteration 91, loss = 0.15207463\n",
      "Iteration 149, loss = 0.23444680\n",
      "Iteration 94, loss = 0.08321510\n",
      "Iteration 143, loss = 0.22112926\n",
      "Iteration 186, loss = 0.23544205\n",
      "Iteration 123, loss = 0.28947057\n",
      "Iteration 150, loss = 0.23409951\n",
      "Iteration 187, loss = 0.23586716\n",
      "Iteration 144, loss = 0.22184108\n",
      "Iteration 92, loss = 0.15141938\n",
      "Iteration 95, loss = 0.08222673\n",
      "Iteration 124, loss = 0.28915788\n",
      "Iteration 188, loss = 0.23523400\n",
      "Iteration 151, loss = 0.23373972\n",
      "Iteration 145, loss = 0.22095864\n",
      "Iteration 189, loss = 0.23546632\n",
      "Iteration 152, loss = 0.23417184\n",
      "Iteration 125, loss = 0.28933565\n",
      "Iteration 93, loss = 0.15197259\n",
      "Iteration 96, loss = 0.08210196\n",
      "Iteration 146, loss = 0.22138251\n",
      "Iteration 190, loss = 0.23528399\n",
      "Iteration 153, loss = 0.23402570\n",
      "Iteration 126, loss = 0.28896748\n",
      "Iteration 191, loss = 0.23543467\n",
      "Iteration 147, loss = 0.22104334\n",
      "Iteration 94, loss = 0.15146756\n",
      "Iteration 154, loss = 0.23371564\n",
      "Iteration 97, loss = 0.08200668\n",
      "Iteration 192, loss = 0.23545502\n",
      "Iteration 148, loss = 0.22104779\n",
      "Iteration 127, loss = 0.28885208\n",
      "Iteration 193, loss = 0.23491890\n",
      "Iteration 155, loss = 0.23383935\n",
      "Iteration 95, loss = 0.15079429\n",
      "Iteration 149, loss = 0.22068993\n",
      "Iteration 98, loss = 0.08157553\n",
      "Iteration 194, loss = 0.23503279\n",
      "Iteration 128, loss = 0.28903489\n",
      "Iteration 156, loss = 0.23387062\n",
      "Iteration 195, loss = 0.23532733\n",
      "Iteration 150, loss = 0.22061231\n",
      "Iteration 157, loss = 0.23340641\n",
      "Iteration 96, loss = 0.15092915\n",
      "Iteration 129, loss = 0.28858336\n",
      "Iteration 99, loss = 0.08163109\n",
      "Iteration 196, loss = 0.23447513\n",
      "Iteration 151, loss = 0.22039329\n",
      "Iteration 158, loss = 0.23373559\n",
      "Iteration 197, loss = 0.23517394\n",
      "Iteration 130, loss = 0.28898162\n",
      "Iteration 152, loss = 0.22070393\n",
      "Iteration 97, loss = 0.15072658\n",
      "Iteration 100, loss = 0.08162991\n",
      "Iteration 198, loss = 0.23491337\n",
      "Iteration 159, loss = 0.23359597\n",
      "Iteration 131, loss = 0.28864047\n",
      "Iteration 153, loss = 0.22021463\n",
      "Iteration 199, loss = 0.23473700\n",
      "Iteration 160, loss = 0.23296403\n",
      "Iteration 98, loss = 0.15075882\n",
      "Iteration 101, loss = 0.08171159\n",
      "Iteration 200, loss = 0.23447040\n",
      "Iteration 154, loss = 0.22044769\n",
      "Iteration 161, loss = 0.23304598\n",
      "Iteration 132, loss = 0.28856347\n",
      "Iteration 201, loss = 0.23470660\n",
      "Iteration 155, loss = 0.22018470\n",
      "Iteration 162, loss = 0.23328303\n",
      "Iteration 99, loss = 0.15037215\n",
      "Iteration 202, loss = 0.23473759\n",
      "Iteration 133, loss = 0.28839759\n",
      "Iteration 102, loss = 0.08105370\n",
      "Iteration 156, loss = 0.22020026\n",
      "Iteration 163, loss = 0.23392201\n",
      "Iteration 203, loss = 0.23456494\n",
      "Iteration 134, loss = 0.28834161\n",
      "Iteration 204, loss = 0.23481537\n",
      "Iteration 100, loss = 0.15013205\n",
      "Iteration 157, loss = 0.21980263\n",
      "Iteration 103, loss = 0.08076763\n",
      "Iteration 164, loss = 0.23279038\n",
      "Iteration 205, loss = 0.23495972\n",
      "Iteration 135, loss = 0.28823281\n",
      "Iteration 158, loss = 0.21995836\n",
      "Iteration 165, loss = 0.23299764\n",
      "Iteration 206, loss = 0.23453443\n",
      "Iteration 101, loss = 0.14971292\n",
      "Iteration 104, loss = 0.08110806\n",
      "Iteration 166, loss = 0.23295230\n",
      "Iteration 159, loss = 0.22042137\n",
      "Iteration 136, loss = 0.28816525\n",
      "Iteration 207, loss = 0.23430219\n",
      "Iteration 167, loss = 0.23272192\n",
      "Iteration 208, loss = 0.23446406\n",
      "Iteration 160, loss = 0.21973939\n",
      "Iteration 102, loss = 0.14976686\n",
      "Iteration 105, loss = 0.08045272\n",
      "Iteration 137, loss = 0.28827576\n",
      "Iteration 209, loss = 0.23431428\n",
      "Iteration 168, loss = 0.23304600\n",
      "Iteration 161, loss = 0.21970403\n",
      "Iteration 210, loss = 0.23419606\n",
      "Iteration 138, loss = 0.28787621\n",
      "Iteration 103, loss = 0.14948930\n",
      "Iteration 106, loss = 0.08043257\n",
      "Iteration 169, loss = 0.23282083\n",
      "Iteration 162, loss = 0.21961344\n",
      "Iteration 211, loss = 0.23437787\n",
      "Iteration 139, loss = 0.28827602\n",
      "Iteration 170, loss = 0.23269808\n",
      "Iteration 212, loss = 0.23384636\n",
      "Iteration 163, loss = 0.21984217\n",
      "Iteration 107, loss = 0.08060070\n",
      "Iteration 104, loss = 0.14932008\n",
      "Iteration 213, loss = 0.23405760\n",
      "Iteration 171, loss = 0.23275723\n",
      "Iteration 140, loss = 0.28776628\n",
      "Iteration 164, loss = 0.21987222\n",
      "Iteration 214, loss = 0.23409032\n",
      "Iteration 172, loss = 0.23306511\n",
      "Iteration 108, loss = 0.08038977\n",
      "Iteration 105, loss = 0.14917515\n",
      "Iteration 165, loss = 0.21919316\n",
      "Iteration 141, loss = 0.28796659\n",
      "Iteration 215, loss = 0.23416468\n",
      "Iteration 173, loss = 0.23224428\n",
      "Iteration 166, loss = 0.21995975\n",
      "Iteration 216, loss = 0.23377681\n",
      "Iteration 142, loss = 0.28804382\n",
      "Iteration 109, loss = 0.07978483\n",
      "Iteration 106, loss = 0.14872824\n",
      "Iteration 174, loss = 0.23255132\n",
      "Iteration 217, loss = 0.23372922\n",
      "Iteration 167, loss = 0.21936073\n",
      "Iteration 218, loss = 0.23408222\n",
      "Iteration 143, loss = 0.28749124\n",
      "Iteration 175, loss = 0.23263225\n",
      "Iteration 110, loss = 0.07995659\n",
      "Iteration 168, loss = 0.21953402\n",
      "Iteration 107, loss = 0.14951369\n",
      "Iteration 219, loss = 0.23420776\n",
      "Iteration 176, loss = 0.23236972\n",
      "Iteration 144, loss = 0.28780750\n",
      "Iteration 169, loss = 0.21938766\n",
      "Iteration 220, loss = 0.23369344\n",
      "Iteration 111, loss = 0.08031101\n",
      "Iteration 177, loss = 0.23242638\n",
      "Iteration 108, loss = 0.14852097\n",
      "Iteration 221, loss = 0.23373596\n",
      "Iteration 170, loss = 0.21893947\n",
      "Iteration 145, loss = 0.28774518\n",
      "Iteration 178, loss = 0.23217753\n",
      "Iteration 222, loss = 0.23408000\n",
      "Iteration 171, loss = 0.21960091\n",
      "Iteration 112, loss = 0.07947706\n",
      "Iteration 146, loss = 0.28759533\n",
      "Iteration 109, loss = 0.14860613\n",
      "Iteration 223, loss = 0.23370130\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 179, loss = 0.23207951\n",
      "Iteration 172, loss = 0.21885263\n",
      "Iteration 180, loss = 0.23179776\n",
      "Iteration 147, loss = 0.28731609\n",
      "Iteration 113, loss = 0.07980029\n",
      "Iteration 110, loss = 0.14814694\n",
      "Iteration 173, loss = 0.21875084\n",
      "Iteration 181, loss = 0.23235283\n",
      "Iteration 148, loss = 0.28746683\n",
      "Iteration 174, loss = 0.21873322\n",
      "Iteration 114, loss = 0.07938170\n",
      "Iteration 182, loss = 0.23181251\n",
      "Iteration 111, loss = 0.14849705\n",
      "Iteration 149, loss = 0.28754353\n",
      "Iteration 175, loss = 0.21881706\n",
      "Iteration 183, loss = 0.23207355\n",
      "Iteration 115, loss = 0.07939824\n",
      "Iteration 176, loss = 0.21861208\n",
      "Iteration 150, loss = 0.28702164\n",
      "Iteration 112, loss = 0.14802736\n",
      "Iteration 184, loss = 0.23192048\n",
      "Iteration 177, loss = 0.21967730\n",
      "Iteration 185, loss = 0.23198893\n",
      "Iteration 151, loss = 0.28724733\n",
      "Iteration 116, loss = 0.07906491\n",
      "Iteration 113, loss = 0.14756287\n",
      "Iteration 178, loss = 0.21863697\n",
      "Iteration 186, loss = 0.23127394\n",
      "Iteration 152, loss = 0.28739302\n",
      "Iteration 117, loss = 0.07924026\n",
      "Iteration 179, loss = 0.21849341\n",
      "Iteration 187, loss = 0.23159734\n",
      "Iteration 114, loss = 0.14780928\n",
      "Iteration 153, loss = 0.28696297\n",
      "Iteration 180, loss = 0.21868151\n",
      "Iteration 188, loss = 0.23136347\n",
      "Iteration 118, loss = 0.07873041\n",
      "Iteration 115, loss = 0.14757999\n",
      "Iteration 154, loss = 0.28767322\n",
      "Iteration 181, loss = 0.21847593\n",
      "Iteration 189, loss = 0.23172312\n",
      "Iteration 182, loss = 0.21850783\n",
      "Iteration 190, loss = 0.23148021\n",
      "Iteration 119, loss = 0.07886620\n",
      "Iteration 155, loss = 0.28686532\n",
      "Iteration 116, loss = 0.14759397\n",
      "Iteration 191, loss = 0.23139329\n",
      "Iteration 183, loss = 0.21832447\n",
      "Iteration 156, loss = 0.28693253\n",
      "Iteration 120, loss = 0.07863856\n",
      "Iteration 192, loss = 0.23109925\n",
      "Iteration 184, loss = 0.21816567\n",
      "Iteration 117, loss = 0.14750739\n",
      "Iteration 157, loss = 0.28716393\n",
      "Iteration 193, loss = 0.23168008\n",
      "Iteration 185, loss = 0.21836660\n",
      "Iteration 121, loss = 0.07840981\n",
      "Iteration 118, loss = 0.14677710\n",
      "Iteration 158, loss = 0.28662439\n",
      "Iteration 194, loss = 0.23096035\n",
      "Iteration 186, loss = 0.21810853\n",
      "Iteration 122, loss = 0.07800422\n",
      "Iteration 195, loss = 0.23116397\n",
      "Iteration 159, loss = 0.28693071\n",
      "Iteration 187, loss = 0.21819360\n",
      "Iteration 119, loss = 0.14648407\n",
      "Iteration 196, loss = 0.23166774\n",
      "Iteration 188, loss = 0.21831305\n",
      "Iteration 160, loss = 0.28688433\n",
      "Iteration 123, loss = 0.07795226\n",
      "Iteration 197, loss = 0.23088714\n",
      "Iteration 120, loss = 0.14706439\n",
      "Iteration 189, loss = 0.21763409\n",
      "Iteration 161, loss = 0.28651749\n",
      "Iteration 198, loss = 0.23084421\n",
      "Iteration 124, loss = 0.07849140\n",
      "Iteration 190, loss = 0.21745851\n",
      "Iteration 121, loss = 0.14663601\n",
      "Iteration 162, loss = 0.28662133\n",
      "Iteration 199, loss = 0.23103793\n",
      "Iteration 191, loss = 0.21807594\n",
      "Iteration 125, loss = 0.07831083\n",
      "Iteration 200, loss = 0.23102612\n",
      "Iteration 163, loss = 0.28680324\n",
      "Iteration 192, loss = 0.21777003\n",
      "Iteration 122, loss = 0.14634479\n",
      "Iteration 201, loss = 0.23080069\n",
      "Iteration 126, loss = 0.07791430\n",
      "Iteration 193, loss = 0.21761882\n",
      "Iteration 164, loss = 0.28647303\n",
      "Iteration 202, loss = 0.23146904\n",
      "Iteration 123, loss = 0.14619778\n",
      "Iteration 194, loss = 0.21793905\n",
      "Iteration 165, loss = 0.28625596\n",
      "Iteration 203, loss = 0.23074528\n",
      "Iteration 127, loss = 0.07792696\n",
      "Iteration 195, loss = 0.21732588\n",
      "Iteration 124, loss = 0.14599278\n",
      "Iteration 204, loss = 0.23132635\n",
      "Iteration 166, loss = 0.28630437\n",
      "Iteration 196, loss = 0.21766671\n",
      "Iteration 128, loss = 0.07775057\n",
      "Iteration 205, loss = 0.23050938\n",
      "Iteration 167, loss = 0.28645948\n",
      "Iteration 125, loss = 0.14632350\n",
      "Iteration 197, loss = 0.21756233\n",
      "Iteration 206, loss = 0.23063022\n",
      "Iteration 129, loss = 0.07751748\n",
      "Iteration 168, loss = 0.28616831\n",
      "Iteration 198, loss = 0.21716165\n",
      "Iteration 207, loss = 0.23139395\n",
      "Iteration 126, loss = 0.14576125\n",
      "Iteration 199, loss = 0.21789415\n",
      "Iteration 169, loss = 0.28599736\n",
      "Iteration 208, loss = 0.23045490\n",
      "Iteration 130, loss = 0.07750587\n",
      "Iteration 200, loss = 0.21714284\n",
      "Iteration 127, loss = 0.14570615\n",
      "Iteration 209, loss = 0.23065634\n",
      "Iteration 170, loss = 0.28631497\n",
      "Iteration 131, loss = 0.07724420\n",
      "Iteration 201, loss = 0.21818127\n",
      "Iteration 210, loss = 0.23029220\n",
      "Iteration 171, loss = 0.28603530\n",
      "Iteration 128, loss = 0.14557390\n",
      "Iteration 202, loss = 0.21676845\n",
      "Iteration 211, loss = 0.23095674\n",
      "Iteration 132, loss = 0.07688173\n",
      "Iteration 172, loss = 0.28611963\n",
      "Iteration 203, loss = 0.21715443\n",
      "Iteration 212, loss = 0.23045590\n",
      "Iteration 129, loss = 0.14567154\n",
      "Iteration 173, loss = 0.28577906\n",
      "Iteration 133, loss = 0.07701023\n",
      "Iteration 204, loss = 0.21766384\n",
      "Iteration 213, loss = 0.23002554\n",
      "Iteration 130, loss = 0.14565175\n",
      "Iteration 205, loss = 0.21685238\n",
      "Iteration 214, loss = 0.23019974\n",
      "Iteration 174, loss = 0.28609077\n",
      "Iteration 134, loss = 0.07757079\n",
      "Iteration 215, loss = 0.23040032\n",
      "Iteration 206, loss = 0.21745232\n",
      "Iteration 175, loss = 0.28571693\n",
      "Iteration 131, loss = 0.14518730\n",
      "Iteration 216, loss = 0.23033714\n",
      "Iteration 207, loss = 0.21663503\n",
      "Iteration 135, loss = 0.07635254\n",
      "Iteration 176, loss = 0.28610099\n",
      "Iteration 217, loss = 0.23059419\n",
      "Iteration 208, loss = 0.21677307\n",
      "Iteration 132, loss = 0.14536809\n",
      "Iteration 136, loss = 0.07660326\n",
      "Iteration 177, loss = 0.28555124\n",
      "Iteration 218, loss = 0.22992528\n",
      "Iteration 209, loss = 0.21703710\n",
      "Iteration 133, loss = 0.14495861\n",
      "Iteration 219, loss = 0.23035923\n",
      "Iteration 178, loss = 0.28568902\n",
      "Iteration 210, loss = 0.21713199\n",
      "Iteration 137, loss = 0.07682429\n",
      "Iteration 220, loss = 0.23032742\n",
      "Iteration 211, loss = 0.21683502\n",
      "Iteration 179, loss = 0.28578259\n",
      "Iteration 134, loss = 0.14504541\n",
      "Iteration 221, loss = 0.22967258\n",
      "Iteration 138, loss = 0.07649267\n",
      "Iteration 212, loss = 0.21658046\n",
      "Iteration 180, loss = 0.28552761\n",
      "Iteration 222, loss = 0.23027068\n",
      "Iteration 135, loss = 0.14453073\n",
      "Iteration 213, loss = 0.21663899\n",
      "Iteration 139, loss = 0.07625995\n",
      "Iteration 181, loss = 0.28553115\n",
      "Iteration 223, loss = 0.22993549\n",
      "Iteration 214, loss = 0.21669146\n",
      "Iteration 136, loss = 0.14454215\n",
      "Iteration 224, loss = 0.23026112\n",
      "Iteration 182, loss = 0.28579219\n",
      "Iteration 140, loss = 0.07626665\n",
      "Iteration 215, loss = 0.21648290\n",
      "Iteration 225, loss = 0.22993412\n",
      "Iteration 183, loss = 0.28540118\n",
      "Iteration 137, loss = 0.14420177\n",
      "Iteration 216, loss = 0.21683722\n",
      "Iteration 226, loss = 0.22930797\n",
      "Iteration 141, loss = 0.07647260\n",
      "Iteration 217, loss = 0.21644942\n",
      "Iteration 184, loss = 0.28548949\n",
      "Iteration 227, loss = 0.23019360\n",
      "Iteration 138, loss = 0.14444097\n",
      "Iteration 218, loss = 0.21679160\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 142, loss = 0.07595777\n",
      "Iteration 185, loss = 0.28532300\n",
      "Iteration 228, loss = 0.22974334\n",
      "Iteration 139, loss = 0.14382782\n",
      "Iteration 229, loss = 0.22987149\n",
      "Iteration 186, loss = 0.28521272\n",
      "Iteration 143, loss = 0.07618662\n",
      "Iteration 230, loss = 0.22974322\n",
      "Iteration 140, loss = 0.14436046\n",
      "Iteration 187, loss = 0.28556232\n",
      "Iteration 144, loss = 0.07593532\n",
      "Iteration 231, loss = 0.22970977\n",
      "Iteration 188, loss = 0.28511594\n",
      "Iteration 232, loss = 0.22963155\n",
      "Iteration 141, loss = 0.14403641\n",
      "Iteration 145, loss = 0.07640837\n",
      "Iteration 189, loss = 0.28544637\n",
      "Iteration 233, loss = 0.22941535\n",
      "Iteration 142, loss = 0.14389730\n",
      "Iteration 234, loss = 0.22979748\n",
      "Iteration 146, loss = 0.07518094\n",
      "Iteration 190, loss = 0.28501635\n",
      "Iteration 235, loss = 0.22913474\n",
      "Iteration 143, loss = 0.14386512\n",
      "Iteration 191, loss = 0.28505669\n",
      "Iteration 147, loss = 0.07545307\n",
      "Iteration 236, loss = 0.22985795\n",
      "Iteration 192, loss = 0.28535134\n",
      "Iteration 237, loss = 0.22928231\n",
      "Iteration 144, loss = 0.14390820\n",
      "Iteration 148, loss = 0.07584426\n",
      "Iteration 238, loss = 0.22962734\n",
      "Iteration 193, loss = 0.28491270\n",
      "Iteration 145, loss = 0.14363795\n",
      "Iteration 239, loss = 0.22918409\n",
      "Iteration 149, loss = 0.07555350\n",
      "Iteration 194, loss = 0.28488627\n",
      "Iteration 240, loss = 0.22919138\n",
      "Iteration 146, loss = 0.14312677\n",
      "Iteration 195, loss = 0.28495341\n",
      "Iteration 150, loss = 0.07530540\n",
      "Iteration 241, loss = 0.22905165\n",
      "Iteration 196, loss = 0.28501640\n",
      "Iteration 242, loss = 0.22955209\n",
      "Iteration 147, loss = 0.14344670\n",
      "Iteration 151, loss = 0.07519989\n",
      "Iteration 243, loss = 0.22951281\n",
      "Iteration 197, loss = 0.28472163\n",
      "Iteration 148, loss = 0.14391302\n",
      "Iteration 244, loss = 0.22912738\n",
      "Iteration 152, loss = 0.07514934\n",
      "Iteration 198, loss = 0.28501072\n",
      "Iteration 245, loss = 0.22905048\n",
      "Iteration 149, loss = 0.14327779\n",
      "Iteration 199, loss = 0.28456157\n",
      "Iteration 153, loss = 0.07489618\n",
      "Iteration 246, loss = 0.22919358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 200, loss = 0.28455659\n",
      "Iteration 150, loss = 0.14289156\n",
      "Iteration 154, loss = 0.07510294\n",
      "Iteration 201, loss = 0.28500273\n",
      "Iteration 151, loss = 0.14275162\n",
      "Iteration 155, loss = 0.07478114\n",
      "Iteration 202, loss = 0.28496743\n",
      "Iteration 152, loss = 0.14257253\n",
      "Iteration 203, loss = 0.28464714\n",
      "Iteration 156, loss = 0.07464360\n",
      "Iteration 204, loss = 0.28438610\n",
      "Iteration 153, loss = 0.14341103\n",
      "Iteration 157, loss = 0.07479062\n",
      "Iteration 205, loss = 0.28458765\n",
      "Iteration 154, loss = 0.14250570\n",
      "Iteration 158, loss = 0.07451048\n",
      "Iteration 206, loss = 0.28469778\n",
      "Iteration 155, loss = 0.14258398\n",
      "Iteration 207, loss = 0.28431836\n",
      "Iteration 159, loss = 0.07446059\n",
      "Iteration 208, loss = 0.28474682\n",
      "Iteration 156, loss = 0.14235403\n",
      "Iteration 160, loss = 0.07528606\n",
      "Iteration 209, loss = 0.28439824\n",
      "Iteration 157, loss = 0.14233945\n",
      "Iteration 161, loss = 0.07440659\n",
      "Iteration 210, loss = 0.28449087\n",
      "Iteration 158, loss = 0.14189883\n",
      "Iteration 162, loss = 0.07424618\n",
      "Iteration 211, loss = 0.28446563\n",
      "Iteration 212, loss = 0.28451184\n",
      "Iteration 159, loss = 0.14242300\n",
      "Iteration 163, loss = 0.07403273\n",
      "Iteration 213, loss = 0.28429259\n",
      "Iteration 160, loss = 0.14240133\n",
      "Iteration 164, loss = 0.07450752\n",
      "Iteration 214, loss = 0.28433328\n",
      "Iteration 161, loss = 0.14188729\n",
      "Iteration 165, loss = 0.07389019\n",
      "Iteration 215, loss = 0.28401781\n",
      "Iteration 216, loss = 0.28427367\n",
      "Iteration 162, loss = 0.14172615\n",
      "Iteration 166, loss = 0.07433000\n",
      "Iteration 217, loss = 0.28409112\n",
      "Iteration 167, loss = 0.07392151\n",
      "Iteration 163, loss = 0.14196008\n",
      "Iteration 218, loss = 0.28408886\n",
      "Iteration 168, loss = 0.07379045\n",
      "Iteration 164, loss = 0.14146111\n",
      "Iteration 219, loss = 0.28409114\n",
      "Iteration 169, loss = 0.07365344\n",
      "Iteration 220, loss = 0.28402479\n",
      "Iteration 165, loss = 0.14197963\n",
      "Iteration 221, loss = 0.28422839\n",
      "Iteration 170, loss = 0.07427950\n",
      "Iteration 166, loss = 0.14156467\n",
      "Iteration 222, loss = 0.28408543\n",
      "Iteration 171, loss = 0.07347843\n",
      "Iteration 167, loss = 0.14126370\n",
      "Iteration 223, loss = 0.28394864\n",
      "Iteration 172, loss = 0.07347047\n",
      "Iteration 224, loss = 0.28400700\n",
      "Iteration 168, loss = 0.14184029\n",
      "Iteration 225, loss = 0.28425174\n",
      "Iteration 173, loss = 0.07345729\n",
      "Iteration 169, loss = 0.14100295\n",
      "Iteration 226, loss = 0.28395271\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 174, loss = 0.07345622\n",
      "Iteration 170, loss = 0.14140875\n",
      "Iteration 175, loss = 0.07327459\n",
      "Iteration 171, loss = 0.14110268\n",
      "Iteration 176, loss = 0.07383669\n",
      "Iteration 172, loss = 0.14095910\n",
      "Iteration 177, loss = 0.07271680\n",
      "Iteration 173, loss = 0.14094413\n",
      "Iteration 178, loss = 0.07276998\n",
      "Iteration 174, loss = 0.14078035\n",
      "Iteration 179, loss = 0.07332775\n",
      "Iteration 175, loss = 0.14103049\n",
      "Iteration 180, loss = 0.07317710\n",
      "Iteration 176, loss = 0.14066423\n",
      "Iteration 181, loss = 0.07279329\n",
      "Iteration 177, loss = 0.14053956\n",
      "Iteration 182, loss = 0.07320578\n",
      "Iteration 178, loss = 0.14072461\n",
      "Iteration 183, loss = 0.07271563\n",
      "Iteration 179, loss = 0.14071287\n",
      "Iteration 184, loss = 0.07264333\n",
      "Iteration 180, loss = 0.14017747\n",
      "Iteration 185, loss = 0.07273791\n",
      "Iteration 181, loss = 0.14101235\n",
      "Iteration 186, loss = 0.07284991\n",
      "Iteration 182, loss = 0.13996618\n",
      "Iteration 187, loss = 0.07291173\n",
      "Iteration 183, loss = 0.14032810\n",
      "Iteration 188, loss = 0.07242555\n",
      "Iteration 184, loss = 0.13992979\n",
      "Iteration 189, loss = 0.07236443\n",
      "Iteration 185, loss = 0.14011641\n",
      "Iteration 190, loss = 0.07263314\n",
      "Iteration 186, loss = 0.13968841\n",
      "Iteration 191, loss = 0.07242311\n",
      "Iteration 187, loss = 0.13960905\n",
      "Iteration 192, loss = 0.07240991\n",
      "Iteration 188, loss = 0.13984358\n",
      "Iteration 193, loss = 0.07243720\n",
      "Iteration 189, loss = 0.13931106\n",
      "Iteration 194, loss = 0.07200379\n",
      "Iteration 190, loss = 0.13989433\n",
      "Iteration 195, loss = 0.07262478\n",
      "Iteration 191, loss = 0.14001121\n",
      "Iteration 196, loss = 0.07184733\n",
      "Iteration 192, loss = 0.13960649\n",
      "Iteration 197, loss = 0.07219800\n",
      "Iteration 193, loss = 0.13955063\n",
      "Iteration 198, loss = 0.07198851\n",
      "Iteration 194, loss = 0.13940481\n",
      "Iteration 199, loss = 0.07151580\n",
      "Iteration 195, loss = 0.13918684\n",
      "Iteration 200, loss = 0.07179939\n",
      "Iteration 196, loss = 0.13946249\n",
      "Iteration 201, loss = 0.07151317\n",
      "Iteration 197, loss = 0.13939596\n",
      "Iteration 202, loss = 0.07183423\n",
      "Iteration 198, loss = 0.13908339\n",
      "Iteration 203, loss = 0.07161002\n",
      "Iteration 199, loss = 0.13872786\n",
      "Iteration 204, loss = 0.07180461\n",
      "Iteration 200, loss = 0.13958427\n",
      "Iteration 205, loss = 0.07132920\n",
      "Iteration 201, loss = 0.13853674\n",
      "Iteration 206, loss = 0.07234669\n",
      "Iteration 202, loss = 0.13883679\n",
      "Iteration 207, loss = 0.07105313\n",
      "Iteration 203, loss = 0.13907265\n",
      "Iteration 208, loss = 0.07079473\n",
      "Iteration 204, loss = 0.13875312\n",
      "Iteration 209, loss = 0.07157794\n",
      "Iteration 205, loss = 0.13863079\n",
      "Iteration 210, loss = 0.07092797\n",
      "Iteration 206, loss = 0.13886426\n",
      "Iteration 211, loss = 0.07130493\n",
      "Iteration 207, loss = 0.13904632\n",
      "Iteration 212, loss = 0.07150497\n",
      "Iteration 208, loss = 0.13829743\n",
      "Iteration 213, loss = 0.07108019\n",
      "Iteration 209, loss = 0.13895432\n",
      "Iteration 214, loss = 0.07114127\n",
      "Iteration 210, loss = 0.13831364\n",
      "Iteration 215, loss = 0.07118979\n",
      "Iteration 211, loss = 0.13825532\n",
      "Iteration 216, loss = 0.07087310\n",
      "Iteration 212, loss = 0.13866456\n",
      "Iteration 217, loss = 0.07067169\n",
      "Iteration 213, loss = 0.13826857\n",
      "Iteration 218, loss = 0.07082919\n",
      "Iteration 214, loss = 0.13831249\n",
      "Iteration 219, loss = 0.07163274\n",
      "Iteration 215, loss = 0.13837334\n",
      "Iteration 220, loss = 0.07041177\n",
      "Iteration 216, loss = 0.13796466\n",
      "Iteration 221, loss = 0.07087022\n",
      "Iteration 217, loss = 0.13797874\n",
      "Iteration 222, loss = 0.07045764\n",
      "Iteration 218, loss = 0.13790667\n",
      "Iteration 223, loss = 0.07074598\n",
      "Iteration 219, loss = 0.13799533\n",
      "Iteration 224, loss = 0.07059526\n",
      "Iteration 220, loss = 0.13761675\n",
      "Iteration 225, loss = 0.07073693\n",
      "Iteration 226, loss = 0.07056292\n",
      "Iteration 221, loss = 0.13825295\n",
      "Iteration 227, loss = 0.07011129\n",
      "Iteration 222, loss = 0.13758905\n",
      "Iteration 228, loss = 0.07042425\n",
      "Iteration 223, loss = 0.13757603\n",
      "Iteration 229, loss = 0.07030589\n",
      "Iteration 224, loss = 0.13763656\n",
      "Iteration 230, loss = 0.07015513\n",
      "Iteration 225, loss = 0.13759347\n",
      "Iteration 231, loss = 0.07002462\n",
      "Iteration 226, loss = 0.13775641\n",
      "Iteration 232, loss = 0.07032898\n",
      "Iteration 227, loss = 0.13750260\n",
      "Iteration 233, loss = 0.06997622\n",
      "Iteration 228, loss = 0.13779753\n",
      "Iteration 234, loss = 0.06993563\n",
      "Iteration 229, loss = 0.13724313\n",
      "Iteration 235, loss = 0.07009174\n",
      "Iteration 230, loss = 0.13740238\n",
      "Iteration 236, loss = 0.06984790\n",
      "Iteration 231, loss = 0.13704712\n",
      "Iteration 237, loss = 0.06971798\n",
      "Iteration 232, loss = 0.13678621\n",
      "Iteration 238, loss = 0.06992513\n",
      "Iteration 233, loss = 0.13722375\n",
      "Iteration 239, loss = 0.06970841\n",
      "Iteration 234, loss = 0.13733718\n",
      "Iteration 240, loss = 0.06982800\n",
      "Iteration 235, loss = 0.13718430\n",
      "Iteration 241, loss = 0.06999664\n",
      "Iteration 236, loss = 0.13698006\n",
      "Iteration 242, loss = 0.06964461\n",
      "Iteration 237, loss = 0.13755750\n",
      "Iteration 243, loss = 0.06978876\n",
      "Iteration 238, loss = 0.13662282\n",
      "Iteration 244, loss = 0.06979562\n",
      "Iteration 239, loss = 0.13704021\n",
      "Iteration 245, loss = 0.06951560\n",
      "Iteration 240, loss = 0.13666891\n",
      "Iteration 246, loss = 0.06989946\n",
      "Iteration 241, loss = 0.13678329\n",
      "Iteration 247, loss = 0.06946030\n",
      "Iteration 242, loss = 0.13664868\n",
      "Iteration 248, loss = 0.06987364\n",
      "Iteration 243, loss = 0.13668556\n",
      "Iteration 249, loss = 0.06929794\n",
      "Iteration 244, loss = 0.13675100\n",
      "Iteration 250, loss = 0.06967885\n",
      "Iteration 245, loss = 0.13660478\n",
      "Iteration 251, loss = 0.06944458\n",
      "Iteration 246, loss = 0.13670954\n",
      "Iteration 252, loss = 0.06969223\n",
      "Iteration 247, loss = 0.13664904\n",
      "Iteration 253, loss = 0.06883752\n",
      "Iteration 248, loss = 0.13691801\n",
      "Iteration 254, loss = 0.06954222\n",
      "Iteration 249, loss = 0.13622907\n",
      "Iteration 255, loss = 0.06882335\n",
      "Iteration 250, loss = 0.13658518\n",
      "Iteration 256, loss = 0.06923350\n",
      "Iteration 251, loss = 0.13676786\n",
      "Iteration 257, loss = 0.06895353\n",
      "Iteration 252, loss = 0.13597855\n",
      "Iteration 258, loss = 0.06919616\n",
      "Iteration 253, loss = 0.13628825\n",
      "Iteration 259, loss = 0.06887371\n",
      "Iteration 254, loss = 0.13649427\n",
      "Iteration 260, loss = 0.06918347\n",
      "Iteration 255, loss = 0.13623397\n",
      "Iteration 261, loss = 0.06903866\n",
      "Iteration 256, loss = 0.13617173\n",
      "Iteration 262, loss = 0.06899469\n",
      "Iteration 257, loss = 0.13646225\n",
      "Iteration 263, loss = 0.06881247\n",
      "Iteration 258, loss = 0.13567766\n",
      "Iteration 264, loss = 0.06885295\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 259, loss = 0.13592601\n",
      "Iteration 260, loss = 0.13620591\n",
      "Iteration 261, loss = 0.13568554\n",
      "Iteration 262, loss = 0.13587376\n",
      "Iteration 263, loss = 0.13582086\n",
      "Iteration 264, loss = 0.13589815\n",
      "Iteration 265, loss = 0.13552128\n",
      "Iteration 266, loss = 0.13554309\n",
      "Iteration 267, loss = 0.13627742\n",
      "Iteration 268, loss = 0.13550720\n",
      "Iteration 269, loss = 0.13573384\n",
      "Iteration 270, loss = 0.13546996\n",
      "Iteration 271, loss = 0.13541154\n",
      "Iteration 272, loss = 0.13565983\n",
      "Iteration 273, loss = 0.13541621\n",
      "Iteration 274, loss = 0.13512940\n",
      "Iteration 275, loss = 0.13514316\n",
      "Iteration 276, loss = 0.13491705\n",
      "Iteration 277, loss = 0.13545123\n",
      "Iteration 278, loss = 0.13535799\n",
      "Iteration 279, loss = 0.13529396\n",
      "Iteration 280, loss = 0.13508713\n",
      "Iteration 281, loss = 0.13484457\n",
      "Iteration 282, loss = 0.13525284\n",
      "Iteration 283, loss = 0.13497999\n",
      "Iteration 284, loss = 0.13484645\n",
      "Iteration 285, loss = 0.13479769\n",
      "Iteration 286, loss = 0.13523019\n",
      "Iteration 287, loss = 0.13492358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy on training set: 0.76\n"
     ]
    }
   ],
   "source": [
    "# create the MLP classifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 50, 20, 10,), verbose=True, max_iter=max_iterations)\n",
    "\n",
    "# create the one-vs-one classifier\n",
    "ovr_classifier2 = OneVsRestClassifier(mlp, n_jobs=-1)\n",
    "\n",
    "ovr_classifier2.fit(X_train, t_train)\n",
    "\n",
    "# save the trained classifier to a file\n",
    "with open('./models/model4.pkl', 'wb') as f:\n",
    "    pickle.dump(ovr_classifier2, f)\n",
    "\n",
    "print('Accuracy on training set: {:.2f}'.format(ovr_classifier2.score(X_train, t_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5a81fa658f12e1e79dc8217c975b6b57413c027bc13a166564b5f36dc19450f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
